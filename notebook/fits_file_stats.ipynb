{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import norm, median_abs_deviation\n",
    "from scipy.io import loadmat\n",
    "from astropy.coordinates import Angle, SkyCoord\n",
    "from astropy.time import Time\n",
    "import astropy.units as u\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as ticker\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fits_data_index(fits_file: str):\n",
    "    '''\n",
    "    Finds the location of a FITS file's image data array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fits_file : str\n",
    "        The path of the FITS file to be searched.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The index of the image data array in the FITS file.\n",
    "    '''\n",
    "\n",
    "    file_index = 0\n",
    "\n",
    "    #open FITS file\n",
    "    try:\n",
    "        file = fits.open(fits_file)\n",
    "    except:\n",
    "        print(f'Unable to open {fits_file}')\n",
    "\n",
    "    info = file[file_index]\n",
    "    data = info.data\n",
    "    while data is None:\n",
    "        #going through the indices of file to find the array\n",
    "        try:\n",
    "            file_index += 1\n",
    "            info = file[file_index]\n",
    "            data = info.data\n",
    "        except:\n",
    "            print(f'Error in locating data index of {fits_file}')\n",
    "\n",
    "    return file_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_theta(coord, amp, sigma, theta, mu_x, mu_y):\n",
    "    '''\n",
    "    Finds the value at a point on a 2D Gaussian.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coord : tuple\n",
    "        The first entry is the x-coordinate or a list of x-coordinates\n",
    "        and the second entry is the y-coordinate or a list of y-coordinates\n",
    "        of a point or multiple points.\n",
    "    amp : float\n",
    "        The factor in front of the 2D Gaussian's exponent.\n",
    "    sigma : float\n",
    "        The standard deviation of the 2D Gaussian.\n",
    "    theta : float\n",
    "        The angle of rotation of the 2D Gaussian.\n",
    "    mu_x : float\n",
    "        The x-value of the peak of the 2D Gaussian.\n",
    "    mu_y : float\n",
    "        The y-value of the peak of the 2D Gaussian.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The value of the 2D Gaussian evaluated at the given point.\n",
    "    '''\n",
    "\n",
    "    x, y = coord\n",
    "    return amp * np.exp(-(((x-mu_x)*math.cos(theta)+(y-mu_y)*math.sin(theta))**2+(-(x-mu_x)*math.sin(theta)+(y-mu_y)*math.cos(theta))**2)/(2*sigma**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_stats(fits_file: str, center: list = [], radius: list = [], invert: bool = False, Gaussian: bool = True, internal: bool = True,\\\n",
    "                 outer_radius: float = None):\n",
    "    '''\n",
    "    Finds the statistics of a region of an image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fits_file : str\n",
    "        The path of the FITS file that contains the image.\n",
    "    center : list (optional)\n",
    "        A list of center coordinates in units of pixels.\n",
    "        If no center coordinates are given, eventually defaults to ((length of x-axis)/2, (length of y-axis)/2), rounded up.\n",
    "    radius : list (optional)\n",
    "        A list of search radii in units of arcsec.\n",
    "        If no radius list is given, defaults to an empty list.\n",
    "    invert : bool (optional)\n",
    "        Whether to swap the inclusion and exclusion regions.\n",
    "        If no value is given, defaults to False.\n",
    "    Gaussian : bool (optional)\n",
    "        Whether to use a 2D Gaussian fit to estimate the true maximum flux and its corresponding coordinates.\n",
    "        If no value is given, defaults to True.\n",
    "    internal : bool (optional)\n",
    "        Whether the peak to search for is internal (in which case to use a 5x5 pixel region if using a Gaussian fit)\n",
    "        or external (in which case to use a 3x3 pixel region if using a Gaussian fit).\n",
    "        If no value is given, defaults to True.\n",
    "    outer radius : float (optional)\n",
    "        The radius outside of which everything will be excluded. This is not affected by value invert.\n",
    "        If no value is given, defaults to None and will not be used to exclude data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with:\n",
    "            float\n",
    "                The region's maximum flux in Jy.\n",
    "            tuple (int, int)\n",
    "                The coordinates in pixels of the image's center.\n",
    "            tuple (int, int)\n",
    "                The coordinates in pixels of the region's maximum flux.\n",
    "            float\n",
    "                The region's rms in Jy.\n",
    "            float\n",
    "                The image's beam size in arcseconds squared.\n",
    "            float\n",
    "                The image's x-axis length in arcsec.\n",
    "            float\n",
    "                The image's y-axis length in arcsec.\n",
    "            float\n",
    "                The area included in the mask in arcseconds squared.\n",
    "            float\n",
    "                The area excluded by the mask in arcseconds squared.\n",
    "            float\n",
    "                The number of measurements included in the mask.\n",
    "            float\n",
    "                The number of measurements excluded by the mask.\n",
    "            float\n",
    "                The median absolute deviation of the flux of the image.\n",
    "            float\n",
    "                The standard deviation of the flux of the image, as estimated by the MAD.\n",
    "            float\n",
    "                The most negative flux in the image, if such a flux exists. If not, this is None.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    IndexError\n",
    "        If center list and radius list are of different lengths.\n",
    "    '''\n",
    "\n",
    "    if center != [] and len(center) != len(radius):\n",
    "        raise IndexError ('Center list and radius list are of different lengths')\n",
    "\n",
    "    i = fits_data_index(fits_file)\n",
    "\n",
    "    #open FITS file\n",
    "    try:\n",
    "        file = fits.open(fits_file)\n",
    "    except:\n",
    "        print(f'Unable to open {fits_file}')\n",
    "\n",
    "    #extract data array\n",
    "    info = file[i]\n",
    "    data = info.data\n",
    "\n",
    "    neg_peak = float(np.min(data[0]))\n",
    "    if neg_peak >= 0:\n",
    "        neg_peak = None\n",
    "\n",
    "    mad = float(median_abs_deviation(data[0].flatten()))\n",
    "    sd_mad = float(norm.ppf(0.84) / norm.ppf(0.75) * mad) #estimate standard deviation from MAD\n",
    "\n",
    "    #getting dimensions for array\n",
    "    x_dim = info.header['NAXIS1']\n",
    "    y_dim = info.header['NAXIS2']\n",
    "\n",
    "    x_dist_array = np.tile(np.arange(x_dim),(y_dim, 1)) #array of each pixel's horizontal distance (in pixels) from y-axis\n",
    "    y_dist_array = x_dist_array.T #array of each pixel's vertical distance (in pixels) from x-axis\n",
    "\n",
    "    #keep center pixel coordinates if specified, set to default if unspecified\n",
    "    center_pix = center\n",
    "    field_center = (round(x_dim/2), round(y_dim/2))\n",
    "    if center == []:\n",
    "        center_pix = [field_center]\n",
    "        if len(radius) > 1:\n",
    "            center_pix = center_pix * len(radius)\n",
    "\n",
    "    #find units of axes\n",
    "    x_unit = info.header['CUNIT1']\n",
    "    y_unit = info.header['CUNIT2']\n",
    "\n",
    "    #find cell size (units of arcsec)\n",
    "    x_cell_size = (Angle(info.header['CDELT1'], x_unit)).to(u.arcsec)\n",
    "    y_cell_size = (Angle(info.header['CDELT2'], y_unit)).to(u.arcsec)\n",
    "\n",
    "    #find beam size (unitless but in arcsec^2)\n",
    "    beam_size = float(((np.pi/4) * info.header['BMAJ'] * info.header['BMIN'] * Angle(1, x_unit) * Angle(1, y_unit) / np.log(2)).to(u.arcsec**2)\\\n",
    "                / (u.arcsec**2))\n",
    "\n",
    "    #find axis sizes\n",
    "    x_axis_size = x_dim * x_cell_size\n",
    "    y_axis_size = y_dim * y_cell_size\n",
    "\n",
    "    #distance from center array\n",
    "    dist_from_center =((((x_dist_array - center_pix[0][0])*x_cell_size)**2 + ((y_dist_array - center_pix[0][1])*y_cell_size)**2)**0.5)\n",
    "\n",
    "    #boolean mask and apply\n",
    "    mask = (dist_from_center <= radius[0] * u.arcsec)\n",
    "    if len(center) > 1:\n",
    "        for j in range(1, len(center)):\n",
    "            dist_from_center = ((((x_dist_array - center_pix[j][0])*x_cell_size)**2 + ((y_dist_array - center_pix[j][1])*y_cell_size)**2)**0.5)\n",
    "            mask = np.logical_or(mask, (dist_from_center <= radius[j] * u.arcsec))\n",
    "\n",
    "    if invert:\n",
    "        mask = np.logical_not(mask)\n",
    "\n",
    "    if outer_radius is not None:\n",
    "        dist_from_field_center = ((((x_dist_array - field_center[0])*x_cell_size)**2 + ((y_dist_array - field_center[1])*y_cell_size)**2)**0.5)\n",
    "        outer_mask = (dist_from_field_center <= outer_radius * u.arcsec)\n",
    "        mask = np.logical_and(mask, outer_mask)\n",
    "\n",
    "    incl_area = float(mask.sum() * x_cell_size * y_cell_size / (u.arcsec)**2)\n",
    "    excl_area = float(np.logical_not(mask).sum() * x_cell_size * y_cell_size / (u.arcsec)**2)\n",
    "\n",
    "    masked_data = data[0][mask]\n",
    "\n",
    "    #get peak\n",
    "    try:\n",
    "        peak = float(max(masked_data))\n",
    "    except ValueError:\n",
    "        print('No values after mask applied. Check inclusion and exclusion radii.')\n",
    "\n",
    "    #find coordinates of peak\n",
    "    peak_pix = np.where(data[0] == peak)\n",
    "    peak_x = peak_pix[1][0]\n",
    "    peak_y = peak_pix[0][0]\n",
    "    peak_coord = (peak_x, peak_y)\n",
    "\n",
    "    #fit for peak and coordinates assuming Gaussian\n",
    "    #use data from 5x5 region if internal peak\n",
    "    if Gaussian and internal and (peak_x - 2) >= 0 and (peak_x + 2) <= x_dim and (peak_y - 2) >= 0 and (peak_y + 2) <= y_dim:\n",
    "        neg2_2 = data[0][peak_x - 2][peak_y + 2]\n",
    "        neg2_1 = data[0][peak_x - 2][peak_y + 1]\n",
    "        neg2_0 = data[0][peak_x - 2][peak_y]\n",
    "        neg2_neg1 = data[0][peak_x - 2][peak_y - 1]\n",
    "        neg2_neg2 = data[0][peak_x - 2][peak_y - 2]\n",
    "        neg1_2 = data[0][peak_x - 1][peak_y + 2]\n",
    "        neg1_1 = data[0][peak_x - 1][peak_y + 1]\n",
    "        neg1_0 = data[0][peak_x - 1][peak_y]\n",
    "        neg1_neg1 = data[0][peak_x - 1][peak_y - 1]\n",
    "        neg1_neg2 = data[0][peak_x - 1][peak_y - 2]\n",
    "        zero_2 = data[0][peak_x][peak_y + 2]\n",
    "        zero_1 = data[0][peak_x][peak_y + 1]\n",
    "        zero_neg1 = data[0][peak_x][peak_y - 1]\n",
    "        zero_neg2 = data[0][peak_x][peak_y - 2]\n",
    "        pos1_2 = data[0][peak_x + 1][peak_y + 2]\n",
    "        pos1_1 = data[0][peak_x + 1][peak_y + 1]\n",
    "        pos1_0 = data[0][peak_x + 1][peak_y]\n",
    "        pos1_neg1 = data[0][peak_x + 1][peak_y - 1]\n",
    "        pos1_neg2 = data[0][peak_x + 1][peak_y - 2]\n",
    "        pos2_2 = data[0][peak_x + 2][peak_y + 2]\n",
    "        pos2_1 = data[0][peak_x + 2][peak_y + 1]\n",
    "        pos2_0 = data[0][peak_x + 2][peak_y]\n",
    "        pos2_neg1 = data[0][peak_x + 2][peak_y - 1]\n",
    "        pos2_neg2 = data[0][peak_x + 2][peak_y - 2]\n",
    "\n",
    "        z_data = [neg2_2, neg2_1, neg2_0, neg2_neg1, neg2_neg2,\\\n",
    "                neg1_2, neg1_1, neg1_0, neg1_neg1, neg1_neg2,\\\n",
    "                zero_2, zero_1, peak, zero_neg1, zero_neg2,\\\n",
    "                pos1_2, pos1_1, pos1_0, pos1_neg1, pos1_neg2,\\\n",
    "                pos2_2, pos2_1, pos2_0, pos2_neg1, pos2_neg2]\n",
    "        x_data = [-2]*5 + [-1]*5 + [0]*5 + [1]*5 + [2]*5\n",
    "        y_data = [2, 1, 0, -1, -2]*5\n",
    "\n",
    "        try:\n",
    "            popt, pcov = curve_fit(gaussian_theta, (x_data, y_data), z_data, bounds=([peak,0,0,-1,-1],[float('inf'),float('inf'),2*np.pi,1,1]))\n",
    "            amp, sigma, theta, mu_x, mu_y = popt\n",
    "            peak = float(amp)\n",
    "            peak_coord = (float(peak_x + mu_x), float(peak_y + mu_y))\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "\n",
    "    #use data from 3x3 region if external peak\n",
    "    elif Gaussian and (not internal) and (peak_x - 1) >= 0 and (peak_x + 1) <= x_dim and (peak_y - 1) >= 0 and (peak_y + 1) <= y_dim:\n",
    "        left_top = data[0][peak_x - 1][peak_y + 1]\n",
    "        left_middle = data[0][peak_x - 1][peak_y]\n",
    "        left_bottom = data[0][peak_x - 1][peak_y - 1]\n",
    "        middle_top = data[0][peak_x][peak_y + 1]\n",
    "        middle_bottom = data[0][peak_x][peak_y - 1]\n",
    "        right_top = data[0][peak_x + 1][peak_y + 1]\n",
    "        right_middle = data[0][peak_x + 1][peak_y]\n",
    "        right_bottom = data[0][peak_x + 1][peak_y - 1]\n",
    "\n",
    "        z_data = [left_top, left_middle, left_bottom, middle_top, peak, middle_bottom, right_top, right_middle, right_bottom]\n",
    "        x_data = [-1]*3 + [0]*3 + [1]*3\n",
    "        y_data = [1, 0, -1] * 3\n",
    "\n",
    "        try:\n",
    "            popt, pcov = curve_fit(gaussian_theta, (x_data, y_data), z_data, bounds=([peak,0,0,-1,-1],[float('inf'),float('inf'),2*np.pi,1,1]))\n",
    "            amp, sigma, theta, mu_x, mu_y = popt\n",
    "            peak = float(amp)\n",
    "            peak_coord = (float(peak_x + mu_x), float(peak_y + mu_y))\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "\n",
    "    rms = float((np.var(masked_data))**0.5)\n",
    "\n",
    "    stats = {'peak': peak, 'field_center': field_center, 'peak_coord': peak_coord, 'rms': rms, 'beam_size': beam_size,\\\n",
    "             'x_axis': float(x_axis_size / u.arcsec), 'y_axis': float(y_axis_size / u.arcsec), 'incl_area': incl_area, 'excl_area': excl_area,\\\n",
    "             'n_incl_meas': float(incl_area / beam_size), 'n_excl_meas': float(excl_area / beam_size), 'mad': mad, 'sd_mad': sd_mad,\\\n",
    "             'neg_peak': neg_peak}\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prob_from_rms_uncert(peak: float, rms: float, n_excl: float, n_incl: float = None):\n",
    "    '''\n",
    "    Estimates the probability of a value or greater occurring in some number of measurements\n",
    "    of a Gaussian distribution with an imprecisely known RMS.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    peak : float\n",
    "        The smallest value in the range of values whose probability of occurring will be estimated.\n",
    "    rms : float\n",
    "        The imprecisely known RMS value.\n",
    "    n_excl : float\n",
    "        The number of measurements in the region from which the RMS is measured.\n",
    "        If no value is given for n_incl, this is also the number of measurements\n",
    "        for which the probability will be estimated.\n",
    "    n_incl : float (optional)\n",
    "        The number of measurements for which the probability will be estimated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The estimated probability.\n",
    "    '''\n",
    "\n",
    "    #calculate error for rms\n",
    "    rms_err = rms * (n_excl)**(-1/2)\n",
    "\n",
    "    #create normal distributions from rms and error for rms\n",
    "    uncert = np.linspace(-5 * rms_err, 5 * rms_err, 100)\n",
    "    uncert_pdf = norm.pdf(uncert, loc = 0, scale = rms_err)\n",
    "\n",
    "    #sum and normalize to find probabilities\n",
    "    if n_incl == None:\n",
    "        return float(sum((norm.cdf((-1 * peak)/(rms + uncert)) * n_excl) * uncert_pdf) / sum(uncert_pdf))\n",
    "    else:\n",
    "        return float(sum((norm.cdf((-1 * peak)/(rms + uncert)) * n_incl) * uncert_pdf) / sum(uncert_pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_dict_from_rms_uncert(fits_file: str, center: list = [], rms: float = None, threshold: float = 0.01, radius_buffer: float = 5.0,\\\n",
    "                              ext_threshold: float = None):\n",
    "    '''\n",
    "    Finds the probabilities of the internal and external peaks, along with other statistics of an image.\n",
    "    '''\n",
    "\n",
    "    i = fits_data_index(fits_file)\n",
    "\n",
    "    #open FITS file\n",
    "    try:\n",
    "        file = fits.open(fits_file)\n",
    "    except:\n",
    "        print(f'Unable to open {fits_file}')\n",
    "\n",
    "    #extract data array\n",
    "    info = file[i]\n",
    "\n",
    "    beam_fwhm = float((info.header['BMAJ'] * (Angle(1, info.header['CUNIT1'])).to(u.arcsec) / u.arcsec)) #unitless but in arcsec\n",
    "    search_radius = beam_fwhm + radius_buffer #unitless but in arcsec\n",
    "\n",
    "    #search for brightest internal peak\n",
    "    int_stats1 = region_stats(fits_file=fits_file, center=center, radius=[search_radius], invert=False, Gaussian=False, internal=True)\n",
    "    int_coord1 = int_stats1['peak_coord']\n",
    "    int_peak1 = int_stats1['peak']\n",
    "    n_incl = int_stats1['n_incl_meas'] #should be the same for all internal peaks\n",
    "    field_center = int_stats1['field_center'] #in pixels\n",
    "    mad = int_stats1['mad'] #should be the same for all peaks\n",
    "    sd_mad = int_stats1['sd_mad'] #should be the same for all peaks\n",
    "    neg_peak = int_stats1['neg_peak']\n",
    "\n",
    "    #find external peaks and get their info\n",
    "    center = [field_center]\n",
    "    radius = [search_radius]\n",
    "\n",
    "    ext_stats1 = region_stats(fits_file=fits_file, center=center, radius=radius, invert=True, Gaussian=False, internal=False)\n",
    "    n_excl = ext_stats1['n_incl_meas'] #should be the same for all external peaks\n",
    "    ext_peak1 = ext_stats1['peak']\n",
    "    rms = ext_stats1['rms'] #can be changed later as we exclude more peaks\n",
    "    ext_prob1 = calc_prob_from_rms_uncert(peak=ext_peak1, rms=rms, n_excl=n_excl)\n",
    "\n",
    "    prob_dict = {'field_center': field_center, 'rms_val': None, 'mad': mad, 'sd_mad': sd_mad, 'n_incl_meas': n_incl, 'n_excl_meas': n_excl,\\\n",
    "                 'fwhm': beam_fwhm, 'incl_radius': search_radius, 'neg_peak': neg_peak,\\\n",
    "                 'int_peak_val': [], 'int_peak_coord': [], 'int_prob': [], 'int_snr': [],\\\n",
    "                 'ext_peak_val': [], 'ext_peak_coord': [], 'ext_prob': [], 'ext_snr': [], 'next_ext_peak': None}\n",
    "\n",
    "    #update ext_threshold if needed\n",
    "    int_snr1 = int_peak1 / rms\n",
    "    if ext_threshold == None:\n",
    "        if int_snr1 < 20:\n",
    "            ext_threshold = 1e-3\n",
    "        elif int_snr1 < 100:\n",
    "            ext_threshold = 1e-6\n",
    "        else:\n",
    "            ext_threshold = 1e-12\n",
    "\n",
    "    if ext_prob1 < ext_threshold:\n",
    "        ext_significant = True\n",
    "    else:\n",
    "        prob_dict['next_ext_peak'] = ext_peak1\n",
    "        ext_significant = False\n",
    "\n",
    "    while ext_significant:\n",
    "        ext_stats = region_stats(fits_file=fits_file, center=center, radius=radius, invert=True, Gaussian=False, internal=False)\n",
    "        peak = ext_stats['peak']\n",
    "        rms = ext_stats['rms']\n",
    "\n",
    "        ext_prob = calc_prob_from_rms_uncert(peak=peak, rms=rms, n_excl=n_excl)\n",
    "        if ext_prob < ext_threshold:\n",
    "            ext_stats = region_stats(fits_file=fits_file, center=center, radius=radius, invert=True, Gaussian=True, internal=False)\n",
    "            coord = ext_stats['peak_coord']\n",
    "            peak = ext_stats['peak']\n",
    "            ext_prob = calc_prob_from_rms_uncert(peak=peak, rms=rms, n_excl=n_excl)\n",
    "            prob_dict['ext_peak_val'].append(peak)\n",
    "            prob_dict['ext_peak_coord'].append(coord)\n",
    "            prob_dict['ext_prob'].append(ext_prob)\n",
    "            prob_dict['ext_snr'].append(peak / rms)\n",
    "            center.append(coord)\n",
    "            radius.append(beam_fwhm)\n",
    "        else:\n",
    "            prob_dict['next_ext_peak'] = peak\n",
    "            ext_significant = False\n",
    "\n",
    "    prob_dict['rms_val'] = rms\n",
    "\n",
    "    #find prob for 1st internal peak using updated rms\n",
    "    prob_dict['int_peak_val'].append(int_peak1)\n",
    "    prob_dict['int_peak_coord'].append(int_coord1)\n",
    "    int_prob1 = calc_prob_from_rms_uncert(peak=int_peak1, rms=rms, n_excl=n_excl, n_incl=n_incl)\n",
    "    prob_dict['int_prob'].append(int_prob1)\n",
    "    prob_dict['int_snr'].append(int_peak1 / rms)\n",
    "\n",
    "    if threshold == None:\n",
    "        threshold = 0.01\n",
    "    int_significant = (int_prob1 < threshold)\n",
    "\n",
    "    #treat 1st internal peak kind of like an external peak and get rid of search radius so we can look inside\n",
    "    center = [int_coord1]\n",
    "    radius = [beam_fwhm]\n",
    "\n",
    "    #find internal peaks in addition to 1st internal peak\n",
    "    while int_significant:\n",
    "        int_stats = region_stats(fits_file=fits_file, center=center, radius=radius, invert=True, Gaussian=False, internal=True,\\\n",
    "                                 outer_radius=search_radius)\n",
    "        int_peak = int_stats['peak']\n",
    "        int_prob = calc_prob_from_rms_uncert(peak=int_peak, rms=rms, n_excl=n_excl, n_incl=n_incl)\n",
    "        if int_prob < threshold and (int_peak > int_snr1 / 100):\n",
    "            int_stats = region_stats(fits_file=fits_file, center=center, radius=radius, invert=True, Gaussian=True, internal=True,\\\n",
    "                                     outer_radius=search_radius)\n",
    "            int_coord = int_stats['peak_coord']\n",
    "            int_peak = int_stats['peak']\n",
    "            int_prob = calc_prob_from_rms_uncert(peak=int_peak, rms=rms, n_excl=n_excl, n_incl=n_incl)\n",
    "            prob_dict['int_peak_val'].append(int_peak)\n",
    "            prob_dict['int_peak_coord'].append(int_coord)\n",
    "            prob_dict['int_prob'].append(int_prob)\n",
    "            prob_dict['int_snr'].append(int_peak / rms)\n",
    "            center.append(int_coord)\n",
    "            radius.append(beam_fwhm)\n",
    "        else:\n",
    "            int_significant = False\n",
    "\n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_rms_est_from_ext(prob_dict: dict):\n",
    "    '''\n",
    "    Using the rms estimated from the value of the exclusion region's maximum flux,\n",
    "    finds the probability of detecting the inclusion region's maximum flux if there were no source in the inclusion region,\n",
    "    the probability of detecting the exclusion region's maximum flux if there were no source in the exclusion region, and other statistics.\n",
    "\n",
    "    The estimated rms is that the probability of finding such an external peak,\n",
    "    assuming no source in the exclusion region, is 1.\n",
    "    Note: this implies that the external probability will always be 1.\n",
    "\n",
    "    The other statistics include the following as calculated using the rms estimated as described above:\n",
    "    the exclusion region's rms in Jy, the inclusion region's signal to noise ratio,\n",
    "    and the external region's signal to noise ratio.\n",
    "\n",
    "    The remaining statisitcs include the following as calculated using the rms taken directly from the image:\n",
    "    the inclusion region's maximum flux in Jy and its coordinates in pixels,\n",
    "    the exclusion region's maximum flux in Jy and its coordinates in pixels, the exclusion region's rms in Jy,\n",
    "    the number of measurements in the inclusion region, the number of measurements in the exclusion region,\n",
    "    the coordinates in pixels of the image's center, and the radii in pixels of the inclusion zones,\n",
    "    the inclusion region's signal to noise ratio, and the external region's signal to noise ratio.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prob_list : list\n",
    "        The list of statistics, as outputted by get_prob_image_rms(), for an image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list with:\n",
    "            dict(s)\n",
    "                A dictionary with the following, found using the rms taken directly from the image:\n",
    "                    float\n",
    "                        The probability of detecting the inclusion region's maximum flux if there were no source in the inclusion region.\n",
    "                    float\n",
    "                        The probability of detecting the exclusion region's maximum flux if there were no source in the exclusion region.\n",
    "                    float\n",
    "                        The inclusion region's maximum flux in Jy.\n",
    "                    tuple (int, int)\n",
    "                        The coordinates in pixels of the inclusion region's maximum flux.\n",
    "                    float\n",
    "                        The exclusion region's maximum flux in Jy.\n",
    "                    tuple (int, int)\n",
    "                        The coordinates in pixels of the exclusion region's maximum flux.\n",
    "                    float\n",
    "                        The exclusion region's rms in Jy.\n",
    "                    float\n",
    "                        The number of measurements in the inclusion region.\n",
    "                    float\n",
    "                        The number of measurements in the exclusion region.\n",
    "                    tuple (int, int)\n",
    "                        The coordinates in pixels of the image's center.\n",
    "                    list\n",
    "                        A list with:\n",
    "                            float(s)\n",
    "                                The radii in pixels of inclusion zones.\n",
    "                    float\n",
    "                        The inclusion region's signal to noise ratio.\n",
    "                    float\n",
    "                        The exclusion region's signal to noise ratio.\n",
    "            dict\n",
    "                A dictionary with the following, found using the rms estimated as described above:\n",
    "                    float\n",
    "                        The probability of detecting the inclusion region's maximum flux if there were no source in the inclusion region.\n",
    "                    float\n",
    "                        The probability of detecting the exclusion region's maximum flux if there were no source in the exclusion region.\n",
    "                    float\n",
    "                        The exclusion region's rms in Jy.\n",
    "                    float\n",
    "                        The inclusion region's signal to noise ratio.\n",
    "                    float\n",
    "                        The exclusion region's signal to noise ratio.\n",
    "    '''\n",
    "    int_peak_val = prob_dict['int_peak_val']\n",
    "    ext_peak_val = prob_dict['next_ext_peak']\n",
    "    n_incl_meas = prob_dict['n_incl_meas']\n",
    "    n_excl_meas = prob_dict['n_excl_meas']\n",
    "\n",
    "    excl_sigma = -1 * norm.ppf(1/n_excl_meas)\n",
    "    old_rms_val = ext_peak_val / excl_sigma\n",
    "    sigma = norm.ppf(1/(n_incl_meas + n_excl_meas))\n",
    "    neg_peak = prob_dict['neg_peak']\n",
    "    rms_val = neg_peak / sigma\n",
    "\n",
    "    prob_dict['old_calc_rms_val'] = float(old_rms_val)\n",
    "    prob_dict['calc_rms_val'] = float(rms_val)\n",
    "    prob_dict['calc_ext_prob'] = float(norm.cdf((-1 * ext_peak_val)/(rms_val))) * n_excl_meas\n",
    "    prob_dict['calc_ext_snr'] = float(excl_sigma)\n",
    "    for i in range(len(int_peak_val)):\n",
    "        if i == 0:\n",
    "            prob_dict['calc_int_prob'] = [float(norm.cdf((-1 * int_peak_val[i])/(rms_val))) * n_incl_meas]\n",
    "            prob_dict['calc_int_snr'] = [float(int_peak_val[i] / rms_val)]\n",
    "        else:\n",
    "            prob_dict['calc_int_prob'].append(float(norm.cdf((-1 * int_peak_val[i])/(rms_val))) * n_incl_meas)\n",
    "            prob_dict['calc_int_snr'].append(float(int_peak_val[i] / rms_val))\n",
    "\n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(fits_file: str, threshold: float = 0.01, radius_buffer: float = 5.0, ext_threshold: float = None,\\\n",
    "            short_dict: bool = True, plot: bool = True, save_path: str = ''):\n",
    "    '''\n",
    "    Summarizes an image's statistics into a shorter dictionary, a more detailed dictionary, and/or a plot,\n",
    "    with an option to save the plot as a png.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fits_file : str\n",
    "        The path of the FITS file that contains the image.\n",
    "    radius_buffer : float (optional)\n",
    "        The amount of buffer, in arcsec, to add to the beam FWHM to get the initial search radius.\n",
    "        If no value is given, defaults to 5 arcsec.\n",
    "    ext_threshold : float (optional)\n",
    "        The probability that an external peak must be below for it to be considered an external source.\n",
    "        If no value is given, defaults to 0.001.\n",
    "    short_dict : bool (optional)\n",
    "        Whether to return the short dictionary of statistics.\n",
    "        If no value is given, defaults to True.\n",
    "    full_list : bool (optional)\n",
    "        Whether to return the more detailed list of statistics.\n",
    "        If no value is given, defaults to False.\n",
    "    plot : bool (optional)\n",
    "        Whether to plot the image and statistics.\n",
    "        If no value is given, defaults to True.\n",
    "    save_path : str (optional)\n",
    "        The path to which the plot will be saved.\n",
    "        If no value is given, defaults to '' and no image is saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict (if requested)\n",
    "        A shorter dictionary with:\n",
    "            float\n",
    "                The probability, found using the rms taken directly from the image,\n",
    "                of detecting the inclusion region's maximum flux if there were no source in the inclusion region.\n",
    "            list\n",
    "                A list with:\n",
    "                    float(s)\n",
    "                        The probabilities, found using the rms taken directly from the image,\n",
    "                        of detecting the exclusion regions' maximum flux if there were no source in the exclusion regions.\n",
    "                        If there are multiple entries in this list,\n",
    "                        they are the probabilities as the exclusion region becomes increasingly small\n",
    "                        as external peaks deemed significant are added to the inclusion region.\n",
    "            float\n",
    "                The inclusion region's maximum flux in Jy.\n",
    "            tuple (float, float)\n",
    "                The coordinates in relative arcsec of the inclusion region's maximum flux.\n",
    "            list\n",
    "                A list of with:\n",
    "                    float(s)\n",
    "                        The exclusion regions' maximum fluxes in Jy.\n",
    "                        If there are multiple entries in this list,\n",
    "                        they are the maxmimum fluxes as the exclusion region becomes increasingly small\n",
    "                        as external peaks deemed significant are added to the inclusion region.\n",
    "            list\n",
    "                A list with:\n",
    "                    tuple(s) (float, float)\n",
    "                        The coordinates in relative arcsec of the exclusion regions' maximum fluxes.\n",
    "                        If there are multiple entires in this list,\n",
    "                        they are the coordinates as the exclusion region becomes increasingly small\n",
    "                        as external peaks deemed significant are added to the inclusion region.\n",
    "            float\n",
    "                The exclusion region's rms in Jy. This uses the final (smallest) exclusion region.\n",
    "            float\n",
    "                The number of measurements in the inclusion region.\n",
    "            float\n",
    "                The number of measurements in the exclusion region.\n",
    "            tuple (int, int)\n",
    "                The coordinates in relative arcsec of the image's center. Should be (0, 0).\n",
    "            list\n",
    "                A list with:\n",
    "                    float(s):\n",
    "                        The radii in arcsec of inclusion zones.\n",
    "            float\n",
    "                The inclusion region's signal to noise ratio.\n",
    "            list\n",
    "                A list with:\n",
    "                    float(s)\n",
    "                        The exclusion regions' signal to noise ratios.\n",
    "            float\n",
    "                The probability, found using the rms estimated from the value of the exclusion region's maximum flux,\n",
    "                of detecting the inclusion region's maximum flux if there were no source in the inclusion region.\n",
    "            float\n",
    "                The probability, found using the rms estimated from the value of the exclusion region's maximum flux,\n",
    "                of detecting the exclusion region's maximum flux if there were no source in the exclusion region.\n",
    "            float\n",
    "                The rms in Jy estimated from the value of the exclusion region's maximum flux.\n",
    "            float\n",
    "                The inclusion region's signal to noise ratio,\n",
    "                found using the rms estimated from the value of the exclusion region's maximum flux.\n",
    "            float\n",
    "                The exclusion region's signal to noise ratio,\n",
    "                found using the rms estimated from the value of the exclusion region's maximum flux.\n",
    "    list (if requested)\n",
    "        A more detailed list with:\n",
    "            dict(s)\n",
    "                A dictionary with the following, found using the rms taken directly from the image:\n",
    "                    float\n",
    "                        The probability of detecting the inclusion region's maximum flux if there were no source in the inclusion region.\n",
    "                    float\n",
    "                        The probability of detecting the exclusion region's maximum flux if there were no source in the exclusion region.\n",
    "                    float\n",
    "                        The inclusion region's maximum flux in Jy.\n",
    "                    tuple (float, float)\n",
    "                        The coordinates in relative arcsec of the inclusion region's maximum flux.\n",
    "                    float\n",
    "                        The exclusion region's maximum flux in Jy.\n",
    "                    tuple (float, float)\n",
    "                        The coordinates in relative arcsec of the exclusion region's maximum flux.\n",
    "                    float\n",
    "                        The exclusion region's rms in Jy.\n",
    "                    float\n",
    "                        The number of measurements in the inclusion region.\n",
    "                    float\n",
    "                        The number of measurements in the exclusion region.\n",
    "                    tuple (float, float)\n",
    "                        The coordinates in relative arcsec of the image's center. Should be (0.0, 0.0).\n",
    "                    list\n",
    "                        A list with:\n",
    "                            float(s)\n",
    "                                The radii in arcsec of inclusion zones.\n",
    "                    float\n",
    "                        The inclusion region's signal to noise ratio.\n",
    "                    float\n",
    "                        The exclusion region's signal to noise ratio.\n",
    "            dict\n",
    "                A dictionary with the following, found using the rms estimated as described above:\n",
    "                    float\n",
    "                        The probability of detecting the inclusion region's maximum flux if there were no source in the inclusion region.\n",
    "                    float\n",
    "                        The probability of detecting the exclusion region's maximum flux if there were no source in the exclusion region.\n",
    "                    float\n",
    "                        The exclusion region's rms in Jy.\n",
    "                    float\n",
    "                        The inclusion region's signal to noise ratio.\n",
    "                    float\n",
    "                        The exclusion region's signal to noise ratio.\n",
    "    '''\n",
    "    info = (get_prob_rms_est_from_ext(prob_dict_from_rms_uncert(fits_file=fits_file, threshold=threshold, radius_buffer=radius_buffer,\\\n",
    "                                                                ext_threshold=ext_threshold)))\n",
    "\n",
    "    center = info['field_center']\n",
    "\n",
    "    header_data = fits.getheader(fits_file)\n",
    "    pixel_scale = Angle(header_data['CDELT1'], header_data['CUNIT1']).to_value('arcsec')\n",
    "\n",
    "    int_x_coords = []\n",
    "    int_y_coords = []\n",
    "    int_peak_coords = info['int_peak_coord']\n",
    "    n_int_peaks = len(int_peak_coords)\n",
    "    for i in range(n_int_peaks):\n",
    "        #normalized internal peak coordinates\n",
    "        int_x_coords.append((int_peak_coords[i][0] - center[0]) * pixel_scale)\n",
    "        int_y_coords.append((int_peak_coords[i][1] - center[1]) * pixel_scale)\n",
    "    int_x_coords = np.array(int_x_coords)\n",
    "    int_y_coords = np.array(int_y_coords)\n",
    "\n",
    "    incl_radius = info['incl_radius'] #unitless but in arcsec already\n",
    "\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "    ext_peak_coords = info['ext_peak_coord']\n",
    "    n_ext_peaks = len(ext_peak_coords)\n",
    "    for i in range(n_ext_peaks):\n",
    "        #normalized external peak coordinates\n",
    "        x_coords.append((ext_peak_coords[i][0] - center[0]) * pixel_scale)\n",
    "        y_coords.append((ext_peak_coords[i][1] - center[1]) * pixel_scale)\n",
    "\n",
    "    fwhm = info['fwhm']\n",
    "\n",
    "    if plot:\n",
    "        #plt.rcParams['font.family'] = 'serif'\n",
    "        #plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "        plt.rcParams['font.size'] = 15\n",
    "        plt.rcParams['hatch.linewidth'] = 0.5\n",
    "        plt.rcParams['figure.dpi'] = 60\n",
    "\n",
    "        image_data = fits.getdata(fits_file)\n",
    "        shape = image_data.shape\n",
    "\n",
    "        while len(shape) > 2:\n",
    "            image_data = image_data[0]\n",
    "            shape = image_data.shape\n",
    "\n",
    "        plt.set_cmap('inferno')\n",
    "        fig, ax = plt.subplots(figsize=(6.7,5.1))\n",
    "\n",
    "        plt.plot(int_x_coords, int_y_coords, 'wo', fillstyle='none', markersize=15)\n",
    "        plt.plot(int_x_coords, int_y_coords, 'kx', fillstyle='none', markersize=15/np.sqrt(2))\n",
    "\n",
    "        for i in range(n_int_peaks):\n",
    "            int_circle = patches.Circle((int_x_coords[i], int_y_coords[i]), fwhm * pixel_scale, edgecolor='lime', fill=False)\n",
    "            ax.add_artist(int_circle)\n",
    "\n",
    "        int_circle = patches.Circle((0, 0), incl_radius, edgecolor='c', fill=False)\n",
    "        ax.add_artist(int_circle)\n",
    "\n",
    "        if n_ext_peaks > 0:\n",
    "            x_coords = np.array(x_coords)\n",
    "            y_coords = np.array(y_coords)\n",
    "            plt.plot(x_coords, y_coords, 'ko', fillstyle='none', markersize=15)\n",
    "            plt.plot(x_coords, y_coords, 'wx', fillstyle='none', markersize=15/np.sqrt(2))\n",
    "\n",
    "            for i in range(n_ext_peaks):\n",
    "                ext_circle = patches.Circle((x_coords[i], y_coords[i]), fwhm * pixel_scale, edgecolor='lime', fill=False)\n",
    "                ax.add_artist(ext_circle)\n",
    "\n",
    "        int_snr = info['int_snr'][0]\n",
    "\n",
    "        x_min = ((0 - center[0]) - 0.5) * pixel_scale\n",
    "        y_min = ((0 - center[1]) - 0.5) * pixel_scale\n",
    "        x_max = ((image_data.shape[0] -  center[0]) - 0.5) * pixel_scale\n",
    "        y_max = ((image_data.shape[1] -  center[1]) - 0.5) * pixel_scale\n",
    "\n",
    "        beam = patches.Ellipse((x_min*0.88, y_min*0.92), Angle(header_data['BMIN'], header_data['CUNIT1']).to_value('arcsec'),\\\n",
    "                               Angle(header_data['BMAJ'], header_data['CUNIT1']).to_value('arcsec'), fill=True, facecolor='w',\\\n",
    "                                edgecolor='k', angle=header_data['BPA'], hatch='/////', lw=1)\n",
    "        ax.add_artist(beam)\n",
    "\n",
    "        title = fits_file[fits_file.rindex('/')+1:fits_file.index('.fits')]\n",
    "        ax.text(x_min*0.96, y_max*0.96, f'Source: {title}\\nInternal Candidate SNR: {int_snr:.2f}', horizontalalignment='left', verticalalignment='top',\\\n",
    "                fontsize=10, bbox=dict(facecolor='w'))\n",
    "\n",
    "        plt.imshow(image_data, extent=[x_min, x_max, y_min, y_max], origin='lower')\n",
    "\n",
    "        plt.xlabel('Relative RA Offset [arcsec]', fontsize=15)\n",
    "        plt.ylabel('Relative Dec Offset [arcsec]', fontsize=15)\n",
    "\n",
    "        jy_to_mjy = lambda x, pos: '{}'.format(round(x*1000, 1))\n",
    "        fmt = ticker.FuncFormatter(jy_to_mjy)\n",
    "\n",
    "        cbar = plt.colorbar(shrink=0.8, format=fmt)\n",
    "        cbar.ax.set_ylabel('Intensity [mJy/beam]', fontsize=15, rotation=270, labelpad=24)\n",
    "\n",
    "        if save_path != '':\n",
    "            try:\n",
    "                file = fits_file\n",
    "                while '/' in file:\n",
    "                    file = file[file.index('/')+1:]\n",
    "                file = file.replace('.fits', '')\n",
    "                if ext_threshold == None:\n",
    "                    ext_threshold = 'default'\n",
    "                file += f'_rb{radius_buffer}_et{ext_threshold}'\n",
    "                if save_path[-1] != '/':\n",
    "                    save_path = save_path + '/'\n",
    "                plt.savefig(f'{save_path}{file}.jpg')\n",
    "            except:\n",
    "                print('Error saving figure. Double check path entered.')\n",
    "\n",
    "    if short_dict:\n",
    "        short_info = info\n",
    "\n",
    "        int_peaks = []\n",
    "        for i in range(n_int_peaks):\n",
    "            int_peaks.append((float(int_x_coords[i]), float(int_y_coords[i])))\n",
    "\n",
    "        ext_peaks = []\n",
    "        for i in range(n_ext_peaks):\n",
    "            ext_peaks.append((float(x_coords[i]), float(y_coords[i])))\n",
    "\n",
    "        if n_ext_peaks == 0:\n",
    "            ext_peaks = 'No significant external peak'\n",
    "            short_info['ext_peak_val'] = 'No significant external peak'\n",
    "            short_info['ext_snr'] = 'No significant external peak'\n",
    "            short_info['ext_prob'] = 'No significant external peak'\n",
    "\n",
    "        short_info = info\n",
    "        short_info['int_peak_coord'] = int_peaks\n",
    "        short_info['ext_peak_coord'] = ext_peaks\n",
    "        short_info['field_center'] = (0,0)\n",
    "\n",
    "        del short_info['next_ext_peak']\n",
    "\n",
    "        return short_info\n",
    "\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def significant(fits_file: str, threshold: float = 0.01, radius_buffer: float = 5.0, ext_threshold: float = None):\n",
    "    '''\n",
    "    Finds whether a significant source was detected in a field's center region.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fits_file : str\n",
    "        The path of the FITS file that contains the image.\n",
    "    threshold : float (optional)\n",
    "        The threshold for a significant detection.\n",
    "        If the probability of detecting the center region's maximum flux assuming no source in the image\n",
    "        is less than this threshold, then the detection is deemed significant.\n",
    "        If no value is given, defaults to 0.01.\n",
    "    radius_buffer : float (optional)\n",
    "        The amount of buffer, in arcsec, to add to the beam FWHM to get the initial search radius.\n",
    "        If no value is given, defaults to 5 arcsec.\n",
    "    ext_threshold : float (optional)\n",
    "        The probability that an external peak must be below for it to be considered an external source.\n",
    "        If no value is given, defaults to 0.001.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool : Whether a significant source was detected in the field's center region.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If threshold is not between 0 and 1, inclusive.\n",
    "    '''\n",
    "\n",
    "    #make sure reasonable input\n",
    "    if not (threshold >= 0 and threshold <= 1):\n",
    "        raise ValueError('Threshold must be between 0 and 1, inclusive.')\n",
    "\n",
    "    summ = summary(fits_file=fits_file, radius_buffer=radius_buffer, ext_threshold=ext_threshold, short_dict=True, plot=False)\n",
    "    return (summ['int_prob'][0] < threshold and summ['calc_int_prob'][0] < threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_catalog(fits_file: str, threshold: float = 0.01, radius_buffer: float = 5.0, ext_threshold: float = None):\n",
    "    '''\n",
    "    Summarizes information on any significant point sources detected in an image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fits_file : str\n",
    "        The path of the FITS file that contains the image.\n",
    "    threshold : float (optional)\n",
    "        The threshold for a significant detection.\n",
    "        If the probability of detecting the center region's maximum flux assuming no source in the image\n",
    "        is less than this threshold, then the detection is deemed significant.\n",
    "        If no value is given, defaults to 0.01.\n",
    "    radius_buffer : float (optional)\n",
    "        The amount of buffer, in arcsec, to add to the beam FWHM to get the initial search radius.\n",
    "        If no value is given, defaults to 5 arcsec.\n",
    "    ext_threshold : float (optional)\n",
    "        The probability that an external peak must be below for it to be considered an external source.\n",
    "        If no value is given, defaults to 0.001.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with:\n",
    "            dict(s)\n",
    "                A dictionary with:\n",
    "                    str\n",
    "                        The name of the target object of the observation.\n",
    "                    str\n",
    "                        The date and time of the observation.\n",
    "                    str\n",
    "                        The name of the FITS file with the image.\n",
    "                    Angle\n",
    "                        The restoring beam major axis.\n",
    "                    Angle\n",
    "                        The restoring beam minor axis.\n",
    "                    Angle\n",
    "                        The restoring beam position angle.\n",
    "                    float\n",
    "                        The uncertainty in flux density measurements. The rms excluding any significant sources and a small circular region around them.\n",
    "                    float\n",
    "                        The flux density of the detected point source.\n",
    "                    SkyCoord\n",
    "                        The location of the detected point source.\n",
    "                    bool\n",
    "                        Whether the detected point source is in the initial search region.\n",
    "    '''\n",
    "\n",
    "    summ = summary(fits_file=fits_file, radius_buffer=radius_buffer, ext_threshold=ext_threshold, short_dict=True, plot=False)\n",
    "\n",
    "    header_data = fits.getheader(fits_file)\n",
    "    name = header_data['OBJECT']\n",
    "    obs_date_time = header_data['DATE-OBS']\n",
    "    bmaj = header_data['BMAJ']\n",
    "    bmin = header_data['BMIN']\n",
    "    bpa = header_data['BPA']\n",
    "    ctype1 = header_data['CTYPE1']\n",
    "    crval1 = header_data['CRVAL1']\n",
    "    cunit1 = header_data['CUNIT1']\n",
    "    ctype2 = header_data['CTYPE2']\n",
    "    crval2 = header_data['CRVAL2']\n",
    "    cunit2 = header_data['CUNIT2']\n",
    "    ctype3 = header_data['CTYPE3']\n",
    "    crval3 = header_data['CRVAL3']\n",
    "    cunit3 = header_data['CUNIT3']\n",
    "\n",
    "    freq = 'Not found'\n",
    "    if ctype3 == 'FREQ':\n",
    "        freq = '{} {}'.format(crval3, cunit3)\n",
    "    elif ctype3 == 'CHANNUM':\n",
    "        hdul = fits.open(fits_file)\n",
    "        try:\n",
    "            freq_col = hdul[1].columns[1]\n",
    "            if freq_col.name == 'Freq':\n",
    "                if freq_col.unit == 'Hz':\n",
    "                    freq = '{} GHz'.format(hdul[1].data[1][1] / 1e9)\n",
    "                elif freq_col.unit == 'GHz':\n",
    "                    freq = '{} GHz'.format(hdul[1].data[1][1])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    #assume beam axes in same units as CUNIT1 and CUNIT2 and BPA in degrees\n",
    "    beam_maj_axis = Angle(bmaj, cunit1)\n",
    "    beam_min_axis = Angle(bmin, cunit1)\n",
    "    bpa_rad = math.radians(bpa)\n",
    "\n",
    "    moving_objects = ['venus', 'mars', 'jupiter', 'uranus', 'neptune', 'io', 'europa', 'ganymede', 'callisto', 'titan',\\\n",
    "               'ceres', 'vesta', 'pallas', 'juno']\n",
    "\n",
    "    stationary = True\n",
    "    if name.lower() in moving_objects:\n",
    "        stationary = False\n",
    "    else:\n",
    "        for obj in moving_objects:\n",
    "            if obj in name.lower():\n",
    "                stationary = False\n",
    "                break\n",
    "\n",
    "    interesting_sources = {}\n",
    "    field_info = {'Field Name': name, 'Obs Date Time': obs_date_time, 'File Name': fits_file[fits_file.rindex('/')+1:],\\\n",
    "                   'Stationary': stationary,\\\n",
    "                   'Beam Maj Axis': round(float(beam_maj_axis.to(u.arcsec)/u.arcsec), 3) * u.arcsec,\\\n",
    "                   'Beam Min Axis': round(float(beam_min_axis.to(u.arcsec)/u.arcsec), 3) * u.arcsec,\\\n",
    "                   'Beam Pos Angle': round(bpa, 3) * u.deg,\\\n",
    "                   'Freq': freq, 'Flux Uncert': round(summ['rms_val'] * 1000, 3) * u.mJy,}\n",
    "\n",
    "    n_int_sources = len(summ['int_peak_val'])\n",
    "    if type(summ['ext_peak_val']) == str:\n",
    "        n_ext_sources = 0\n",
    "    else:\n",
    "        n_ext_sources = len(summ['ext_peak_val'])\n",
    "\n",
    "    ra_index = 0\n",
    "    dec_index = 1\n",
    "\n",
    "    if 'RA' in ctype1:\n",
    "        ra = crval1\n",
    "    elif 'RA' in ctype2:\n",
    "        ra = crval2\n",
    "        ra_index = 1\n",
    "    else:\n",
    "        raise ValueError('No RA in image')\n",
    "\n",
    "    if 'DEC' in ctype1:\n",
    "        dec = crval1\n",
    "        dec_index = 0\n",
    "    elif 'DEC' in ctype2:\n",
    "        dec = crval2\n",
    "    else:\n",
    "        raise ValueError('No dec in image')\n",
    "\n",
    "    if cunit1 != cunit2:\n",
    "        raise ValueError('Axes have different units')\n",
    "\n",
    "    center = SkyCoord(ra, dec, unit=cunit1)\n",
    "\n",
    "    pt_source_count = 1\n",
    "\n",
    "    for i in range(n_int_sources):\n",
    "        if (summ['int_prob'][i] < threshold and summ['calc_int_prob'][i] < threshold):\n",
    "            info = field_info.copy()\n",
    "            info['Flux Density'] = round(summ[f'int_peak_val'][i] * 1000, 3) * u.mJy\n",
    "\n",
    "            snr = summ[f'int_snr'][i]\n",
    "            b_min_uncert = float(bmaj / snr)\n",
    "            b_maj_uncert = float(bmin / snr)\n",
    "            info['RA Uncert'] = round(b_min_uncert*abs(math.sin(bpa)) + b_maj_uncert*abs(math.cos(bpa)), 3) * u.arcsec\n",
    "            info['Dec Uncert'] = round(b_maj_uncert*abs(math.sin(bpa)) + b_min_uncert*abs(math.cos(bpa)), 3) * u.arcsec\n",
    "\n",
    "            ra_offset = summ[f'int_peak_coord'][i][ra_index] * u.arcsec\n",
    "            dec_offset = summ[f'int_peak_coord'][i][dec_index] * u.arcsec\n",
    "            coord = center.spherical_offsets_by(ra_offset, dec_offset)\n",
    "\n",
    "            ra_str = str(coord.ra)\n",
    "            dec_str = str(coord.dec)\n",
    "\n",
    "            try:\n",
    "                m_index = ra_str.index('m')\n",
    "                s_index = ra_str.index('s')\n",
    "                ra_seconds = ra_str[m_index + 1: s_index]\n",
    "                ra_str = ra_str[:m_index + 1] + str(round(float(ra_seconds), 2)) + 's'\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                m_index = dec_str.index('m')\n",
    "                s_index = dec_str.index('s')\n",
    "                dec_seconds = dec_str[m_index + 1: s_index]\n",
    "                dec_str = dec_str[:m_index + 1] + str(round(float(dec_seconds), 2)) + 's'\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            info['Coord RA'] = ra_str\n",
    "            info['Coord Dec'] = dec_str\n",
    "            info['Internal'] = True\n",
    "\n",
    "            key = f'Source {pt_source_count}'\n",
    "            interesting_sources[key] = info\n",
    "            pt_source_count +=1\n",
    "\n",
    "    for i in range(n_ext_sources):\n",
    "        info = field_info.copy()\n",
    "        info['Flux Density'] = round(summ[f'ext_peak_val'][i] * 1000, 3) * u.mJy\n",
    "\n",
    "        snr = summ[f'ext_snr'][i]\n",
    "        b_min_uncert = float(bmaj / snr)\n",
    "        b_maj_uncert = float(bmin / snr)\n",
    "        info['RA Uncert'] = round(b_min_uncert*abs(math.sin(bpa)) + b_maj_uncert*abs(math.cos(bpa)), 3) * u.arcsec\n",
    "        info['Dec Uncert'] = round(b_maj_uncert*abs(math.sin(bpa)) + b_min_uncert*abs(math.cos(bpa)), 3) * u.arcsec\n",
    "\n",
    "        ra_offset = summ[f'ext_peak_coord'][i][ra_index] * u.arcsec\n",
    "        dec_offset = summ[f'ext_peak_coord'][i][dec_index] * u.arcsec\n",
    "        coord = center.spherical_offsets_by(ra_offset, dec_offset)\n",
    "\n",
    "        ra_str = str(coord.ra)\n",
    "        dec_str = str(coord.dec)\n",
    "\n",
    "        try:\n",
    "            m_index = ra_str.index('m')\n",
    "            s_index = ra_str.index('s')\n",
    "            ra_seconds = ra_str[m_index + 1: s_index]\n",
    "            ra_str = ra_str[:m_index + 1] + str(round(float(ra_seconds), 2)) + 's'\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            m_index = dec_str.index('m')\n",
    "            s_index = dec_str.index('s')\n",
    "            dec_seconds = dec_str[m_index + 1: s_index]\n",
    "            dec_str = dec_str[:m_index + 1] + str(round(float(dec_seconds), 2)) + 's'\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        info['Coord RA'] = ra_str\n",
    "        info['Coord Dec'] = dec_str\n",
    "        info['Internal'] = False\n",
    "\n",
    "        key = f'Source {pt_source_count}'\n",
    "        interesting_sources[key] = info\n",
    "        pt_source_count +=1\n",
    "\n",
    "    if interesting_sources == {}:\n",
    "        return\n",
    "    else:\n",
    "        return interesting_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_catalogs(catalog_1: dict, catalog_2: dict):\n",
    "    '''\n",
    "    Combines two catalogs in the format returned by make_catalog() into a single catalog of the same format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    catalog_1 : dict\n",
    "        The catalog to which the other catalog will be \"appended.\"\n",
    "    catalog_2 : dict\n",
    "        The catalog to \"append\" to the other catalog.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary of the combined catalogs in the same catalog format.\n",
    "    '''\n",
    "\n",
    "    shift = len(catalog_1)\n",
    "    for key, value in catalog_2.items():\n",
    "        new_number = int(key.replace('Source ', ''))\n",
    "        new_key = f'Source {new_number + shift}'\n",
    "        catalog_1[new_key] = value\n",
    "    return catalog_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_html(html_path):\n",
    "    '''\n",
    "    Starts source_info.html, in which source information can be stored.\n",
    "    '''\n",
    "\n",
    "    with open(html_path, 'w') as html_file:\n",
    "        start = '''\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <style>\n",
    "        img.field {\n",
    "        width: 40%;\n",
    "        height: 40%\n",
    "        }\n",
    "        img.bp {\n",
    "        width: 20%;\n",
    "        height: 20%\n",
    "        }\n",
    "        img.gain {\n",
    "        width: 45%;\n",
    "        height: 45%\n",
    "        }\n",
    "        .centered-large-text {\n",
    "        text-align: center;\n",
    "        font-size: 36px;\n",
    "        }\n",
    "        </style>\n",
    "        <body>\n",
    "        '''\n",
    "        html_file.write(start)\n",
    "        html_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_info_to_html(json_file: str, html_path: str):\n",
    "    '''\n",
    "    Appends observation information table to source_info.html using information from a .json file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    json_file : str\n",
    "        The path of the .json file that contains the observation information.\n",
    "    '''\n",
    "\n",
    "    with open(html_path, 'a') as html_file:\n",
    "        try:\n",
    "            with open(json_file, 'r') as file:\n",
    "                obs_dict = json.load(file)\n",
    "\n",
    "            #cleaning up obs_dict\n",
    "            for key, value in obs_dict.items():\n",
    "                if type(value) == list:\n",
    "                    string = ', '.join(value)\n",
    "                    obs_dict[key] = [string]\n",
    "            obs_id = obs_dict.pop('obsID')\n",
    "            base_name = obs_dict.pop('basename')\n",
    "\n",
    "            df = pd.DataFrame(obs_dict)\n",
    "            df_transposed = df.T\n",
    "\n",
    "            html_table = df_transposed.to_html()\n",
    "\n",
    "            html_file.write(f'<p class=\\'centered-large-text\\'>Source Information for {base_name} (ObsID {obs_id}) </p>')\n",
    "            html_file.write(html_table)\n",
    "        except:\n",
    "            html_file.write('<p> Error generating observation information table. </p>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap_eff_to_html(html_path, matlab: str):\n",
    "\n",
    "    try:\n",
    "        data = loadmat(matlab)\n",
    "        ap_eff_array = data['apEffCorr']\n",
    "\n",
    "        n_ants = len(ap_eff_array)\n",
    "        panda_dict = {}\n",
    "\n",
    "        for ant in range(n_ants):\n",
    "            ant_eff = {}\n",
    "            ant_eff['RxA LSB'] = float(ap_eff_array[ant][0])\n",
    "            ant_eff['RxA USB'] = float(ap_eff_array[ant][1])\n",
    "            ant_eff['RxB LSB'] = float(ap_eff_array[ant][2])\n",
    "            ant_eff['RxB USB'] = float(ap_eff_array[ant][3])\n",
    "            panda_dict[f'Ant {ant+1}'] = ant_eff\n",
    "\n",
    "        df = pd.DataFrame.from_dict(panda_dict)\n",
    "        df_transposed = df.T\n",
    "        html_table = df_transposed.to_html()\n",
    "\n",
    "        with open(html_path, 'a') as html_file:\n",
    "            html_file.write(html_table)\n",
    "    except:\n",
    "        print('Error with aperture efficiency data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration_plots(html_path, matlab: str):\n",
    "\n",
    "    plt.rcdefaults()\n",
    "    plt.rcParams['figure.dpi'] = 60\n",
    "    plt.rcParams['font.size'] = 8\n",
    "\n",
    "\n",
    "    data = loadmat(matlab)\n",
    "    gt = data['gainTime']\n",
    "    gws = data['gainWinSoln']\n",
    "    gcs = data['gainChanSoln']\n",
    "    gain_type = data['gainType']\n",
    "\n",
    "    n_times = len(gt)\n",
    "    n_ants = len(gws[0])\n",
    "    n_spws = len(gws[0][0])\n",
    "    n_chans = len(gcs[0][0][0])\n",
    "\n",
    "    utc_midpts = []\n",
    "    for t in range(len(gt)):\n",
    "        midpt = 0.5 * (gt[t][0].real + gt[t][0].imag)\n",
    "        utc_midpts.append((midpt%1)*24)\n",
    "\n",
    "    colors = ['blue','r','y','purple','orange','g','m','c']\n",
    "\n",
    "    chan_bit = 7\n",
    "    if all(bit == 0 for bit in (gain_type & (2**chan_bit))):\n",
    "        chan_bit = 0\n",
    "    spw_bit = 6\n",
    "    if all(bit == 0 for bit in (gain_type & (2**spw_bit))):\n",
    "        spw_bit = 1\n",
    "\n",
    "    #plotting bandpass gain solutions for amplitude and phase\n",
    "    fig, ax = plt.subplots(nrows=n_ants, ncols=1, sharex=True, figsize=(3,8))\n",
    "    fig2, ax2 = plt.subplots(nrows=n_ants, ncols=1, sharex=True, figsize=(3,8))\n",
    "\n",
    "    max_amp = 0\n",
    "\n",
    "    for time in range(n_times):\n",
    "        if (gain_type & (2**chan_bit))[time] != 0:\n",
    "            for ant in range(n_ants):\n",
    "\n",
    "                #shifting for cosmetics\n",
    "                pos = ax[ant].get_position()\n",
    "                pos.x0 += 0.05\n",
    "                pos.x1 += 0.05\n",
    "                ax[ant].set_position(pos)\n",
    "                pos2 = ax2[ant].get_position()\n",
    "                pos2.x0 += 0.06\n",
    "                pos2.x1 += 0.06\n",
    "                ax2[ant].set_position(pos2)\n",
    "\n",
    "                #no x axis ticks\n",
    "                ax[ant].xaxis.set_tick_params(labelbottom=False)\n",
    "                ax2[ant].xaxis.set_tick_params(labelbottom=False)\n",
    "\n",
    "\n",
    "                for spw in range(n_spws):\n",
    "                    amp_to_plot = [abs(a) for a in gcs.copy()[time][ant][spw]]\n",
    "                    pha_to_plot = [np.angle(p, deg=True) for p in gcs.copy()[time][ant][spw]]\n",
    "                    if max(amp_to_plot) > max_amp:\n",
    "                        max_amp = max(amp_to_plot)\n",
    "\n",
    "                    x_axis = np.arange(spw * n_chans + 1, (1 + spw) * n_chans + 1)\n",
    "\n",
    "                    ax[ant].scatter(x_axis, amp_to_plot, c=colors[spw], s=20, marker='x', linewidths=1.5)\n",
    "                    ax2[ant].scatter(x_axis, pha_to_plot, c=colors[spw], s=20, marker='x', linewidths=1.5)\n",
    "\n",
    "                    ax[ant].yaxis.set_label_position('right')\n",
    "                    ax2[ant].yaxis.set_label_position('right')\n",
    "                    ax[ant].set_ylabel(f'Ant{ant+1}')\n",
    "                    ax2[ant].set_ylabel(f'Ant{ant+1}')\n",
    "\n",
    "    plt.setp(ax, yticks=np.arange(0, max_amp+1, 0.5))\n",
    "    plt.setp(ax2, yticks=[-180,-120,-60,0,60,120,180])\n",
    "    fig.suptitle('Bandpass gain solutions for amplitude', y=0.92)\n",
    "    fig2.suptitle('Bandpass gain solutions for phase', y=0.92)\n",
    "    fig.supxlabel('Full antenna bandwidth', y=0.07)\n",
    "    fig2.supxlabel('Full antenna bandwidth', y=0.07)\n",
    "    fig.supylabel('Gain amplitude')\n",
    "    fig2.supylabel('Gain phase')\n",
    "\n",
    "    html_folder = os.path.dirname(html_path)\n",
    "\n",
    "    fig.savefig(os.path.join(html_folder, 'bp_amp.jpg'))\n",
    "    fig2.savefig(os.path.join(html_folder, 'bp_pha.jpg'))\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    #plotting gain solutions for amplitude and phase\n",
    "    n_rows = math.ceil(n_ants / 2)\n",
    "    n_cols = 2\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=n_rows, ncols=n_cols, sharex=True, figsize=(5.7,4))\n",
    "    fig2, ax2 = plt.subplots(nrows=n_rows, ncols=n_cols, sharex=True, figsize=(5.7,4))\n",
    "\n",
    "    max_amp, min_time, max_time = 0, float('inf'), 0\n",
    "\n",
    "    for spw in range(n_spws):\n",
    "        for ant in range(n_ants):\n",
    "            amp_to_plot, pha_to_plot = [], []\n",
    "            times = []\n",
    "\n",
    "            if ant < n_rows:\n",
    "                row, col = ant, 0\n",
    "\n",
    "                #shifting for cosmetics\n",
    "                pos = ax[row, col].get_position()\n",
    "                pos.x0 -= 0.005\n",
    "                pos.x1 -= 0.005\n",
    "                ax[row, col].set_position(pos)\n",
    "                pos2 = ax2[row, col].get_position()\n",
    "                pos2.x0 -= 0.005\n",
    "                pos2.x1 -= 0.005\n",
    "                ax2[row, col].set_position(pos2)\n",
    "            else:\n",
    "                row, col = ant % n_rows, 1\n",
    "\n",
    "            for time in range(n_times):\n",
    "                if gain_type[time] & (2**6) != 0:\n",
    "                    amp_val = abs((gws.copy())[time][ant][spw])\n",
    "                    pha_val = np.angle((gws.copy())[time][ant][spw], deg=True)\n",
    "                    amp_to_plot.append(amp_val)\n",
    "                    pha_to_plot.append(pha_val)\n",
    "\n",
    "                    if amp_val > max_amp:\n",
    "                        max_amp = amp_val\n",
    "\n",
    "                    t = utc_midpts[time]\n",
    "                    if t < min_time:\n",
    "                        min_time = t\n",
    "                    if t > max_time:\n",
    "                        max_time = t\n",
    "\n",
    "                    times.append(t)\n",
    "\n",
    "            ax[row, col].scatter(times, amp_to_plot, c=colors[spw], s=4, marker='D')\n",
    "            ax2[row, col].scatter(times, pha_to_plot, c=colors[spw], s=4, marker='D')\n",
    "\n",
    "            ax[row, col].yaxis.set_label_position('right')\n",
    "            ax2[row, col].yaxis.set_label_position('right')\n",
    "            ax[row, col].set_ylabel(f'Ant{ant+1}')\n",
    "            ax2[row, col].set_ylabel(f'Ant{ant+1}')\n",
    "            amp_to_plot, pha_to_plot = [], []\n",
    "\n",
    "    plt.setp(ax, xticks=np.arange(min_time//1, math.ceil(max_time), 1), yticks=np.arange(0, max_amp+1, 0.5))\n",
    "    plt.setp(ax2, xticks=np.arange(min_time//1, math.ceil(max_time), 1), yticks=[-180,-120,-60,0,60,120,180])\n",
    "    fig.suptitle('Gain solutions for amplitude')\n",
    "    fig2.suptitle('Gain solutions for phase')\n",
    "    fig.supxlabel('UT hours')\n",
    "    fig2.supxlabel('UT hours')\n",
    "    fig.supylabel('Gain amplitude')\n",
    "    fig2.supylabel('Gain phase')\n",
    "\n",
    "    fig.savefig(os.path.join(html_folder, 'g_amp.jpg'))\n",
    "    fig2.savefig(os.path.join(html_folder, 'g_pha.jpg'))\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_to_html(html_path: str, fits_file: str, radius_buffer: float = 5.0, ext_threshold: float = None):\n",
    "    '''\n",
    "    Appends source figures to source_info.html.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fits_file : str\n",
    "        The path of the FITS file that contains the image.\n",
    "    radius_buffer : float (optional)\n",
    "        The amount of buffer, in arcsec, to add to the beam FWHM to get the initial search radius.\n",
    "        If no value is given, defaults to 5 arcsec.\n",
    "    ext_threshold : float (optional)\n",
    "        The probability that an external peak must be below for it to be considered an external source.\n",
    "        If no value is given, defaults to 0.001.\n",
    "    '''\n",
    "\n",
    "    with open(html_path, 'a') as html_file:\n",
    "        try:\n",
    "            summary(fits_file=fits_file, radius_buffer=radius_buffer, ext_threshold=ext_threshold,\\\n",
    "                    short_dict=False, plot=True, save_path=os.path.dirname(html_path))\n",
    "\n",
    "            #getting full path\n",
    "            file = fits_file\n",
    "            while '/' in file:\n",
    "                file = file[file.index('/')+1:]\n",
    "            file = file.replace('.fits', '')\n",
    "            if ext_threshold == None:\n",
    "                ext_threshold = 'default'\n",
    "            file += f'_rb{radius_buffer}_et{ext_threshold}'\n",
    "            full_path = f'./{file}.jpg'\n",
    "\n",
    "            html_figure = f'''\n",
    "            <img class=\\'field\\' src=\\'{full_path}\\'>\n",
    "            <br>\n",
    "            '''\n",
    "\n",
    "            html_file.write(html_figure)\n",
    "        except:\n",
    "            html_file.write(f'<p> Error generating figure for {fits_file}. </p>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catalog_to_html(catalog: dict, html_path):\n",
    "    '''\n",
    "    Appends source information table to source_info.html.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    catalog : dict\n",
    "        A catalog in the format returned by make_catalog().\n",
    "    '''\n",
    "\n",
    "    df = pd.DataFrame.from_dict(catalog)\n",
    "    df_transposed = df.T\n",
    "    html_table = df_transposed.to_html()\n",
    "\n",
    "    with open(html_path, 'a') as html_file:\n",
    "        html_file.write(html_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_html(html_path: str):\n",
    "    '''\n",
    "    Ends source_info.html, in which source information can be stored.\n",
    "    '''\n",
    "\n",
    "    with open(html_path, 'a') as html_file:\n",
    "\n",
    "        end = '''\n",
    "        </body>\n",
    "        </html>\n",
    "        '''\n",
    "\n",
    "        html_file.write(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_html_and_txt(folder: str, threshold: float = 0.01, radius_buffer: float = 5.0, ext_threshold: float = None):\n",
    "    '''\n",
    "    From a folder of FITS files, creates source_info.html with observation information table, source figures, and source information table\n",
    "    and creates interesting_field.txt with names of objects with any (possibly) interesting detections.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder : str\n",
    "        The path of the folder containing the FITS files to be analyzed.\n",
    "    threshold : float (optional)\n",
    "        The threshold for a significant detection.\n",
    "        If the probability of detecting the center region's maximum flux assuming no source in the image\n",
    "        is less than this threshold, then the detection is deemed significant.\n",
    "        If no value is given, defaults to 0.01.\n",
    "    radius_buffer : float (optional)\n",
    "        The amount of buffer, in arcsec, to add to the beam FWHM to get the initial search radius.\n",
    "        If no value is given, defaults to 5 arcsec.\n",
    "    ext_threshold : float (optional)\n",
    "        The probability that an external peak must be below for it to be considered an external source.\n",
    "        If no value is given, defaults to 0.001.\n",
    "    '''\n",
    "\n",
    "    html_path = os.path.join(folder, 'index.html')\n",
    "    matlab_file = os.path.join(folder, 'gains.mat')\n",
    "\n",
    "    start_html(html_path)\n",
    "\n",
    "    json_file = os.path.join(folder, 'polaris.json')\n",
    "\n",
    "    obs_info_to_html(json_file, html_path)\n",
    "\n",
    "    ap_eff_to_html(html_path, matlab_file)\n",
    "\n",
    "    try:\n",
    "        calibration_plots(html_path, matlab_file)\n",
    "\n",
    "        with open(html_path, 'a') as html_file:\n",
    "            html_gain_info = f'''\n",
    "            <img class=\\'bp\\' src=\\'./bp_amp.jpg'\\'>\n",
    "            <img class=\\'bp\\' src=\\'./bp_pha.jpg'\\'>\n",
    "            <br>\n",
    "            <img class=\\'gain\\' src=\\'./g_amp.jpg'\\'>\n",
    "            <img class=\\'gain\\' src=\\'./g_pha.jpg'\\'>\n",
    "            <br>\n",
    "            '''\n",
    "            html_file.write(html_gain_info)\n",
    "    except:\n",
    "        print('Error with gain calibration information.')\n",
    "\n",
    "    final_catalog = {}\n",
    "    with open(json_file, 'r') as file:\n",
    "        obs_dict = json.load(file)\n",
    "\n",
    "    sci_targs = [targ.lower() for targ in obs_dict['sciTargs']]\n",
    "    pol_cals = [cal.lower() for cal in obs_dict['polCals']]\n",
    "    with open(os.path.join(folder, 'interesting_fields.txt'), 'w') as txt:\n",
    "        for file in glob.glob(os.path.join(folder, '*.fits')):\n",
    "            obj = fits.getheader(file)['OBJECT']\n",
    "            if obj.lower() not in pol_cals:\n",
    "                fig_to_html(html_path, file, radius_buffer=radius_buffer, ext_threshold=ext_threshold)\n",
    "            if obj.lower() in sci_targs:\n",
    "                catalog = make_catalog(file, threshold=threshold, radius_buffer=radius_buffer, ext_threshold=ext_threshold)\n",
    "\n",
    "                #add field name to .txt file if it is a science target with a significant detection in the initial inclusion region\n",
    "                if catalog != None:\n",
    "                    for key, value in catalog.items():\n",
    "                        if value['Internal'] == True:\n",
    "                            txt.write(f'{obj}\\n')\n",
    "                    final_catalog = combine_catalogs(final_catalog, catalog)\n",
    "\n",
    "    catalog_to_html(final_catalog, html_path)\n",
    "    end_html(html_path)\n",
    "\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_level_csv(folder, csv_path = './low_level.csv'):\n",
    "\n",
    "    master_catalog = None\n",
    "    old_df = None\n",
    "    str_obs_id = 'Unknown'\n",
    "\n",
    "    try:\n",
    "        old_df = pd.read_csv(csv_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        str_obs_id = folder.replace('/mnt/COMPASS9/sma/quality/', '')\n",
    "        obs_id = str_obs_id.replace('/', '')\n",
    "        obs_id = int(obs_id) #will throw Exception if obs_id isn't just numbers\n",
    "        if old_df is not None:\n",
    "            old_df = old_df[(old_df['Obs ID']) != obs_id] #removing old or outdated entries\n",
    "    except Exception as e:\n",
    "        obs_id = 'Unknown'\n",
    "        print(f'Error with obsID: {e}. WARNING: Old/outdated data may not be deleted.')\n",
    "\n",
    "    if old_df is not None:\n",
    "        master_catalog = (old_df.T).to_dict()\n",
    "\n",
    "    for file in glob.glob(os.path.join(folder, '*.fits')):\n",
    "        try:\n",
    "            catalog = make_catalog(file)\n",
    "            if catalog is not None:\n",
    "                for value in catalog.values():\n",
    "                    value['Obs ID'] = obs_id\n",
    "                    value['Source ID'] = 'Unknown'\n",
    "                if master_catalog is None:\n",
    "                    master_catalog = catalog\n",
    "                elif catalog is not None:\n",
    "                    master_catalog = combine_catalogs(master_catalog, catalog)\n",
    "        except Exception as e:\n",
    "            print(f'Error for {file}: {e}')\n",
    "\n",
    "    df = pd.DataFrame.from_dict(master_catalog)\n",
    "    df = df.T\n",
    "    df.to_csv(csv_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_level_csv(low_level_path = './low_level.csv', high_level_path = './high_level.csv'):\n",
    "\n",
    "    low_df = pd.read_csv(low_level_path)\n",
    "    unique_sources = None\n",
    "\n",
    "    try:\n",
    "        unique_sources = pd.read_csv(high_level_path).to_dict(orient='list')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #coarse matching\n",
    "    for row in range(len(low_df)):\n",
    "        if low_df['Source ID'].iloc[row] == 'Unknown': #check to make sure we didn't already do coarse matching\n",
    "            if low_df['Stationary'].iloc[row]:\n",
    "                if unique_sources is not None:\n",
    "                    ra = low_df['Coord RA'].iloc[row]\n",
    "                    dec = low_df['Coord Dec'].iloc[row]\n",
    "                    coord1 = SkyCoord(ra, dec)\n",
    "                    fwhm = low_df['Beam Maj Axis'].iloc[row]\n",
    "                    fwhm1_val = float(fwhm.replace(' arcsec', ''))\n",
    "                    source_ids = unique_sources['Source ID']\n",
    "                    matched  = False\n",
    "                    while not matched:\n",
    "                        for i in range(len(source_ids)): #compare with each unique source\n",
    "                            coord2 = SkyCoord(unique_sources['RA'][i], unique_sources['Dec'][i])\n",
    "                            sep = coord1.separation(coord2)\n",
    "                            fwhm2_val = float(unique_sources['FWHM'][i].replace(' arcsec', ''))\n",
    "                            max_sep = (fwhm1_val * fwhm2_val)**(1/2) * u.arcsec\n",
    "                            matched = (sep <= max_sep)\n",
    "                            if matched:\n",
    "                                low_df.loc[row, 'Source ID'] = source_ids[i]\n",
    "                                break\n",
    "                        break\n",
    "                    if not matched:\n",
    "                        num = 1\n",
    "                        id_nums = [int(source_id.replace('id', '')) for source_id in unique_sources['Source ID']]\n",
    "                        while num in id_nums:\n",
    "                            num += 1\n",
    "                        next_number = '0' * (4 - len(str(num))) + str(num)\n",
    "                        next_id = f'id{next_number}'\n",
    "                        source_ids.append(next_id)\n",
    "                        unique_sources['RA'].append(ra)\n",
    "                        unique_sources['Dec'].append(dec)\n",
    "                        unique_sources['FWHM'].append(fwhm)\n",
    "                        low_df.loc[row, 'Source ID'] = next_id\n",
    "                        unique_sources['Ambiguous Ties'].append('Unknown')\n",
    "                else:\n",
    "                    ra = low_df['Coord RA'].iloc[row]\n",
    "                    dec = low_df['Coord Dec'].iloc[row]\n",
    "                    fwhm = low_df['Beam Maj Axis'].iloc[row]\n",
    "                    unique_sources = {'Source ID': ['id0001'], 'RA': [ra], 'Dec': [dec], 'FWHM': [fwhm], 'Ambiguous Ties': ['Unknown']}\n",
    "                    low_df.loc[row, 'Source ID'] = 'id0001'\n",
    "            else:\n",
    "                low_df.loc[row, 'Source ID'] = 'Not Stationary'\n",
    "\n",
    "    #further refining matches\n",
    "    new_sources = unique_sources.copy()\n",
    "    refined = []\n",
    "    to_skip = []\n",
    "    for i in range(len(unique_sources['Source ID'])):\n",
    "        temp_df = low_df[(low_df['Source ID']) == unique_sources['Source ID'][i]]\n",
    "        ra_list = [Angle(ra, u.deg) for ra in temp_df['Coord RA']]\n",
    "        dec_list = [Angle(dec, u.deg) for dec in temp_df['Coord Dec']]\n",
    "        fwhm_list = [Angle(fwhm, u.arcsec) for fwhm in temp_df['Beam Maj Axis']]\n",
    "        if len(unique_sources['Source ID']) > 1 and i not in to_skip:\n",
    "            for j in range(i + 1, len(unique_sources['Source ID'])):\n",
    "                if j not in to_skip:\n",
    "                    temp_df2 = low_df[(low_df['Source ID']) == unique_sources['Source ID'][j]]\n",
    "                    ra_list2 = [Angle(ra, u.deg) for ra in temp_df2['Coord RA']]\n",
    "                    dec_list2 = [Angle(dec, u.deg) for dec in temp_df2['Coord Dec']]\n",
    "                    fwhm_list2 = [Angle(fwhm, u.arcsec) for fwhm in temp_df2['Beam Maj Axis']]\n",
    "                    new_ra_list = ra_list + ra_list2\n",
    "                    new_dec_list = dec_list + dec_list2\n",
    "                    new_fwhm_list = fwhm_list + fwhm_list2\n",
    "                    num_pts = len(new_ra_list)\n",
    "                    avg_ra = sum(new_ra_list) / num_pts\n",
    "                    avg_dec = sum(new_dec_list) / num_pts\n",
    "                    geo_avg_fwhm = math.prod(new_fwhm_list) ** (1/num_pts)\n",
    "                    avg_pt = SkyCoord(avg_ra, avg_dec)\n",
    "                    temp = 0\n",
    "                    for pt in range(num_pts):\n",
    "                        sep = avg_pt.separation(SkyCoord(new_ra_list[pt], new_dec_list[pt]))\n",
    "                        if sep > geo_avg_fwhm / 2:\n",
    "                            temp += 1\n",
    "                    proportion = (num_pts - temp) / (num_pts)\n",
    "                    if proportion == 1: #average point is a good representative for all points, same source\n",
    "                        refined.append(new_sources['Source ID'][i])\n",
    "                        #match found, update averages\n",
    "                        new_sources['RA'][i] = avg_ra\n",
    "                        new_sources['Dec'][i] = avg_dec\n",
    "                        new_sources['FWHM'][i] = geo_avg_fwhm\n",
    "                        #get rid of \"replaced\" source in Ambiguous Ties\n",
    "                        for k in range(len(unique_sources['Source ID'])):\n",
    "                            unique_sources['Ambiguous Ties'][k] = unique_sources['Ambiguous Ties'][k].replace(unique_sources['Source ID'][j], '')\n",
    "                            unique_sources['Ambiguous Ties'][k] = unique_sources['Ambiguous Ties'][k].replace('__', '_')\n",
    "                            if unique_sources['Ambiguous Ties'][k][0] == '_':\n",
    "                                unique_sources['Ambiguous Ties'][k] = unique_sources['Ambiguous Ties'][k][1:]\n",
    "                            if unique_sources['Ambiguous Ties'][k][-1] == '_':\n",
    "                                unique_sources['Ambiguous Ties'][k] = unique_sources['Ambiguous Ties'][k][:-1]\n",
    "                        #update low_df\n",
    "                        indices = low_df.index[low_df['Source ID'] == unique_sources['Source ID'][j]]\n",
    "                        low_df.loc[indices, 'Source ID'] = unique_sources['Source ID'][i]\n",
    "                        to_skip.append(j)\n",
    "                    elif proportion > 0.7: #average point is a good representative for over 70% but less than 100% of points, ambiguous\n",
    "                        if new_sources['Ambiguous Ties'][i] == 'Unknown' or new_sources['Ambiguous Ties'][i] == 'None found':\n",
    "                            new_sources['Ambiguous Ties'][i] = unique_sources['Source ID'][j]\n",
    "                        elif unique_sources['Source ID'][j] not in new_sources['Ambiguous Ties'][i]:\n",
    "                            new_sources['Ambiguous Ties'][i] += '_{}'.format(unique_sources['Source ID'][j])\n",
    "                        if new_sources['Ambiguous Ties'][j] == 'Unknown' or new_sources['Ambiguous Ties'][j] == 'None found':\n",
    "                            new_sources['Ambiguous Ties'][j] = unique_sources['Source ID'][i]\n",
    "                        elif unique_sources['Source ID'][i] not in new_sources['Ambiguous Ties'][j]:\n",
    "                            new_sources['Ambiguous Ties'][j] += '_{}'.format(unique_sources['Source ID'][i])\n",
    "                    if new_sources['Ambiguous Ties'][i] == 'Unknown':\n",
    "                        new_sources['Ambiguous Ties'][i] = 'None found'\n",
    "                    if new_sources['Ambiguous Ties'][j] == 'Unknown':\n",
    "                        new_sources['Ambiguous Ties'][j] = 'None found'\n",
    "    to_skip.sort(reverse=True)\n",
    "    for k in to_skip:\n",
    "        del new_sources['Source ID'][k]\n",
    "        del new_sources['RA'][k]\n",
    "        del new_sources['Dec'][k]\n",
    "        del new_sources['FWHM'][k]\n",
    "        del new_sources['Ambiguous Ties'][k]\n",
    "\n",
    "    #get averages for sources only matched with coarse matching\n",
    "    for i in range(len(new_sources['Source ID'])):\n",
    "        if new_sources['Source ID'][i] not in refined:\n",
    "            temp_df = low_df[(low_df['Source ID']) == new_sources['Source ID'][i]]\n",
    "            ra_list = [Angle(ra, u.deg) for ra in temp_df['Coord RA']]\n",
    "            dec_list = [Angle(dec, u.deg) for dec in temp_df['Coord Dec']]\n",
    "            fwhm_list = [Angle(fwhm, u.arcsec) for fwhm in temp_df['Beam Maj Axis']]\n",
    "            num_pts = len(ra_list)\n",
    "            avg_ra = sum(ra_list) / num_pts\n",
    "            avg_dec = sum(dec_list) / num_pts\n",
    "            geo_avg_fwhm = math.prod(fwhm_list) ** (1/num_pts)\n",
    "            new_sources['RA'][i] = avg_ra\n",
    "            new_sources['Dec'][i] = avg_dec\n",
    "            new_sources['FWHM'][i] = geo_avg_fwhm\n",
    "\n",
    "    df = pd.DataFrame.from_dict(new_sources)\n",
    "    df.to_csv(high_level_path, mode='w', header=True, index=False)\n",
    "    low_df.to_csv(low_level_path, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def light_curve(low_path: str, high_path: str, unique_ids: list = None):\n",
    "\n",
    "    low_df = pd.read_csv(low_path)\n",
    "    high_df = pd.read_csv(high_path)\n",
    "    if unique_ids == None:\n",
    "        unique_ids = high_df['Source ID'].tolist()\n",
    "    for source in unique_ids:\n",
    "        plt.subplots()\n",
    "        source_df = low_df[low_df['Source ID'] == source]\n",
    "        fluxes = source_df['Flux Density']\n",
    "        fluxes = [float(flux.replace('mJy', '')) for flux in fluxes]\n",
    "        flux_errs = source_df['Flux Uncert']\n",
    "        flux_errs = [float(err.replace('mJy', '')) for err in flux_errs]\n",
    "        flux_unit = 'mJy'\n",
    "        if max(fluxes) > 1000:\n",
    "            flux_unit = 'Jy'\n",
    "            for i in range(len(fluxes)):\n",
    "                fluxes[i] /= 1000\n",
    "                flux_errs[i] /= 1000\n",
    "        date_times = source_df['Obs Date Time'].tolist()\n",
    "        for i in range(len(date_times)):\n",
    "            dt = date_times[i]\n",
    "            m_end = dt.rindex(':')\n",
    "            s_start = m_end + 1\n",
    "            if dt[s_start:] == '60':\n",
    "                dt = dt[:s_start] + '0'\n",
    "                fmt = '%m-%d-%y %H:%M'\n",
    "                date_times[i] = (datetime.strptime(dt[:m_end], fmt) + timedelta(minutes=1)).strftime('%m-%d-%y %H:%M:%S')\n",
    "\n",
    "        fmt_str = '%m-%d-%y %H:%M:%S'\n",
    "        date_times = [Time(datetime.strptime(dt, fmt_str), format='datetime', scale='utc').mjd for dt in date_times]\n",
    "\n",
    "        freqs = source_df['Freq'].tolist()\n",
    "        other = []\n",
    "        milli = []\n",
    "        micro = []\n",
    "        for i in range(len(freqs)):\n",
    "            if freqs[i] == 'Not found':\n",
    "                other.append(i)\n",
    "                pass\n",
    "            else:\n",
    "                try:\n",
    "                    freqs[i] = float(freqs[i].replace('GHz', ''))\n",
    "                    if freqs[i] > 214 and freqs[i] < 273:\n",
    "                        milli.append(i)\n",
    "                    elif freqs[i] > 340 and freqs[i] < 355:\n",
    "                        micro.append(i)\n",
    "                    else:\n",
    "                        other.append(i)\n",
    "                except Exception as e:\n",
    "                    print(f'Error while getting the frequencies for source {source}: {e}')\n",
    "        other_dt = [date_times[a] for a in other]\n",
    "        other_flx = [fluxes[a] for a in other]\n",
    "        other_flx_err = [flux_errs[a] for a in other]\n",
    "        milli_dt = [date_times[b] for b in milli]\n",
    "        milli_flx = [fluxes[b] for b in milli]\n",
    "        milli_flx_err = [flux_errs[b] for b in milli]\n",
    "        micro_dt = [date_times[c] for c in micro]\n",
    "        micro_flx = [fluxes[c] for c in micro]\n",
    "        micro_flx_err = [flux_errs[c] for c in micro]\n",
    "        if milli != []:\n",
    "            plt.errorbar(milli_dt, milli_flx, yerr=milli_flx_err, color='r', fmt='x', capsize=5, markersize=4,\\\n",
    "                        capthick=0.5, elinewidth=0.5, label='1.4-1.1mm SMA')\n",
    "        if micro != []:\n",
    "            plt.errorbar(micro_dt, micro_flx, yerr=micro_flx_err, color='g', fmt='x', capsize=5, markersize=4,\\\n",
    "                        capthick=0.5, elinewidth=0.5, label='870m SMA')\n",
    "        if other != []:\n",
    "            plt.errorbar(other_dt, other_flx, yerr=other_flx_err, color='b', fmt='x', capsize=5, markersize=4,\\\n",
    "                        capthick=0.5, elinewidth=0.5, label='Other/not found')\n",
    "        plt.title(f'Source {source[2:]}')\n",
    "        plt.xlabel('Modified Julian Date')\n",
    "        plt.ylabel(f'Flux in {flux_unit}')\n",
    "        plt.legend()\n",
    "        plt.ylim(bottom=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in glob.glob('../data/multi_track/*.fits'):\n",
    "#    try:\n",
    "#        summary(fits_file=file, short_dict=False, plot=True, save_path='../test')\n",
    "        #print(s['sd_mad'])\n",
    "        #print('calc_rms: {}, old_calc_rms: {}. rms: {}, sd_mad: {}'.format(s['calc_rms_val'], s['old_calc_rms_val'], s['rms_val'], s['sd_mad']))\n",
    "#    except Exception as e:\n",
    "#        print(f'Oops: {e} for {file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = summary('../data/250611_03:56:34/1310+323_full.fits', plot=False)\n",
    "#for key, value in s.items():\n",
    "#    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "#full_html_and_txt('../data/250611_03:56:34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%lprun -f full_html_and_txt full_html_and_txt('../data/250611_03:56:34/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_catalog('/reduction/karto/SMA/polaris_test/250103_05:25:18/g232.6207+00.9959_full.fits')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
