{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import norm, median_abs_deviation\n",
    "from scipy.io import loadmat\n",
    "from astropy.coordinates import Angle, SkyCoord\n",
    "from astropy.time import Time\n",
    "import astropy.units as u\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as ticker\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fits_data_index(fits_file: str):\n",
    "    '''\n",
    "    Finds the location of a FITS file's image data array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fits_file : str\n",
    "        The path of the FITS file to be searched.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The index of the image data array in the FITS file.\n",
    "    '''\n",
    "\n",
    "    file_index = 0\n",
    "\n",
    "    #open FITS file\n",
    "    try:\n",
    "        file = fits.open(fits_file)\n",
    "    except:\n",
    "        print(f'Unable to open {fits_file}')\n",
    "\n",
    "    info = file[file_index]\n",
    "    data = info.data\n",
    "    while data is None:\n",
    "        #going through the indices of file to find the array\n",
    "        try:\n",
    "            file_index += 1\n",
    "            info = file[file_index]\n",
    "            data = info.data\n",
    "        except:\n",
    "            print(f'Error in locating data index of {fits_file}')\n",
    "\n",
    "    return file_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_theta(coord, amp, sigma, theta, mu_x, mu_y):\n",
    "    '''\n",
    "    Finds the value at a point on a 2D Gaussian.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coord : tuple\n",
    "        The coordinate(s) of the point(s) where the first entry is the x-coordinate or a list of x-coordinates\n",
    "        and the second entry is the y-coordinate or a list of y-coordinates.\n",
    "    amp : float\n",
    "        The factor in front of the 2D Gaussian's exponent.\n",
    "    sigma : float\n",
    "        The standard deviation of the 2D Gaussian.\n",
    "    theta : float\n",
    "        The angle of rotation of the 2D Gaussian.\n",
    "    mu_x : float\n",
    "        The x-value of the peak of the 2D Gaussian.\n",
    "    mu_y : float\n",
    "        The y-value of the peak of the 2D Gaussian.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The value of the 2D Gaussian evaluated at the given point.\n",
    "    '''\n",
    "\n",
    "    x, y = coord\n",
    "    return amp * np.exp(-(((x-mu_x)*math.cos(theta)+(y-mu_y)*math.sin(theta))**2+(-(x-mu_x)*math.sin(theta)+(y-mu_y)*math.cos(theta))**2)/(2*sigma**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_stats(fits_file: str, center: list = [], radius: list = [], invert: bool = False, Gaussian: bool = True, internal: bool = True,\\\n",
    "                 outer_radius: float = None):\n",
    "    '''\n",
    "    Finds the statistics of a region of an image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fits_file : str\n",
    "        The path of the FITS file that contains the image.\n",
    "    center : list (optional)\n",
    "        A list of center coordinates in units of pixels.\n",
    "        If no center coordinates are given, eventually defaults to [((length of x-axis)/2, (length of y-axis)/2)], rounded up.\n",
    "    radius : list (optional)\n",
    "        A list of search radii in units of arcsec.\n",
    "        If no radius list is given, defaults to an empty list.\n",
    "    invert : bool (optional)\n",
    "        Whether to swap the inclusion and exclusion regions.\n",
    "        If no value is given, defaults to False.\n",
    "    Gaussian : bool (optional)\n",
    "        Whether to use a 2D Gaussian fit to estimate the true maximum flux and its corresponding coordinates.\n",
    "        If no value is given, defaults to True.\n",
    "    internal : bool (optional)\n",
    "        Whether the peak to search for is internal (in which case to use a 5x5 pixel region if using a Gaussian fit)\n",
    "        or external (in which case to use a 3x3 pixel region if using a Gaussian fit).\n",
    "        If no value is given, defaults to True.\n",
    "    outer radius : float (optional)\n",
    "        The radius outside of which everything will be excluded. This is not affected by value invert.\n",
    "        If no value is given, defaults to None and will not be used to exclude data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with:\n",
    "            float\n",
    "                The region's maximum flux in Jy.\n",
    "            tuple (int, int)\n",
    "                The coordinates in pixels of the image's center.\n",
    "            tuple (int, int)\n",
    "                The coordinates in pixels of the region's maximum flux.\n",
    "            float\n",
    "                The region's rms in Jy.\n",
    "            float\n",
    "                The image's beam size in arcseconds squared.\n",
    "            float\n",
    "                The image's x-axis length in arcsec.\n",
    "            float\n",
    "                The image's y-axis length in arcsec.\n",
    "            float\n",
    "                The area included in the mask in arcseconds squared.\n",
    "            float\n",
    "                The area excluded by the mask in arcseconds squared.\n",
    "            float\n",
    "                The number of measurements included in the mask.\n",
    "            float\n",
    "                The number of measurements excluded by the mask.\n",
    "            float\n",
    "                The median absolute deviation of the flux of the image.\n",
    "            float\n",
    "                The standard deviation of the flux of the image, as estimated by the MAD.\n",
    "            float\n",
    "                The most negative flux in the image, if such a flux exists. If not, this is None.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    IndexError\n",
    "        If center list and radius list are of different lengths.\n",
    "    '''\n",
    "\n",
    "    if center != [] and len(center) != len(radius):\n",
    "        raise IndexError ('Center list and radius list are of different lengths')\n",
    "\n",
    "    i = fits_data_index(fits_file)\n",
    "\n",
    "    #open FITS file\n",
    "    try:\n",
    "        file = fits.open(fits_file)\n",
    "    except:\n",
    "        print(f'Unable to open {fits_file}')\n",
    "\n",
    "    #extract data array\n",
    "    info = file[i]\n",
    "    data = info.data\n",
    "\n",
    "    neg_peak = float(np.min(data[0]))\n",
    "    if neg_peak >= 0:\n",
    "        neg_peak = None\n",
    "\n",
    "    mad = float(median_abs_deviation(data[0].flatten()))\n",
    "    sd_mad = float(norm.ppf(0.84) / norm.ppf(0.75) * mad) #estimate standard deviation from MAD\n",
    "\n",
    "    #getting dimensions for array\n",
    "    x_dim = info.header['NAXIS1']\n",
    "    y_dim = info.header['NAXIS2']\n",
    "\n",
    "    x_dist_array = np.tile(np.arange(x_dim),(y_dim, 1)) #array of each pixel's horizontal distance (in pixels) from y-axis\n",
    "    y_dist_array = x_dist_array.T #array of each pixel's vertical distance (in pixels) from x-axis\n",
    "\n",
    "    #keep center pixel coordinates if specified, set to default if unspecified\n",
    "    center_pix = center\n",
    "    field_center = (round(x_dim/2), round(y_dim/2))\n",
    "    if center == []:\n",
    "        center_pix = [field_center]\n",
    "        if len(radius) > 1:\n",
    "            center_pix = center_pix * len(radius)\n",
    "\n",
    "    #find units of axes\n",
    "    x_unit = info.header['CUNIT1']\n",
    "    y_unit = info.header['CUNIT2']\n",
    "\n",
    "    #find cell size (units of arcsec)\n",
    "    x_cell_size = (Angle(info.header['CDELT1'], x_unit)).to(u.arcsec)\n",
    "    y_cell_size = (Angle(info.header['CDELT2'], y_unit)).to(u.arcsec)\n",
    "\n",
    "    #find beam size (unitless but in arcsec^2)\n",
    "    beam_size = float(((np.pi/4) * info.header['BMAJ'] * info.header['BMIN'] * Angle(1, x_unit) * Angle(1, y_unit) / np.log(2)).to(u.arcsec**2)\\\n",
    "                / (u.arcsec**2))\n",
    "\n",
    "    #find axis sizes\n",
    "    x_axis_size = x_dim * x_cell_size\n",
    "    y_axis_size = y_dim * y_cell_size\n",
    "\n",
    "    #distance from center array\n",
    "    dist_from_center =((((x_dist_array - center_pix[0][0])*x_cell_size)**2 + ((y_dist_array - center_pix[0][1])*y_cell_size)**2)**0.5)\n",
    "\n",
    "    #boolean mask and apply\n",
    "    mask = (dist_from_center <= radius[0] * u.arcsec)\n",
    "    if len(center) > 1:\n",
    "        for j in range(1, len(center)):\n",
    "            dist_from_center = ((((x_dist_array - center_pix[j][0])*x_cell_size)**2 + ((y_dist_array - center_pix[j][1])*y_cell_size)**2)**0.5)\n",
    "            mask = np.logical_or(mask, (dist_from_center <= radius[j] * u.arcsec))\n",
    "\n",
    "    if invert:\n",
    "        mask = np.logical_not(mask)\n",
    "\n",
    "    if outer_radius is not None:\n",
    "        dist_from_field_center = ((((x_dist_array - field_center[0])*x_cell_size)**2 + ((y_dist_array - field_center[1])*y_cell_size)**2)**0.5)\n",
    "        outer_mask = (dist_from_field_center <= outer_radius * u.arcsec)\n",
    "        mask = np.logical_and(mask, outer_mask)\n",
    "\n",
    "    incl_area = float(mask.sum() * x_cell_size * y_cell_size / (u.arcsec)**2)\n",
    "    excl_area = float(np.logical_not(mask).sum() * x_cell_size * y_cell_size / (u.arcsec)**2)\n",
    "\n",
    "    masked_data = data[0][mask]\n",
    "\n",
    "    #get peak\n",
    "    try:\n",
    "        peak = float(max(masked_data))\n",
    "    except ValueError:\n",
    "        print('No values after mask applied. Check inclusion and exclusion radii.')\n",
    "\n",
    "    #find coordinates of peak\n",
    "    peak_pix = np.where(data[0] == peak)\n",
    "    peak_x = int(peak_pix[1][0])\n",
    "    peak_y = int(peak_pix[0][0])\n",
    "    peak_coord = (peak_x, peak_y)\n",
    "\n",
    "    #fit for peak and coordinates assuming Gaussian\n",
    "    #use data from 5x5 region if internal peak\n",
    "    if Gaussian and internal and (peak_x - 2) >= 0 and (peak_x + 2) <= x_dim and (peak_y - 2) >= 0 and (peak_y + 2) <= y_dim:\n",
    "        neg2_2 = data[0][peak_x - 2][peak_y + 2]\n",
    "        neg2_1 = data[0][peak_x - 2][peak_y + 1]\n",
    "        neg2_0 = data[0][peak_x - 2][peak_y]\n",
    "        neg2_neg1 = data[0][peak_x - 2][peak_y - 1]\n",
    "        neg2_neg2 = data[0][peak_x - 2][peak_y - 2]\n",
    "        neg1_2 = data[0][peak_x - 1][peak_y + 2]\n",
    "        neg1_1 = data[0][peak_x - 1][peak_y + 1]\n",
    "        neg1_0 = data[0][peak_x - 1][peak_y]\n",
    "        neg1_neg1 = data[0][peak_x - 1][peak_y - 1]\n",
    "        neg1_neg2 = data[0][peak_x - 1][peak_y - 2]\n",
    "        zero_2 = data[0][peak_x][peak_y + 2]\n",
    "        zero_1 = data[0][peak_x][peak_y + 1]\n",
    "        zero_neg1 = data[0][peak_x][peak_y - 1]\n",
    "        zero_neg2 = data[0][peak_x][peak_y - 2]\n",
    "        pos1_2 = data[0][peak_x + 1][peak_y + 2]\n",
    "        pos1_1 = data[0][peak_x + 1][peak_y + 1]\n",
    "        pos1_0 = data[0][peak_x + 1][peak_y]\n",
    "        pos1_neg1 = data[0][peak_x + 1][peak_y - 1]\n",
    "        pos1_neg2 = data[0][peak_x + 1][peak_y - 2]\n",
    "        pos2_2 = data[0][peak_x + 2][peak_y + 2]\n",
    "        pos2_1 = data[0][peak_x + 2][peak_y + 1]\n",
    "        pos2_0 = data[0][peak_x + 2][peak_y]\n",
    "        pos2_neg1 = data[0][peak_x + 2][peak_y - 1]\n",
    "        pos2_neg2 = data[0][peak_x + 2][peak_y - 2]\n",
    "\n",
    "        z_data = [neg2_2, neg2_1, neg2_0, neg2_neg1, neg2_neg2,\\\n",
    "                neg1_2, neg1_1, neg1_0, neg1_neg1, neg1_neg2,\\\n",
    "                zero_2, zero_1, peak, zero_neg1, zero_neg2,\\\n",
    "                pos1_2, pos1_1, pos1_0, pos1_neg1, pos1_neg2,\\\n",
    "                pos2_2, pos2_1, pos2_0, pos2_neg1, pos2_neg2]\n",
    "        x_data = [-2]*5 + [-1]*5 + [0]*5 + [1]*5 + [2]*5\n",
    "        y_data = [2, 1, 0, -1, -2]*5\n",
    "\n",
    "        try:\n",
    "            popt, pcov = curve_fit(gaussian_theta, (x_data, y_data), z_data, bounds=([peak,0,0,-1,-1],[float('inf'),float('inf'),2*np.pi,1,1]))\n",
    "            amp, sigma, theta, mu_x, mu_y = popt\n",
    "            peak = float(amp)\n",
    "            peak_coord = (float(peak_x + mu_x), float(peak_y + mu_y))\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "\n",
    "    #use data from 3x3 region if external peak\n",
    "    elif Gaussian and (not internal) and (peak_x - 1) >= 0 and (peak_x + 1) <= x_dim and (peak_y - 1) >= 0 and (peak_y + 1) <= y_dim:\n",
    "        left_top = data[0][peak_x - 1][peak_y + 1]\n",
    "        left_middle = data[0][peak_x - 1][peak_y]\n",
    "        left_bottom = data[0][peak_x - 1][peak_y - 1]\n",
    "        middle_top = data[0][peak_x][peak_y + 1]\n",
    "        middle_bottom = data[0][peak_x][peak_y - 1]\n",
    "        right_top = data[0][peak_x + 1][peak_y + 1]\n",
    "        right_middle = data[0][peak_x + 1][peak_y]\n",
    "        right_bottom = data[0][peak_x + 1][peak_y - 1]\n",
    "\n",
    "        z_data = [left_top, left_middle, left_bottom, middle_top, peak, middle_bottom, right_top, right_middle, right_bottom]\n",
    "        x_data = [-1]*3 + [0]*3 + [1]*3\n",
    "        y_data = [1, 0, -1] * 3\n",
    "\n",
    "        try:\n",
    "            popt, pcov = curve_fit(gaussian_theta, (x_data, y_data), z_data, bounds=([peak,0,0,-1,-1],[float('inf'),float('inf'),2*np.pi,1,1]))\n",
    "            amp, sigma, theta, mu_x, mu_y = popt\n",
    "            peak = float(amp)\n",
    "            peak_coord = (float(peak_x + mu_x), float(peak_y + mu_y))\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "\n",
    "    rms = float((np.var(masked_data))**0.5)\n",
    "\n",
    "    stats = {'peak': peak, 'field_center': field_center, 'peak_coord': peak_coord, 'rms': rms, 'beam_size': beam_size,\\\n",
    "             'x_axis': float(x_axis_size / u.arcsec), 'y_axis': float(y_axis_size / u.arcsec), 'incl_area': incl_area, 'excl_area': excl_area,\\\n",
    "             'n_incl_meas': float(incl_area / beam_size), 'n_excl_meas': float(excl_area / beam_size), 'mad': mad, 'sd_mad': sd_mad,\\\n",
    "             'neg_peak': neg_peak}\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prob_from_rms_uncert(peak: float, rms: float, n_excl: float, n_incl: float = None):\n",
    "    '''\n",
    "    Estimates the probability of a value or greater occurring in some number of measurements\n",
    "    of a Gaussian distribution with an imprecisely known RMS.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    peak : float\n",
    "        The smallest value in the range of values whose probability of occurring will be estimated.\n",
    "    rms : float\n",
    "        The imprecisely known RMS value.\n",
    "    n_excl : float\n",
    "        The number of measurements in the region from which the RMS is measured.\n",
    "        If no value is given for n_incl, this is also the number of measurements\n",
    "        for which the probability will be estimated.\n",
    "    n_incl : float (optional)\n",
    "        The number of measurements for which the probability will be estimated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The estimated probability.\n",
    "    '''\n",
    "\n",
    "    #calculate error for rms\n",
    "    rms_err = rms * (n_excl)**(-1/2)\n",
    "\n",
    "    #create normal distributions from rms and error for rms\n",
    "    uncert = np.linspace(-5 * rms_err, 5 * rms_err, 100)\n",
    "    uncert_pdf = norm.pdf(uncert, loc = 0, scale = rms_err)\n",
    "\n",
    "    #sum and normalize to find probabilities\n",
    "    if n_incl == None:\n",
    "        return float(sum((norm.cdf((-1 * peak)/(rms + uncert)) * n_excl) * uncert_pdf) / sum(uncert_pdf))\n",
    "    else:\n",
    "        return float(sum((norm.cdf((-1 * peak)/(rms + uncert)) * n_incl) * uncert_pdf) / sum(uncert_pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_dict_from_rms_uncert(fits_file: str, center: list = [], threshold: float = 0.01, radius_buffer: float = 5.0,\\\n",
    "                              ext_threshold: float = None):\n",
    "    '''\n",
    "    Finds the probabilities of the internal and external peaks, as well as other relevant statistics of an image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fits_file : str\n",
    "        The path of the FITS file that contains the image.\n",
    "    center : list (optional)\n",
    "        A list of center coordinates in units of pixels.\n",
    "        If no center coordinates are given, first defaults to [((length of x-axis)/2, (length of y-axis)/2)], rounded up.\n",
    "    threshold : float (optional)\n",
    "        The maximum probability, assuming no source in the image, for a significant internal detection.\n",
    "        If no value is given, defaults to 0.01.\n",
    "    radius_buffer : float (optional)\n",
    "        The amount of buffer, in arcsec, to add to the beam FWHM to get the initial search radius.\n",
    "        If no value is given, defaults to 5 arcsec.\n",
    "    ext_threshold : float (optional)\n",
    "        The probability that an external peak must be below for it to be considered an external source.\n",
    "        If no value is given, defaults to 1e-3, 1e-6, or 1e-12, depending on the SNR of the internal peak.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with:\n",
    "            tuple (int, int)\n",
    "                The coordinates in pixels of the image's center.\n",
    "            float\n",
    "                The image's rms in Jy.\n",
    "            float\n",
    "                The median absolute deviation of the flux of the image.\n",
    "            float\n",
    "                The standard deviation of the flux of the image, as estimated by the MAD.\n",
    "            float\n",
    "                The number of measurements included in the mask.\n",
    "            float\n",
    "                The number of measurements excluded by the mask.\n",
    "            float\n",
    "                The length of the beam major axis in arcsec.\n",
    "            float\n",
    "                The radius of the initial inclusion region in arcsec.\n",
    "            float\n",
    "                The most negative flux in the image, if such a flux exists. If not, this is None.\n",
    "            list\n",
    "                A list with:\n",
    "                    float(s)\n",
    "                        The flux of the brightest internal peak and the fluxes of the remaining significant internal peaks,\n",
    "                        if these exist.\n",
    "            list\n",
    "                A list with:\n",
    "                    tuple(s) (int, int)\n",
    "                        The coordinates in pixels of the brightest internal peak and the remaining significant internal peaks,\n",
    "                        if these exist.\n",
    "            list\n",
    "                A list with:\n",
    "                    float(s)\n",
    "                        The probability/probabilities of the brightest internal peak and the remaining significant internal peaks,\n",
    "                        if these exist.\n",
    "            list\n",
    "                A list with:\n",
    "                    float(s)\n",
    "                        The signal to noise ratios of the brightest internal peak and the remaining significant internal peaks,\n",
    "                        if these exist.\n",
    "            list\n",
    "                A list with:\n",
    "                    float(s)\n",
    "                        The flux(es) the significant external peak(s), if these exist.\n",
    "\n",
    "ext_peak_coord: []\n",
    "ext_prob: []\n",
    "ext_snr: []\n",
    "next_ext_peak: 0.11062327027320862\n",
    "    '''\n",
    "\n",
    "    i = fits_data_index(fits_file)\n",
    "\n",
    "    #open FITS file\n",
    "    try:\n",
    "        file = fits.open(fits_file)\n",
    "    except:\n",
    "        print(f'Unable to open {fits_file}')\n",
    "\n",
    "    #extract data array\n",
    "    info = file[i]\n",
    "\n",
    "    beam_fwhm = float((info.header['BMAJ'] * (Angle(1, info.header['CUNIT1'])).to(u.arcsec) / u.arcsec)) #unitless but in arcsec\n",
    "    search_radius = beam_fwhm + radius_buffer #unitless but in arcsec\n",
    "\n",
    "    #search for brightest internal peak\n",
    "    int_stats1 = region_stats(fits_file=fits_file, center=center, radius=[search_radius], invert=False, Gaussian=False, internal=True)\n",
    "    int_coord1 = int_stats1['peak_coord']\n",
    "    int_peak1 = int_stats1['peak']\n",
    "    n_incl = int_stats1['n_incl_meas'] #should be the same for all internal peaks\n",
    "    field_center = int_stats1['field_center'] #in pixels\n",
    "    mad = int_stats1['mad'] #should be the same for all peaks\n",
    "    sd_mad = int_stats1['sd_mad'] #should be the same for all peaks\n",
    "    neg_peak = int_stats1['neg_peak']\n",
    "\n",
    "    #find external peaks and get their info\n",
    "    center = [field_center]\n",
    "    radius = [search_radius]\n",
    "\n",
    "    ext_stats1 = region_stats(fits_file=fits_file, center=center, radius=radius, invert=True, Gaussian=False, internal=False)\n",
    "    n_excl = ext_stats1['n_incl_meas'] #should be the same for all external peaks\n",
    "    ext_peak1 = ext_stats1['peak']\n",
    "    rms = ext_stats1['rms'] #can be changed later as we exclude more peaks\n",
    "    ext_prob1 = calc_prob_from_rms_uncert(peak=ext_peak1, rms=rms, n_excl=n_excl)\n",
    "\n",
    "    prob_dict = {'field_center': field_center, 'rms_val': None, 'mad': mad, 'sd_mad': sd_mad, 'n_incl_meas': n_incl, 'n_excl_meas': n_excl,\\\n",
    "                 'fwhm': beam_fwhm, 'incl_radius': search_radius, 'neg_peak': neg_peak,\\\n",
    "                 'int_peak_val': [], 'int_peak_coord': [], 'int_prob': [], 'int_snr': [],\\\n",
    "                 'ext_peak_val': [], 'ext_peak_coord': [], 'ext_prob': [], 'ext_snr': [], 'next_ext_peak': None}\n",
    "\n",
    "    #update ext_threshold if needed\n",
    "    int_snr1 = int_peak1 / rms\n",
    "    if ext_threshold == None:\n",
    "        if int_snr1 < 20:\n",
    "            ext_threshold = 1e-3\n",
    "        elif int_snr1 < 100:\n",
    "            ext_threshold = 1e-6\n",
    "        else:\n",
    "            ext_threshold = 1e-12\n",
    "\n",
    "    if ext_prob1 < ext_threshold:\n",
    "        ext_significant = True\n",
    "    else:\n",
    "        prob_dict['next_ext_peak'] = ext_peak1\n",
    "        ext_significant = False\n",
    "\n",
    "    while ext_significant:\n",
    "        ext_stats = region_stats(fits_file=fits_file, center=center, radius=radius, invert=True, Gaussian=False, internal=False)\n",
    "        peak = ext_stats['peak']\n",
    "        rms = ext_stats['rms']\n",
    "\n",
    "        ext_prob = calc_prob_from_rms_uncert(peak=peak, rms=rms, n_excl=n_excl)\n",
    "        if ext_prob < ext_threshold:\n",
    "            ext_stats = region_stats(fits_file=fits_file, center=center, radius=radius, invert=True, Gaussian=True, internal=False)\n",
    "            coord = ext_stats['peak_coord']\n",
    "            peak = ext_stats['peak']\n",
    "            ext_prob = calc_prob_from_rms_uncert(peak=peak, rms=rms, n_excl=n_excl)\n",
    "            prob_dict['ext_peak_val'].append(peak)\n",
    "            prob_dict['ext_peak_coord'].append(coord)\n",
    "            prob_dict['ext_prob'].append(ext_prob)\n",
    "            prob_dict['ext_snr'].append(peak / rms)\n",
    "            center.append(coord)\n",
    "            radius.append(beam_fwhm)\n",
    "        else:\n",
    "            prob_dict['next_ext_peak'] = peak\n",
    "            ext_significant = False\n",
    "\n",
    "    prob_dict['rms_val'] = rms\n",
    "\n",
    "    #find prob for 1st internal peak using updated rms\n",
    "    prob_dict['int_peak_val'].append(int_peak1)\n",
    "    prob_dict['int_peak_coord'].append(int_coord1)\n",
    "    int_prob1 = calc_prob_from_rms_uncert(peak=int_peak1, rms=rms, n_excl=n_excl, n_incl=n_incl)\n",
    "    prob_dict['int_prob'].append(int_prob1)\n",
    "    prob_dict['int_snr'].append(int_peak1 / rms)\n",
    "\n",
    "    if threshold == None:\n",
    "        threshold = 0.01\n",
    "    int_significant = (int_prob1 < threshold)\n",
    "\n",
    "    #treat 1st internal peak kind of like an external peak and get rid of search radius so we can look inside\n",
    "    center = [int_coord1]\n",
    "    radius = [beam_fwhm]\n",
    "\n",
    "    #find internal peaks in addition to 1st internal peak\n",
    "    while int_significant:\n",
    "        int_stats = region_stats(fits_file=fits_file, center=center, radius=radius, invert=True, Gaussian=False, internal=True,\\\n",
    "                                 outer_radius=search_radius)\n",
    "        int_peak = int_stats['peak']\n",
    "        int_prob = calc_prob_from_rms_uncert(peak=int_peak, rms=rms, n_excl=n_excl, n_incl=n_incl)\n",
    "        if int_prob < threshold and (int_peak > int_snr1 / 100):\n",
    "            int_stats = region_stats(fits_file=fits_file, center=center, radius=radius, invert=True, Gaussian=True, internal=True,\\\n",
    "                                     outer_radius=search_radius)\n",
    "            int_coord = int_stats['peak_coord']\n",
    "            int_peak = int_stats['peak']\n",
    "            int_prob = calc_prob_from_rms_uncert(peak=int_peak, rms=rms, n_excl=n_excl, n_incl=n_incl)\n",
    "            prob_dict['int_peak_val'].append(int_peak)\n",
    "            prob_dict['int_peak_coord'].append(int_coord)\n",
    "            prob_dict['int_prob'].append(int_prob)\n",
    "            prob_dict['int_snr'].append(int_peak / rms)\n",
    "            center.append(int_coord)\n",
    "            radius.append(beam_fwhm)\n",
    "        else:\n",
    "            int_significant = False\n",
    "\n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_rms_est_from_ext(prob_dict: dict):\n",
    "    '''\n",
    "    Using the rms estimated from the value of the exclusion region's maximum flux,\n",
    "    finds the probability of detecting the inclusion region's maximum flux if there were no source in the inclusion region,\n",
    "    the probability of detecting the exclusion region's maximum flux if there were no source in the exclusion region, and other statistics.\n",
    "\n",
    "    The estimated rms is that the probability of finding such an external peak,\n",
    "    assuming no source in the exclusion region, is 1.\n",
    "    Note: this implies that the external probability will always be 1.\n",
    "\n",
    "    The other statistics include the following as calculated using the rms estimated as described above:\n",
    "    the exclusion region's rms in Jy, the inclusion region's signal to noise ratio,\n",
    "    and the external region's signal to noise ratio.\n",
    "\n",
    "    The remaining statisitcs include the following as calculated using the rms taken directly from the image:\n",
    "    the inclusion region's maximum flux in Jy and its coordinates in pixels,\n",
    "    the exclusion region's maximum flux in Jy and its coordinates in pixels, the exclusion region's rms in Jy,\n",
    "    the number of measurements in the inclusion region, the number of measurements in the exclusion region,\n",
    "    the coordinates in pixels of the image's center, and the radii in pixels of the inclusion zones,\n",
    "    the inclusion region's signal to noise ratio, and the external region's signal to noise ratio.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prob_list : list\n",
    "        The list of statistics, as outputted by get_prob_image_rms(), for an image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list with:\n",
    "            dict(s)\n",
    "                A dictionary with the following, found using the rms taken directly from the image:\n",
    "                    float\n",
    "                        The probability of detecting the inclusion region's maximum flux if there were no source in the inclusion region.\n",
    "                    float\n",
    "                        The probability of detecting the exclusion region's maximum flux if there were no source in the exclusion region.\n",
    "                    float\n",
    "                        The inclusion region's maximum flux in Jy.\n",
    "                    tuple (int, int)\n",
    "                        The coordinates in pixels of the inclusion region's maximum flux.\n",
    "                    float\n",
    "                        The exclusion region's maximum flux in Jy.\n",
    "                    tuple (int, int)\n",
    "                        The coordinates in pixels of the exclusion region's maximum flux.\n",
    "                    float\n",
    "                        The exclusion region's rms in Jy.\n",
    "                    float\n",
    "                        The number of measurements in the inclusion region.\n",
    "                    float\n",
    "                        The number of measurements in the exclusion region.\n",
    "                    tuple (int, int)\n",
    "                        The coordinates in pixels of the image's center.\n",
    "                    list\n",
    "                        A list with:\n",
    "                            float(s)\n",
    "                                The radii in pixels of inclusion zones.\n",
    "                    float\n",
    "                        The inclusion region's signal to noise ratio.\n",
    "                    float\n",
    "                        The exclusion region's signal to noise ratio.\n",
    "            dict\n",
    "                A dictionary with the following, found using the rms estimated as described above:\n",
    "                    float\n",
    "                        The probability of detecting the inclusion region's maximum flux if there were no source in the inclusion region.\n",
    "                    float\n",
    "                        The probability of detecting the exclusion region's maximum flux if there were no source in the exclusion region.\n",
    "                    float\n",
    "                        The exclusion region's rms in Jy.\n",
    "                    float\n",
    "                        The inclusion region's signal to noise ratio.\n",
    "                    float\n",
    "                        The exclusion region's signal to noise ratio.\n",
    "    '''\n",
    "    int_peak_val = prob_dict['int_peak_val']\n",
    "    ext_peak_val = prob_dict['next_ext_peak']\n",
    "    n_incl_meas = prob_dict['n_incl_meas']\n",
    "    n_excl_meas = prob_dict['n_excl_meas']\n",
    "\n",
    "    excl_sigma = -1 * norm.ppf(1/n_excl_meas)\n",
    "    old_rms_val = ext_peak_val / excl_sigma\n",
    "    prob_dict['calc_rms_val'] = float(old_rms_val)\n",
    "\n",
    "    sigma = norm.ppf(1/(n_incl_meas + n_excl_meas))\n",
    "    neg_peak = prob_dict['neg_peak']\n",
    "\n",
    "    if neg_peak is not None:\n",
    "        rms_val = neg_peak / sigma\n",
    "        prob_dict['neg_peak_rms_val'] = float(rms_val)\n",
    "    else:\n",
    "        prob_dict['neg_peak_rms_val'] = None\n",
    "        rms_val = old_rms_val\n",
    "\n",
    "    prob_dict['calc_ext_prob'] = float(norm.cdf((-1 * ext_peak_val)/(rms_val))) * n_excl_meas\n",
    "    prob_dict['calc_ext_snr'] = float(excl_sigma)\n",
    "    for i in range(len(int_peak_val)):\n",
    "        if i == 0:\n",
    "            prob_dict['calc_int_prob'] = [float(norm.cdf((-1 * int_peak_val[i])/(rms_val))) * n_incl_meas]\n",
    "            prob_dict['calc_int_snr'] = [float(int_peak_val[i] / rms_val)]\n",
    "        else:\n",
    "            prob_dict['calc_int_prob'].append(float(norm.cdf((-1 * int_peak_val[i])/(rms_val))) * n_incl_meas)\n",
    "            prob_dict['calc_int_snr'].append(float(int_peak_val[i] / rms_val))\n",
    "\n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(fits_file: str, threshold: float = 0.01, radius_buffer: float = 5.0, ext_threshold: float = None,\\\n",
    "            short_dict: bool = True, plot: bool = True, save_path: str = ''):\n",
    "    '''\n",
    "    Summarizes an image's statistics into a shorter dictionary, a more detailed dictionary, and/or a plot,\n",
    "    with an option to save the plot as a png.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fits_file : str\n",
    "        The path of the FITS file that contains the image.\n",
    "    radius_buffer : float (optional)\n",
    "        The amount of buffer, in arcsec, to add to the beam FWHM to get the initial search radius.\n",
    "        If no value is given, defaults to 5 arcsec.\n",
    "    ext_threshold : float (optional)\n",
    "        The probability that an external peak must be below for it to be considered an external source.\n",
    "        If no value is given, defaults to 0.001.\n",
    "    short_dict : bool (optional)\n",
    "        Whether to return the short dictionary of statistics.\n",
    "        If no value is given, defaults to True.\n",
    "    full_list : bool (optional)\n",
    "        Whether to return the more detailed list of statistics.\n",
    "        If no value is given, defaults to False.\n",
    "    plot : bool (optional)\n",
    "        Whether to plot the image and statistics.\n",
    "        If no value is given, defaults to True.\n",
    "    save_path : str (optional)\n",
    "        The path to which the plot will be saved.\n",
    "        If no value is given, defaults to '' and no image is saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict (if requested)\n",
    "        A shorter dictionary with:\n",
    "            float\n",
    "                The probability, found using the rms taken directly from the image,\n",
    "                of detecting the inclusion region's maximum flux if there were no source in the inclusion region.\n",
    "            list\n",
    "                A list with:\n",
    "                    float(s)\n",
    "                        The probabilities, found using the rms taken directly from the image,\n",
    "                        of detecting the exclusion regions' maximum flux if there were no source in the exclusion regions.\n",
    "                        If there are multiple entries in this list,\n",
    "                        they are the probabilities as the exclusion region becomes increasingly small\n",
    "                        as external peaks deemed significant are added to the inclusion region.\n",
    "            float\n",
    "                The inclusion region's maximum flux in Jy.\n",
    "            tuple (float, float)\n",
    "                The coordinates in relative arcsec of the inclusion region's maximum flux.\n",
    "            list\n",
    "                A list of with:\n",
    "                    float(s)\n",
    "                        The exclusion regions' maximum fluxes in Jy.\n",
    "                        If there are multiple entries in this list,\n",
    "                        they are the maxmimum fluxes as the exclusion region becomes increasingly small\n",
    "                        as external peaks deemed significant are added to the inclusion region.\n",
    "            list\n",
    "                A list with:\n",
    "                    tuple(s) (float, float)\n",
    "                        The coordinates in relative arcsec of the exclusion regions' maximum fluxes.\n",
    "                        If there are multiple entires in this list,\n",
    "                        they are the coordinates as the exclusion region becomes increasingly small\n",
    "                        as external peaks deemed significant are added to the inclusion region.\n",
    "            float\n",
    "                The exclusion region's rms in Jy. This uses the final (smallest) exclusion region.\n",
    "            float\n",
    "                The number of measurements in the inclusion region.\n",
    "            float\n",
    "                The number of measurements in the exclusion region.\n",
    "            tuple (int, int)\n",
    "                The coordinates in relative arcsec of the image's center. Should be (0, 0).\n",
    "            list\n",
    "                A list with:\n",
    "                    float(s):\n",
    "                        The radii in arcsec of inclusion zones.\n",
    "            float\n",
    "                The inclusion region's signal to noise ratio.\n",
    "            list\n",
    "                A list with:\n",
    "                    float(s)\n",
    "                        The exclusion regions' signal to noise ratios.\n",
    "            float\n",
    "                The probability, found using the rms estimated from the value of the exclusion region's maximum flux,\n",
    "                of detecting the inclusion region's maximum flux if there were no source in the inclusion region.\n",
    "            float\n",
    "                The probability, found using the rms estimated from the value of the exclusion region's maximum flux,\n",
    "                of detecting the exclusion region's maximum flux if there were no source in the exclusion region.\n",
    "            float\n",
    "                The rms in Jy estimated from the value of the exclusion region's maximum flux.\n",
    "            float\n",
    "                The inclusion region's signal to noise ratio,\n",
    "                found using the rms estimated from the value of the exclusion region's maximum flux.\n",
    "            float\n",
    "                The exclusion region's signal to noise ratio,\n",
    "                found using the rms estimated from the value of the exclusion region's maximum flux.\n",
    "    list (if requested)\n",
    "        A more detailed list with:\n",
    "            dict(s)\n",
    "                A dictionary with the following, found using the rms taken directly from the image:\n",
    "                    float\n",
    "                        The probability of detecting the inclusion region's maximum flux if there were no source in the inclusion region.\n",
    "                    float\n",
    "                        The probability of detecting the exclusion region's maximum flux if there were no source in the exclusion region.\n",
    "                    float\n",
    "                        The inclusion region's maximum flux in Jy.\n",
    "                    tuple (float, float)\n",
    "                        The coordinates in relative arcsec of the inclusion region's maximum flux.\n",
    "                    float\n",
    "                        The exclusion region's maximum flux in Jy.\n",
    "                    tuple (float, float)\n",
    "                        The coordinates in relative arcsec of the exclusion region's maximum flux.\n",
    "                    float\n",
    "                        The exclusion region's rms in Jy.\n",
    "                    float\n",
    "                        The number of measurements in the inclusion region.\n",
    "                    float\n",
    "                        The number of measurements in the exclusion region.\n",
    "                    tuple (float, float)\n",
    "                        The coordinates in relative arcsec of the image's center. Should be (0.0, 0.0).\n",
    "                    list\n",
    "                        A list with:\n",
    "                            float(s)\n",
    "                                The radii in arcsec of inclusion zones.\n",
    "                    float\n",
    "                        The inclusion region's signal to noise ratio.\n",
    "                    float\n",
    "                        The exclusion region's signal to noise ratio.\n",
    "            dict\n",
    "                A dictionary with the following, found using the rms estimated as described above:\n",
    "                    float\n",
    "                        The probability of detecting the inclusion region's maximum flux if there were no source in the inclusion region.\n",
    "                    float\n",
    "                        The probability of detecting the exclusion region's maximum flux if there were no source in the exclusion region.\n",
    "                    float\n",
    "                        The exclusion region's rms in Jy.\n",
    "                    float\n",
    "                        The inclusion region's signal to noise ratio.\n",
    "                    float\n",
    "                        The exclusion region's signal to noise ratio.\n",
    "    '''\n",
    "    info = (get_prob_rms_est_from_ext(prob_dict_from_rms_uncert(fits_file=fits_file, threshold=threshold, radius_buffer=radius_buffer,\\\n",
    "                                                                ext_threshold=ext_threshold)))\n",
    "\n",
    "    center = info['field_center']\n",
    "\n",
    "    header_data = fits.getheader(fits_file)\n",
    "    pixel_scale = Angle(header_data['CDELT1'], header_data['CUNIT1']).to_value('arcsec')\n",
    "\n",
    "    int_x_coords = []\n",
    "    int_y_coords = []\n",
    "    int_peak_coords = info['int_peak_coord']\n",
    "    n_int_peaks = len(int_peak_coords)\n",
    "    for i in range(n_int_peaks):\n",
    "        #normalized internal peak coordinates\n",
    "        int_x_coords.append((int_peak_coords[i][0] - center[0]) * pixel_scale)\n",
    "        int_y_coords.append((int_peak_coords[i][1] - center[1]) * pixel_scale)\n",
    "    int_x_coords = np.array(int_x_coords)\n",
    "    int_y_coords = np.array(int_y_coords)\n",
    "\n",
    "    incl_radius = info['incl_radius'] #unitless but in arcsec already\n",
    "\n",
    "    # get most conservative rms and internal snr\n",
    "    hdul = fits.open(fits_file)\n",
    "    noise = None\n",
    "    try:\n",
    "        noise_col = hdul[1].columns[2]\n",
    "        if noise_col.name == 'Noise Est':\n",
    "            if noise_col.unit == 'mJy':\n",
    "                noise = float(hdul[1].data[0][2] * 1e3) # into Jy\n",
    "            elif noise_col.unit == 'Jy':\n",
    "                noise = float(hdul[1].data[0][2])\n",
    "    except:\n",
    "        pass\n",
    "    rms_list = [info['rms_val'], info['sd_mad'], info['calc_rms_val'], info['neg_peak_rms_val']] # all in Jy\n",
    "    if info['neg_peak_rms_val'] is not None:\n",
    "        rms_list.append(info['neg_peak_rms_val'])\n",
    "    if noise is not None:\n",
    "        rms_list.append(noise)\n",
    "    conservative_rms = max(rms_list) # in Jy\n",
    "    conservative_snr = round(info['int_peak_val'][0] / conservative_rms, 3)\n",
    "\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "    ext_peak_coords = info['ext_peak_coord']\n",
    "    n_ext_peaks = len(ext_peak_coords)\n",
    "    for i in range(n_ext_peaks):\n",
    "        #normalized external peak coordinates\n",
    "        x_coords.append((ext_peak_coords[i][0] - center[0]) * pixel_scale)\n",
    "        y_coords.append((ext_peak_coords[i][1] - center[1]) * pixel_scale)\n",
    "\n",
    "    fwhm = info['fwhm']\n",
    "\n",
    "    if plot:\n",
    "        #plt.rcParams['font.family'] = 'serif'\n",
    "        #plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "        plt.rcParams['font.size'] = 15\n",
    "        plt.rcParams['hatch.linewidth'] = 0.5\n",
    "        plt.rcParams['figure.dpi'] = 60\n",
    "\n",
    "        image_data = fits.getdata(fits_file)\n",
    "        shape = image_data.shape\n",
    "\n",
    "        while len(shape) > 2:\n",
    "            image_data = image_data[0]\n",
    "            shape = image_data.shape\n",
    "\n",
    "        plt.set_cmap('inferno')\n",
    "        fig, ax = plt.subplots(figsize=(6.7,5.1))\n",
    "\n",
    "        plt.plot(int_x_coords, int_y_coords, 'wo', fillstyle='none', markersize=15)\n",
    "        plt.plot(int_x_coords, int_y_coords, 'kx', fillstyle='none', markersize=15/np.sqrt(2))\n",
    "\n",
    "        for i in range(n_int_peaks):\n",
    "            int_circle = patches.Circle((int_x_coords[i], int_y_coords[i]), fwhm * pixel_scale, edgecolor='lime', fill=False)\n",
    "            ax.add_artist(int_circle)\n",
    "\n",
    "        int_circle = patches.Circle((0, 0), incl_radius, edgecolor='c', fill=False)\n",
    "        ax.add_artist(int_circle)\n",
    "\n",
    "        if n_ext_peaks > 0:\n",
    "            x_coords = np.array(x_coords)\n",
    "            y_coords = np.array(y_coords)\n",
    "            plt.plot(x_coords, y_coords, 'ko', fillstyle='none', markersize=15)\n",
    "            plt.plot(x_coords, y_coords, 'wx', fillstyle='none', markersize=15/np.sqrt(2))\n",
    "\n",
    "            for i in range(n_ext_peaks):\n",
    "                ext_circle = patches.Circle((x_coords[i], y_coords[i]), fwhm * pixel_scale, edgecolor='lime', fill=False)\n",
    "                ax.add_artist(ext_circle)\n",
    "\n",
    "        int_snr = info['int_snr'][0]\n",
    "\n",
    "        x_min = ((0 - center[0]) - 0.5) * pixel_scale\n",
    "        y_min = ((0 - center[1]) - 0.5) * pixel_scale\n",
    "        x_max = ((image_data.shape[0] -  center[0]) - 0.5) * pixel_scale\n",
    "        y_max = ((image_data.shape[1] -  center[1]) - 0.5) * pixel_scale\n",
    "\n",
    "        beam = patches.Ellipse((x_min*0.88, y_min*0.92), Angle(header_data['BMIN'], header_data['CUNIT1']).to_value('arcsec'),\\\n",
    "                               Angle(header_data['BMAJ'], header_data['CUNIT1']).to_value('arcsec'), fill=True, facecolor='w',\\\n",
    "                                edgecolor='k', angle=header_data['BPA'], hatch='/////', lw=1)\n",
    "        ax.add_artist(beam)\n",
    "\n",
    "        title = fits_file[fits_file.rindex('/')+1:fits_file.index('.fits')]\n",
    "        ax.text(x_min*0.96, y_max*0.96, f'Source: {title}\\nInternal Candidate SNR: {conservative_snr}', horizontalalignment='left', verticalalignment='top',\\\n",
    "                fontsize=10, bbox=dict(facecolor='w'))\n",
    "\n",
    "        plt.imshow(image_data, extent=[x_min, x_max, y_min, y_max], origin='lower')\n",
    "\n",
    "        plt.xlabel('Relative RA Offset [arcsec]', fontsize=15)\n",
    "        plt.ylabel('Relative Dec Offset [arcsec]', fontsize=15)\n",
    "\n",
    "        jy_to_mjy = lambda x, pos: '{}'.format(round(x*1000, 1))\n",
    "        fmt = ticker.FuncFormatter(jy_to_mjy)\n",
    "\n",
    "        cbar = plt.colorbar(shrink=0.8, format=fmt)\n",
    "        cbar.ax.set_ylabel('Intensity [mJy/beam]', fontsize=15, rotation=270, labelpad=24)\n",
    "\n",
    "        if save_path != '':\n",
    "            try:\n",
    "                file = fits_file\n",
    "                while '/' in file:\n",
    "                    file = file[file.index('/')+1:]\n",
    "                file = file.replace('.fits', '')\n",
    "                if ext_threshold == None:\n",
    "                    ext_threshold = 'default'\n",
    "                file += f'_rb{radius_buffer}_et{ext_threshold}'\n",
    "                if save_path[-1] != '/':\n",
    "                    save_path = save_path + '/'\n",
    "                plt.savefig(f'{save_path}{file}.jpg')\n",
    "            except:\n",
    "                print('Error saving figure. Double check path entered.')\n",
    "\n",
    "    if short_dict:\n",
    "        short_info = info\n",
    "\n",
    "        int_peaks = []\n",
    "        for i in range(n_int_peaks):\n",
    "            int_peaks.append((float(int_x_coords[i]), float(int_y_coords[i])))\n",
    "\n",
    "        ext_peaks = []\n",
    "        for i in range(n_ext_peaks):\n",
    "            ext_peaks.append((float(x_coords[i]), float(y_coords[i])))\n",
    "\n",
    "        if n_ext_peaks == 0:\n",
    "            ext_peaks = 'No significant external peak'\n",
    "            short_info['ext_peak_val'] = 'No significant external peak'\n",
    "            short_info['ext_snr'] = 'No significant external peak'\n",
    "            short_info['ext_prob'] = 'No significant external peak'\n",
    "\n",
    "        short_info = info\n",
    "        short_info['int_peak_coord'] = int_peaks\n",
    "        short_info['ext_peak_coord'] = ext_peaks\n",
    "        short_info['field_center'] = (0,0)\n",
    "        short_info['conservative_rms'] = conservative_rms\n",
    "        short_info['conservative_snr'] = conservative_snr\n",
    "\n",
    "        del short_info['next_ext_peak']\n",
    "\n",
    "        return short_info\n",
    "\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def significant(fits_file: str, threshold: float = 0.01, radius_buffer: float = 5.0, ext_threshold: float = None):\n",
    "    '''\n",
    "    Finds whether a significant source was detected in a field's center region.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fits_file : str\n",
    "        The path of the FITS file that contains the image.\n",
    "    threshold : float (optional)\n",
    "        The threshold for a significant detection.\n",
    "        If the probability of detecting the center region's maximum flux assuming no source in the image\n",
    "        is less than this threshold, then the detection is deemed significant.\n",
    "        If no value is given, defaults to 0.01.\n",
    "    radius_buffer : float (optional)\n",
    "        The amount of buffer, in arcsec, to add to the beam FWHM to get the initial search radius.\n",
    "        If no value is given, defaults to 5 arcsec.\n",
    "    ext_threshold : float (optional)\n",
    "        The probability that an external peak must be below for it to be considered an external source.\n",
    "        If no value is given, defaults to 0.001.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool : Whether a significant source was detected in the field's center region.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If threshold is not between 0 and 1, inclusive.\n",
    "    '''\n",
    "\n",
    "    #make sure reasonable input\n",
    "    if not (threshold >= 0 and threshold <= 1):\n",
    "        raise ValueError('Threshold must be between 0 and 1, inclusive.')\n",
    "\n",
    "    summ = summary(fits_file=fits_file, radius_buffer=radius_buffer, ext_threshold=ext_threshold, short_dict=True, plot=False)\n",
    "    return (summ['int_prob'][0] < threshold and summ['calc_int_prob'][0] < threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_catalog(fits_file: str, threshold: float = 0.01, radius_buffer: float = 5.0, ext_threshold: float = None):\n",
    "    '''\n",
    "    Summarizes information on any significant point sources detected in an image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fits_file : str\n",
    "        The path of the FITS file that contains the image.\n",
    "    threshold : float (optional)\n",
    "        The threshold for a significant detection.\n",
    "        If the probability of detecting the center region's maximum flux assuming no source in the image\n",
    "        is less than this threshold, then the detection is deemed significant.\n",
    "        If no value is given, defaults to 0.01.\n",
    "    radius_buffer : float (optional)\n",
    "        The amount of buffer, in arcsec, to add to the beam FWHM to get the initial search radius.\n",
    "        If no value is given, defaults to 5 arcsec.\n",
    "    ext_threshold : float (optional)\n",
    "        The probability that an external peak must be below for it to be considered an external source.\n",
    "        If no value is given, defaults to 0.001.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with:\n",
    "            dict(s)\n",
    "                A dictionary with:\n",
    "                    str\n",
    "                        The name of the target object of the observation.\n",
    "                    str\n",
    "                        The date and time of the observation.\n",
    "                    str\n",
    "                        The name of the FITS file with the image.\n",
    "                    Angle\n",
    "                        The restoring beam major axis.\n",
    "                    Angle\n",
    "                        The restoring beam minor axis.\n",
    "                    Angle\n",
    "                        The restoring beam position angle.\n",
    "                    float\n",
    "                        The uncertainty in flux density measurements. The rms excluding any significant sources and a small circular region around them.\n",
    "                    float\n",
    "                        The flux density of the detected point source.\n",
    "                    SkyCoord\n",
    "                        The location of the detected point source.\n",
    "                    bool\n",
    "                        Whether the detected point source is in the initial search region.\n",
    "    '''\n",
    "\n",
    "    summ = summary(fits_file=fits_file, radius_buffer=radius_buffer, ext_threshold=ext_threshold, short_dict=True, plot=False)\n",
    "\n",
    "    header_data = fits.getheader(fits_file)\n",
    "    name = header_data['OBJECT']\n",
    "    obs_date_time = header_data['DATE-OBS']\n",
    "    bmaj = header_data['BMAJ']\n",
    "    bmin = header_data['BMIN']\n",
    "    bpa = header_data['BPA']\n",
    "    ctype1 = header_data['CTYPE1']\n",
    "    crval1 = header_data['CRVAL1']\n",
    "    cunit1 = header_data['CUNIT1']\n",
    "    ctype2 = header_data['CTYPE2']\n",
    "    crval2 = header_data['CRVAL2']\n",
    "    cunit2 = header_data['CUNIT2']\n",
    "    ctype3 = header_data['CTYPE3']\n",
    "    crval3 = header_data['CRVAL3']\n",
    "    cunit3 = header_data['CUNIT3']\n",
    "\n",
    "    freq = 'Not found'\n",
    "    if ctype3 == 'FREQ':\n",
    "        if cunit3 == 'GHz':\n",
    "            freq = crval3\n",
    "        elif cunit2 == 'Hz':\n",
    "            freq = crval3 / 1e9 # into GHz\n",
    "        freq = round(freq, 3)\n",
    "    elif ctype3 == 'CHANNUM':\n",
    "        hdul = fits.open(fits_file)\n",
    "        try:\n",
    "            freq_col = hdul[1].columns[1]\n",
    "            if freq_col.name == 'Freq':\n",
    "                if freq_col.unit == 'Hz':\n",
    "                    freq = hdul[1].data[0][1] / 1e9 # into GHz\n",
    "                elif freq_col.unit == 'GHz':\n",
    "                    freq = hdul[1].data[0][1]\n",
    "            freq = round(freq, 3)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    #assume beam axes in same units as CUNIT1 and CUNIT2 and BPA in degrees\n",
    "    beam_maj_axis = Angle(bmaj, cunit1)\n",
    "    beam_min_axis = Angle(bmin, cunit1)\n",
    "    bpa_rad = math.radians(bpa)\n",
    "\n",
    "    moving_objects = ['venus', 'mars', 'jupiter', 'uranus', 'neptune', 'io', 'europa', 'ganymede', 'callisto', 'titan',\\\n",
    "               'ceres', 'vesta', 'pallas', 'juno']\n",
    "\n",
    "    stationary = True\n",
    "    if name.lower() in moving_objects:\n",
    "        stationary = False\n",
    "    else:\n",
    "        for obj in moving_objects:\n",
    "            if obj in name.lower():\n",
    "                stationary = False\n",
    "                break\n",
    "\n",
    "    interesting_sources = {}\n",
    "    field_info = {'FieldName': name, 'ObsDateTime': obs_date_time, 'FileName': fits_file[fits_file.rindex('/')+1:],\\\n",
    "                   'Stationary': stationary,\\\n",
    "                   'BeamMajAxis_arcsec': round(float(beam_maj_axis.to(u.arcsec)/u.arcsec), 3),\\\n",
    "                   'BeamMinAxis_arcsec': round(float(beam_min_axis.to(u.arcsec)/u.arcsec), 3),\\\n",
    "                   'BeamPosAngle_deg': round(bpa, 3),\\\n",
    "                   'Freq_GHz': freq}\n",
    "\n",
    "    field_info['FluxUncert_mJy'] = round(summ['conservative_rms'] * 1e3, 3)\n",
    "\n",
    "    n_int_sources = len(summ['int_peak_val'])\n",
    "    if type(summ['ext_peak_val']) == str:\n",
    "        n_ext_sources = 0\n",
    "    else:\n",
    "        n_ext_sources = len(summ['ext_peak_val'])\n",
    "\n",
    "    ra_index = 0\n",
    "    dec_index = 1\n",
    "\n",
    "    if 'RA' in ctype1:\n",
    "        ra = crval1\n",
    "    elif 'RA' in ctype2:\n",
    "        ra = crval2\n",
    "        ra_index = 1\n",
    "    else:\n",
    "        raise ValueError('No RA in image')\n",
    "\n",
    "    if 'DEC' in ctype1:\n",
    "        dec = crval1\n",
    "        dec_index = 0\n",
    "    elif 'DEC' in ctype2:\n",
    "        dec = crval2\n",
    "    else:\n",
    "        raise ValueError('No dec in image')\n",
    "\n",
    "    if cunit1 != cunit2:\n",
    "        raise ValueError('Axes have different units')\n",
    "\n",
    "    center = SkyCoord(ra, dec, unit=cunit1)\n",
    "\n",
    "    pt_source_count = 1\n",
    "\n",
    "    for i in range(n_int_sources):\n",
    "        if (summ['int_prob'][i] < threshold and summ['calc_int_prob'][i] < threshold):\n",
    "            info = field_info.copy()\n",
    "            info['Flux_mJy'] = round(summ['int_peak_val'][i] * 1000, 3)\n",
    "\n",
    "            snr = summ['int_peak_val'][i] / summ['conservative_rms']\n",
    "            b_min_uncert = float(bmaj / snr)\n",
    "            b_maj_uncert = float(bmin / snr)\n",
    "            info['RAUncert_arcsec'] = round(b_min_uncert*abs(math.sin(bpa)) + b_maj_uncert*abs(math.cos(bpa)), 3)\n",
    "            info['DecUncert_arcsec'] = round(b_maj_uncert*abs(math.sin(bpa)) + b_min_uncert*abs(math.cos(bpa)), 3)\n",
    "\n",
    "            ra_offset = summ['int_peak_coord'][i][ra_index] * u.arcsec\n",
    "            dec_offset = summ['int_peak_coord'][i][dec_index] * u.arcsec\n",
    "            coord = center.spherical_offsets_by(ra_offset, dec_offset)\n",
    "\n",
    "            ra_tuple = coord.ra.hms\n",
    "            dec_tuple = coord.dec.dms\n",
    "\n",
    "            # rounding the arcseconds to 2 past the decimal\n",
    "            ra_str = f'{int(ra_tuple.h)}h{abs(int(ra_tuple.m))}m{abs(round(float(ra_tuple.s), 2))}s'\n",
    "            dec_str = f'{int(dec_tuple.d)}d{abs(int(dec_tuple.m))}m{abs(round(float(dec_tuple.s), 2))}s'\n",
    "\n",
    "            info['RA'] = ra_str\n",
    "            info['Dec'] = dec_str\n",
    "            info['Internal'] = True\n",
    "\n",
    "            key = f'Source{pt_source_count}'\n",
    "            interesting_sources[key] = info\n",
    "            pt_source_count +=1\n",
    "\n",
    "    for i in range(n_ext_sources):\n",
    "        info = field_info.copy()\n",
    "        info['Flux_mJy'] = round(summ[f'ext_peak_val'][i] * 1000, 3)\n",
    "\n",
    "        snr = summ['ext_peak_val'][i] / summ['conservative_rms']\n",
    "        b_min_uncert = float(bmaj / snr)\n",
    "        b_maj_uncert = float(bmin / snr)\n",
    "        info['RAUncert_arcsec'] = round(b_min_uncert*abs(math.sin(bpa)) + b_maj_uncert*abs(math.cos(bpa)), 3)\n",
    "        info['DecUncert_arcsec'] = round(b_maj_uncert*abs(math.sin(bpa)) + b_min_uncert*abs(math.cos(bpa)), 3)\n",
    "\n",
    "        ra_offset = summ['ext_peak_coord'][i][ra_index] * u.arcsec\n",
    "        dec_offset = summ['ext_peak_coord'][i][dec_index] * u.arcsec\n",
    "        coord = center.spherical_offsets_by(ra_offset, dec_offset)\n",
    "\n",
    "        ra_tuple = coord.ra.hms\n",
    "        dec_tuple = coord.dec.dms\n",
    "\n",
    "        # rounding the arcseconds to 2 past the decimal\n",
    "        ra_str = f'{int(ra_tuple.h)}h{abs(int(ra_tuple.m))}m{abs(round(float(ra_tuple.s), 2))}s'\n",
    "        dec_str = f'{int(dec_tuple.d)}d{abs(int(dec_tuple.m))}m{abs(round(float(dec_tuple.s), 2))}s'\n",
    "\n",
    "        info['RA'] = ra_str\n",
    "        info['Dec'] = dec_str\n",
    "        info['Internal'] = False\n",
    "\n",
    "        key = f'Source{pt_source_count}'\n",
    "        interesting_sources[key] = info\n",
    "        pt_source_count +=1\n",
    "\n",
    "    if interesting_sources == {}:\n",
    "        return\n",
    "    else:\n",
    "        return interesting_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_catalogs(catalog_1: dict, catalog_2: dict):\n",
    "    '''\n",
    "    Combines two catalogs in the format returned by make_catalog() into a single catalog of the same format.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    catalog_1 : dict\n",
    "        The catalog to which the other catalog will be \"appended.\"\n",
    "    catalog_2 : dict\n",
    "        The catalog to \"append\" to the other catalog.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary of the combined catalogs in the same catalog format.\n",
    "    '''\n",
    "\n",
    "    shift = len(catalog_1)\n",
    "    for key, value in catalog_2.items():\n",
    "        new_number = int(key.replace('Source', ''))\n",
    "        new_key = f'Source{new_number + shift}'\n",
    "        catalog_1[new_key] = value\n",
    "    return catalog_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_html(html_path):\n",
    "    '''\n",
    "    Starts source_info.html, in which source information can be stored.\n",
    "    '''\n",
    "\n",
    "    with open(html_path, 'w') as html_file:\n",
    "        start = '''\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <style>\n",
    "        img.field {\n",
    "        width: 40%;\n",
    "        height: 40%\n",
    "        }\n",
    "        img.bp {\n",
    "        width: 20%;\n",
    "        height: 20%\n",
    "        }\n",
    "        img.gain {\n",
    "        width: 45%;\n",
    "        height: 45%\n",
    "        }\n",
    "        .centered-large-text {\n",
    "        text-align: center;\n",
    "        font-size: 36px;\n",
    "        }\n",
    "        </style>\n",
    "        <body>\n",
    "        '''\n",
    "        html_file.write(start)\n",
    "        html_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_info_to_html(json_file: str, html_path: str):\n",
    "    '''\n",
    "    Appends observation information table to source_info.html using information from a .json file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    json_file : str\n",
    "        The path of the .json file that contains the observation information.\n",
    "    '''\n",
    "\n",
    "    with open(html_path, 'a') as html_file:\n",
    "        try:\n",
    "            with open(json_file, 'r') as file:\n",
    "                obs_dict = json.load(file)\n",
    "\n",
    "            #cleaning up obs_dict\n",
    "            for key, value in obs_dict.items():\n",
    "                if type(value) == list:\n",
    "                    string = ', '.join(value)\n",
    "                    obs_dict[key] = [string]\n",
    "            obs_id = obs_dict.pop('obsID')\n",
    "            base_name = obs_dict.pop('basename')\n",
    "\n",
    "            df = pd.DataFrame(obs_dict)\n",
    "            df_transposed = df.T\n",
    "\n",
    "            html_table = df_transposed.to_html()\n",
    "\n",
    "            html_file.write(f'<p class=\\'centered-large-text\\'>Source Information for {base_name} (ObsID {obs_id}) </p>')\n",
    "            html_file.write(html_table)\n",
    "        except:\n",
    "            html_file.write('<p> Error generating observation information table. </p>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap_eff_to_html(html_path, matlab: str):\n",
    "\n",
    "    try:\n",
    "        data = loadmat(matlab)\n",
    "        ap_eff_array = data['apEffCorr']\n",
    "\n",
    "        n_ants = len(ap_eff_array)\n",
    "        panda_dict = {}\n",
    "\n",
    "        for ant in range(n_ants):\n",
    "            ant_eff = {}\n",
    "            ant_eff['RxA LSB'] = float(ap_eff_array[ant][0])\n",
    "            ant_eff['RxA USB'] = float(ap_eff_array[ant][1])\n",
    "            ant_eff['RxB LSB'] = float(ap_eff_array[ant][2])\n",
    "            ant_eff['RxB USB'] = float(ap_eff_array[ant][3])\n",
    "            panda_dict[f'Ant {ant+1}'] = ant_eff\n",
    "\n",
    "        df = pd.DataFrame.from_dict(panda_dict)\n",
    "        df_transposed = df.T\n",
    "        html_table = df_transposed.to_html()\n",
    "\n",
    "        with open(html_path, 'a') as html_file:\n",
    "            html_file.write(html_table)\n",
    "    except:\n",
    "        print('Error with aperture efficiency data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibration_plots(html_path, matlab: str):\n",
    "\n",
    "    plt.rcdefaults()\n",
    "    plt.rcParams['figure.dpi'] = 60\n",
    "    plt.rcParams['font.size'] = 8\n",
    "\n",
    "\n",
    "    data = loadmat(matlab)\n",
    "    gt = data['gainTime']\n",
    "    gws = data['gainWinSoln']\n",
    "    gcs = data['gainChanSoln']\n",
    "    gain_type = data['gainType']\n",
    "\n",
    "    n_times = len(gt)\n",
    "    n_ants = len(gws[0])\n",
    "    n_spws = len(gws[0][0])\n",
    "    n_chans = len(gcs[0][0][0])\n",
    "\n",
    "    utc_midpts = []\n",
    "    for t in range(len(gt)):\n",
    "        midpt = 0.5 * (gt[t][0].real + gt[t][0].imag)\n",
    "        utc_midpts.append((midpt%1)*24)\n",
    "\n",
    "    colors = ['blue','r','y','purple','orange','g','m','c']\n",
    "\n",
    "    chan_bit = 7\n",
    "    if all(bit == 0 for bit in (gain_type & (2**chan_bit))):\n",
    "        chan_bit = 0\n",
    "    spw_bit = 6\n",
    "    if all(bit == 0 for bit in (gain_type & (2**spw_bit))):\n",
    "        spw_bit = 1\n",
    "\n",
    "    #plotting bandpass gain solutions for amplitude and phase\n",
    "    fig, ax = plt.subplots(nrows=n_ants, ncols=1, sharex=True, figsize=(3,8))\n",
    "    fig2, ax2 = plt.subplots(nrows=n_ants, ncols=1, sharex=True, figsize=(3,8))\n",
    "\n",
    "    max_amp = 0\n",
    "\n",
    "    for time in range(n_times):\n",
    "        if (gain_type & (2**chan_bit))[time] != 0:\n",
    "            for ant in range(n_ants):\n",
    "\n",
    "                #shifting for cosmetics\n",
    "                pos = ax[ant].get_position()\n",
    "                pos.x0 += 0.05\n",
    "                pos.x1 += 0.05\n",
    "                ax[ant].set_position(pos)\n",
    "                pos2 = ax2[ant].get_position()\n",
    "                pos2.x0 += 0.06\n",
    "                pos2.x1 += 0.06\n",
    "                ax2[ant].set_position(pos2)\n",
    "\n",
    "                #no x axis ticks\n",
    "                ax[ant].xaxis.set_tick_params(labelbottom=False)\n",
    "                ax2[ant].xaxis.set_tick_params(labelbottom=False)\n",
    "\n",
    "\n",
    "                for spw in range(n_spws):\n",
    "                    amp_to_plot = [abs(a) for a in gcs.copy()[time][ant][spw]]\n",
    "                    pha_to_plot = [np.angle(p, deg=True) for p in gcs.copy()[time][ant][spw]]\n",
    "                    if max(amp_to_plot) > max_amp:\n",
    "                        max_amp = max(amp_to_plot)\n",
    "\n",
    "                    x_axis = np.arange(spw * n_chans + 1, (1 + spw) * n_chans + 1)\n",
    "\n",
    "                    ax[ant].scatter(x_axis, amp_to_plot, c=colors[spw], s=20, marker='x', linewidths=1.5)\n",
    "                    ax2[ant].scatter(x_axis, pha_to_plot, c=colors[spw], s=20, marker='x', linewidths=1.5)\n",
    "\n",
    "                    ax[ant].yaxis.set_label_position('right')\n",
    "                    ax2[ant].yaxis.set_label_position('right')\n",
    "                    ax[ant].set_ylabel(f'Ant{ant+1}')\n",
    "                    ax2[ant].set_ylabel(f'Ant{ant+1}')\n",
    "\n",
    "    plt.setp(ax, yticks=np.arange(0, max_amp+1, 0.5))\n",
    "    plt.setp(ax2, yticks=[-180,-120,-60,0,60,120,180])\n",
    "    fig.suptitle('Bandpass gain solutions for amplitude', y=0.92)\n",
    "    fig2.suptitle('Bandpass gain solutions for phase', y=0.92)\n",
    "    fig.supxlabel('Full antenna bandwidth', y=0.07)\n",
    "    fig2.supxlabel('Full antenna bandwidth', y=0.07)\n",
    "    fig.supylabel('Gain amplitude')\n",
    "    fig2.supylabel('Gain phase')\n",
    "\n",
    "    html_folder = os.path.dirname(html_path)\n",
    "\n",
    "    fig.savefig(os.path.join(html_folder, 'bp_amp.jpg'))\n",
    "    fig2.savefig(os.path.join(html_folder, 'bp_pha.jpg'))\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    #plotting gain solutions for amplitude and phase\n",
    "    n_rows = math.ceil(n_ants / 2)\n",
    "    n_cols = 2\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=n_rows, ncols=n_cols, sharex=True, figsize=(5.7,4))\n",
    "    fig2, ax2 = plt.subplots(nrows=n_rows, ncols=n_cols, sharex=True, figsize=(5.7,4))\n",
    "\n",
    "    max_amp, min_time, max_time = 0, float('inf'), 0\n",
    "\n",
    "    for spw in range(n_spws):\n",
    "        for ant in range(n_ants):\n",
    "            amp_to_plot, pha_to_plot = [], []\n",
    "            times = []\n",
    "\n",
    "            if ant < n_rows:\n",
    "                row, col = ant, 0\n",
    "\n",
    "                #shifting for cosmetics\n",
    "                pos = ax[row, col].get_position()\n",
    "                pos.x0 -= 0.005\n",
    "                pos.x1 -= 0.005\n",
    "                ax[row, col].set_position(pos)\n",
    "                pos2 = ax2[row, col].get_position()\n",
    "                pos2.x0 -= 0.005\n",
    "                pos2.x1 -= 0.005\n",
    "                ax2[row, col].set_position(pos2)\n",
    "            else:\n",
    "                row, col = ant % n_rows, 1\n",
    "\n",
    "            for time in range(n_times):\n",
    "                if gain_type[time] & (2**6) != 0:\n",
    "                    amp_val = abs((gws.copy())[time][ant][spw])\n",
    "                    pha_val = np.angle((gws.copy())[time][ant][spw], deg=True)\n",
    "                    amp_to_plot.append(amp_val)\n",
    "                    pha_to_plot.append(pha_val)\n",
    "\n",
    "                    if amp_val > max_amp:\n",
    "                        max_amp = amp_val\n",
    "\n",
    "                    t = utc_midpts[time]\n",
    "                    if t < min_time:\n",
    "                        min_time = t\n",
    "                    if t > max_time:\n",
    "                        max_time = t\n",
    "\n",
    "                    times.append(t)\n",
    "\n",
    "            ax[row, col].scatter(times, amp_to_plot, c=colors[spw], s=4, marker='D')\n",
    "            ax2[row, col].scatter(times, pha_to_plot, c=colors[spw], s=4, marker='D')\n",
    "\n",
    "            ax[row, col].yaxis.set_label_position('right')\n",
    "            ax2[row, col].yaxis.set_label_position('right')\n",
    "            ax[row, col].set_ylabel(f'Ant{ant+1}')\n",
    "            ax2[row, col].set_ylabel(f'Ant{ant+1}')\n",
    "            amp_to_plot, pha_to_plot = [], []\n",
    "\n",
    "    plt.setp(ax, xticks=np.arange(min_time//1, math.ceil(max_time), 1), yticks=np.arange(0, max_amp+1, 0.5))\n",
    "    plt.setp(ax2, xticks=np.arange(min_time//1, math.ceil(max_time), 1), yticks=[-180,-120,-60,0,60,120,180])\n",
    "    fig.suptitle('Gain solutions for amplitude')\n",
    "    fig2.suptitle('Gain solutions for phase')\n",
    "    fig.supxlabel('UT hours')\n",
    "    fig2.supxlabel('UT hours')\n",
    "    fig.supylabel('Gain amplitude')\n",
    "    fig2.supylabel('Gain phase')\n",
    "\n",
    "    fig.savefig(os.path.join(html_folder, 'g_amp.jpg'))\n",
    "    fig2.savefig(os.path.join(html_folder, 'g_pha.jpg'))\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_to_html(html_path: str, fits_file: str, radius_buffer: float = 5.0, ext_threshold: float = None):\n",
    "    '''\n",
    "    Appends source figures to source_info.html.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fits_file : str\n",
    "        The path of the FITS file that contains the image.\n",
    "    radius_buffer : float (optional)\n",
    "        The amount of buffer, in arcsec, to add to the beam FWHM to get the initial search radius.\n",
    "        If no value is given, defaults to 5 arcsec.\n",
    "    ext_threshold : float (optional)\n",
    "        The probability that an external peak must be below for it to be considered an external source.\n",
    "        If no value is given, defaults to 0.001.\n",
    "    '''\n",
    "\n",
    "    with open(html_path, 'a') as html_file:\n",
    "        try:\n",
    "            summary(fits_file=fits_file, radius_buffer=radius_buffer, ext_threshold=ext_threshold,\\\n",
    "                    short_dict=False, plot=True, save_path=os.path.dirname(html_path))\n",
    "\n",
    "            #getting full path\n",
    "            file = fits_file\n",
    "            while '/' in file:\n",
    "                file = file[file.index('/')+1:]\n",
    "            file = file.replace('.fits', '')\n",
    "            if ext_threshold == None:\n",
    "                ext_threshold = 'default'\n",
    "            file += f'_rb{radius_buffer}_et{ext_threshold}'\n",
    "            full_path = f'./{file}.jpg'\n",
    "\n",
    "            html_figure = f'''\n",
    "            <img class=\\'field\\' src=\\'{full_path}\\'>\n",
    "            <br>\n",
    "            '''\n",
    "\n",
    "            html_file.write(html_figure)\n",
    "        except:\n",
    "            html_file.write(f'<p> Error generating figure for {fits_file}. </p>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catalog_to_html(catalog: dict, html_path):\n",
    "    '''\n",
    "    Appends source information table to source_info.html.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    catalog : dict\n",
    "        A catalog in the format returned by make_catalog().\n",
    "    '''\n",
    "\n",
    "    df = pd.DataFrame.from_dict(catalog)\n",
    "    df_transposed = df.T\n",
    "    html_table = df_transposed.to_html()\n",
    "\n",
    "    with open(html_path, 'a') as html_file:\n",
    "        html_file.write(html_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_html(html_path: str):\n",
    "    '''\n",
    "    Ends source_info.html, in which source information can be stored.\n",
    "    '''\n",
    "\n",
    "    with open(html_path, 'a') as html_file:\n",
    "\n",
    "        end = '''\n",
    "        </body>\n",
    "        </html>\n",
    "        '''\n",
    "\n",
    "        html_file.write(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_html_and_txt(folder: str, threshold: float = 0.01, radius_buffer: float = 5.0, ext_threshold: float = None):\n",
    "    '''\n",
    "    From a folder of FITS files, creates source_info.html with observation information table, source figures, and source information table\n",
    "    and creates interesting_field.txt with names of objects with any (possibly) interesting detections.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder : str\n",
    "        The path of the folder containing the FITS files to be analyzed.\n",
    "    threshold : float (optional)\n",
    "        The threshold for a significant detection.\n",
    "        If the probability of detecting the center region's maximum flux assuming no source in the image\n",
    "        is less than this threshold, then the detection is deemed significant.\n",
    "        If no value is given, defaults to 0.01.\n",
    "    radius_buffer : float (optional)\n",
    "        The amount of buffer, in arcsec, to add to the beam FWHM to get the initial search radius.\n",
    "        If no value is given, defaults to 5 arcsec.\n",
    "    ext_threshold : float (optional)\n",
    "        The probability that an external peak must be below for it to be considered an external source.\n",
    "        If no value is given, defaults to 0.001.\n",
    "    '''\n",
    "\n",
    "    html_path = os.path.join(folder, 'index.html')\n",
    "    matlab_file = os.path.join(folder, 'gains.mat')\n",
    "\n",
    "    start_html(html_path)\n",
    "\n",
    "    json_file = os.path.join(folder, 'polaris.json')\n",
    "\n",
    "    obs_info_to_html(json_file, html_path)\n",
    "\n",
    "    ap_eff_to_html(html_path, matlab_file)\n",
    "\n",
    "    try:\n",
    "        calibration_plots(html_path, matlab_file)\n",
    "\n",
    "        with open(html_path, 'a') as html_file:\n",
    "            html_gain_info = f'''\n",
    "            <img class=\\'bp\\' src=\\'./bp_amp.jpg'\\'>\n",
    "            <img class=\\'bp\\' src=\\'./bp_pha.jpg'\\'>\n",
    "            <br>\n",
    "            <img class=\\'gain\\' src=\\'./g_amp.jpg'\\'>\n",
    "            <img class=\\'gain\\' src=\\'./g_pha.jpg'\\'>\n",
    "            <br>\n",
    "            '''\n",
    "            html_file.write(html_gain_info)\n",
    "    except:\n",
    "        print('Error with gain calibration information.')\n",
    "\n",
    "    final_catalog = {}\n",
    "    with open(json_file, 'r') as file:\n",
    "        obs_dict = json.load(file)\n",
    "\n",
    "    sci_targs = [targ.lower() for targ in obs_dict['sciTargs']]\n",
    "    pol_cals = [cal.lower() for cal in obs_dict['polCals']]\n",
    "    with open(os.path.join(folder, 'interesting_fields.txt'), 'w') as txt:\n",
    "        for file in glob.glob(os.path.join(folder, '*.fits')):\n",
    "            obj = fits.getheader(file)['OBJECT']\n",
    "            if obj.lower() not in pol_cals:\n",
    "                fig_to_html(html_path, file, radius_buffer=radius_buffer, ext_threshold=ext_threshold)\n",
    "            if obj.lower() in sci_targs:\n",
    "                catalog = make_catalog(file, threshold=threshold, radius_buffer=radius_buffer, ext_threshold=ext_threshold)\n",
    "\n",
    "                #add field name to .txt file if it is a science target with a significant detection in the initial inclusion region\n",
    "                if catalog != None:\n",
    "                    for key, value in catalog.items():\n",
    "                        if value['Internal'] == True:\n",
    "                            txt.write(f'{obj}\\n')\n",
    "                    final_catalog = combine_catalogs(final_catalog, catalog)\n",
    "\n",
    "    catalog_to_html(final_catalog, html_path)\n",
    "    end_html(html_path)\n",
    "\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_level_table(folder: str, db_path: str = '../sources.db'):\n",
    "\n",
    "    str_obs_id = 'Unknown'\n",
    "\n",
    "    try:\n",
    "        str_obs_id = folder.replace('/mnt/COMPASS9/sma/quality/', '')\n",
    "        obs_id = str_obs_id.replace('/', '')\n",
    "        obs_id = int(obs_id) #will throw Exception if obs_id isn't just numbers\n",
    "    except Exception as e:\n",
    "        obs_id = 'Unknown'\n",
    "        print(f'Error with obsID: {e}. WARNING: Old/outdated data may not be deleted.')\n",
    "\n",
    "    if os.path.exists(db_path):\n",
    "        # get all rows from existing low level table, if it exists\n",
    "        con1_established = False\n",
    "        con1_closed = False\n",
    "        old_data_cleared = False\n",
    "        try:\n",
    "            con1 = sqlite3.connect(db_path)\n",
    "            con1_established = True\n",
    "            cur1 = con1.cursor()\n",
    "            cur1.execute(\"DELETE FROM low_level WHERE ObsID='{}'\".format(obs_id))\n",
    "            con1.commit()\n",
    "            old_data_cleared = True\n",
    "            con1.close()\n",
    "            con1_closed = True\n",
    "        except Exception as e:\n",
    "            if con1_established and not con1_closed:\n",
    "                con1.close()\n",
    "                if not old_data_cleared:\n",
    "                    print(f'Error removing old/outdated data from table \"low_level\" at {db_path}: {e}')\n",
    "\n",
    "    for file in glob.glob(os.path.join(folder, '*.fits')):\n",
    "        try:\n",
    "            catalog = make_catalog(file)\n",
    "            if catalog is not None:\n",
    "                for value in catalog.values():\n",
    "                    value['ObsID'] = obs_id\n",
    "                    value['SourceID'] = 'Unknown'\n",
    "        except Exception as e:\n",
    "            print(f'Error for {file}: {e}')\n",
    "\n",
    "    if catalog is not None:\n",
    "        df = pd.DataFrame.from_dict(catalog)\n",
    "        df = df.T\n",
    "\n",
    "        # fixing rounding error where 60 appears in the seconds\n",
    "        date_times = df['ObsDateTime'].tolist()\n",
    "        df.drop(columns='ObsDateTime', inplace=True)\n",
    "        for i in range(len(date_times)):\n",
    "            dt = date_times[i]\n",
    "            m_end = dt.rindex(':')\n",
    "            s_start = m_end + 1\n",
    "            if dt[s_start:] == '60':\n",
    "                dt = dt[:s_start] + '0'\n",
    "                fmt = '%m-%d-%y %H:%M'\n",
    "                date_times[i] = (datetime.strptime(dt[:m_end], fmt) + timedelta(minutes=1)).strftime('%m-%d-%y %H:%M:%S')\n",
    "        df['ObsDateTime'] = date_times\n",
    "\n",
    "        # write into low level table\n",
    "        con2_established = False\n",
    "        con2_closed = False\n",
    "        try:\n",
    "            con2 = sqlite3.connect(db_path)\n",
    "            con2_established = True\n",
    "            df.to_sql(\"low_level\", con=con2, if_exists='replace', index=False)\n",
    "            con2.close()\n",
    "            con2_closed = True\n",
    "        except Exception as e:\n",
    "            if con2_established and not con2_closed:\n",
    "                con2.close()\n",
    "            print(f'Error adding to table \"low_level\" at {db_path}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_level_table(db_path: str = '../sources.db'):\n",
    "\n",
    "    unique_sources = None\n",
    "\n",
    "    if os.path.exists(db_path):\n",
    "        # get all rows from low level and high level tables, if they exist\n",
    "        con1_established = False\n",
    "        con1_closed = False\n",
    "        try:\n",
    "            con1 = sqlite3.connect(db_path)\n",
    "            con1_established = True\n",
    "            low_df = pd.read_sql_query(\"SELECT * FROM low_level;\", con1)\n",
    "            if low_df.empty:\n",
    "                raise ValueError('Table \"low_level\" is empty')\n",
    "            unique_sources = pd.read_sql_query(\"SELECT * FROM high_level;\", con1).to_dict(orient='list')\n",
    "            con1.close()\n",
    "            con1_closed = True\n",
    "        except Exception as e:\n",
    "            if con1_established and not con1_closed:\n",
    "                con1.close()\n",
    "            print(f'Error reading from database at {db_path}: {e}')\n",
    "    else:\n",
    "        raise OSError(f'Path {db_path} not found')\n",
    "\n",
    "    #coarse matching\n",
    "    for row in range(len(low_df)):\n",
    "        if low_df['SourceID'].iloc[row] == 'Unknown': #check to make sure we didn't already do coarse matching\n",
    "            if low_df['Stationary'].iloc[row]:\n",
    "                if unique_sources is not None and unique_sources != []:\n",
    "                    ra = low_df['RA'].iloc[row]\n",
    "                    dec = low_df['Dec'].iloc[row]\n",
    "                    coord1 = SkyCoord(ra, dec)\n",
    "                    fwhm = low_df['BeamMajAxis_arcsec'].iloc[row]\n",
    "                    source_ids = unique_sources['SourceID']\n",
    "                    matched  = False\n",
    "                    while not matched:\n",
    "                        for i in range(len(source_ids)): #compare with each unique source\n",
    "                            coord2 = SkyCoord(unique_sources['RA'][i], unique_sources['Dec'][i])\n",
    "                            sep = coord1.separation(coord2)\n",
    "                            fwhm2_val = float(unique_sources['FWHM (arcsec)'][i])\n",
    "                            max_sep = (fwhm * fwhm2_val)**(1/2) * u.arcsec\n",
    "                            matched = (sep <= max_sep)\n",
    "                            if matched:\n",
    "                                low_df.loc[row, 'SourceID'] = source_ids[i]\n",
    "                                break\n",
    "                        break\n",
    "                    if not matched:\n",
    "                        num = 1\n",
    "                        id_nums = [int(source_id.replace('id', '')) for source_id in unique_sources['SourceID']]\n",
    "                        while num in id_nums:\n",
    "                            num += 1\n",
    "                        next_number = '0' * (4 - len(str(num))) + str(num)\n",
    "                        next_id = f'id{next_number}'\n",
    "                        source_ids.append(next_id)\n",
    "                        unique_sources['RA'].append(ra)\n",
    "                        unique_sources['Dec'].append(dec)\n",
    "                        unique_sources['FWHM (arcsec)'].append(fwhm)\n",
    "                        low_df.loc[row, 'SourceID'] = next_id\n",
    "                        unique_sources['Ambiguous Ties'].append('Unknown')\n",
    "                else:\n",
    "                    ra = low_df['RA'].iloc[row]\n",
    "                    dec = low_df['Dec'].iloc[row]\n",
    "                    fwhm = low_df['BeamMajAxis_arcsec'].iloc[row]\n",
    "                    unique_sources = {'SourceID': ['id0001'], 'RA': [ra], 'Dec': [dec], 'FWHM (arcsec)': [fwhm], 'Ambiguous Ties': ['Unknown']}\n",
    "                    low_df.loc[row, 'SourceID'] = 'id0001'\n",
    "            else:\n",
    "                low_df.loc[row, 'SourceID'] = 'Not Stationary'\n",
    "\n",
    "    #further refining matches\n",
    "    new_sources = unique_sources.copy()\n",
    "    refined = []\n",
    "    to_skip = []\n",
    "    for i in range(len(unique_sources['SourceID'])):\n",
    "        temp_df = low_df[(low_df['SourceID']) == unique_sources['SourceID'][i]]\n",
    "        ra_list = [Angle(ra, u.deg) for ra in temp_df['RA']]\n",
    "        dec_list = [Angle(dec, u.deg) for dec in temp_df['Dec']]\n",
    "        fwhm_list = [Angle(fwhm, u.arcsec) for fwhm in temp_df['BeamMajAxis_arcsec']]\n",
    "        if len(unique_sources['SourceID']) > 1 and i not in to_skip:\n",
    "            for j in range(i + 1, len(unique_sources['SourceID'])):\n",
    "                if j not in to_skip:\n",
    "                    temp_df2 = low_df[(low_df['SourceID']) == unique_sources['SourceID'][j]]\n",
    "                    ra_list2 = [Angle(ra, u.deg) for ra in temp_df2['RA']]\n",
    "                    dec_list2 = [Angle(dec, u.deg) for dec in temp_df2['Dec']]\n",
    "                    fwhm_list2 = [Angle(fwhm, u.arcsec) for fwhm in temp_df2['BeamMajAxis_arcsec']]\n",
    "                    new_ra_list = ra_list + ra_list2\n",
    "                    new_dec_list = dec_list + dec_list2\n",
    "                    new_fwhm_list = fwhm_list + fwhm_list2\n",
    "                    num_pts = len(new_ra_list)\n",
    "                    avg_ra = sum(new_ra_list) / num_pts\n",
    "                    avg_dec = sum(new_dec_list) / num_pts\n",
    "                    geo_avg_fwhm = math.prod(new_fwhm_list) ** (1/num_pts)\n",
    "                    avg_pt = SkyCoord(avg_ra, avg_dec)\n",
    "                    temp = 0\n",
    "                    for pt in range(num_pts):\n",
    "                        sep = avg_pt.separation(SkyCoord(new_ra_list[pt], new_dec_list[pt]))\n",
    "                        if sep > geo_avg_fwhm / 2:\n",
    "                            temp += 1\n",
    "                    proportion = (num_pts - temp) / (num_pts)\n",
    "                    if proportion == 1: #average point is a good representative for all points, same source\n",
    "                        refined.append(new_sources['SourceID'][i])\n",
    "                        #match found, update averages\n",
    "                        hms_ra = avg_ra.hms\n",
    "                        str_ra = f'{hms_ra.h}h{hms_ra.m}m{round(hms_ra.s, 2)}s'\n",
    "                        new_sources['RA'][i] = str_ra\n",
    "                        new_sources['Dec'][i] = avg_dec\n",
    "                        new_sources['FWHM (arcsec)'][i] = round(geo_avg_fwhm.value, 3)\n",
    "                        #get rid of \"replaced\" source in Ambiguous Ties\n",
    "                        for k in range(len(unique_sources['SourceID'])):\n",
    "                            unique_sources['Ambiguous Ties'][k] = unique_sources['Ambiguous Ties'][k].replace(unique_sources['SourceID'][j], '')\n",
    "                            unique_sources['Ambiguous Ties'][k] = unique_sources['Ambiguous Ties'][k].replace('__', '_')\n",
    "                            if unique_sources['Ambiguous Ties'][k][0] == '_':\n",
    "                                unique_sources['Ambiguous Ties'][k] = unique_sources['Ambiguous Ties'][k][1:]\n",
    "                            if unique_sources['Ambiguous Ties'][k][-1] == '_':\n",
    "                                unique_sources['Ambiguous Ties'][k] = unique_sources['Ambiguous Ties'][k][:-1]\n",
    "                        #update low_df\n",
    "                        indices = low_df.index[low_df['SourceID'] == unique_sources['SourceID'][j]]\n",
    "                        low_df.loc[indices, 'SourceID'] = unique_sources['SourceID'][i]\n",
    "                        to_skip.append(j)\n",
    "                    elif proportion > 0.7: #average point is a good representative for over 70% but less than 100% of points, ambiguous\n",
    "                        if new_sources['Ambiguous Ties'][i] == 'Unknown' or new_sources['Ambiguous Ties'][i] == 'None found':\n",
    "                            new_sources['Ambiguous Ties'][i] = unique_sources['SourceID'][j]\n",
    "                        elif unique_sources['SourceID'][j] not in new_sources['Ambiguous Ties'][i]:\n",
    "                            new_sources['Ambiguous Ties'][i] += '_{}'.format(unique_sources['SourceID'][j])\n",
    "                        if new_sources['Ambiguous Ties'][j] == 'Unknown' or new_sources['Ambiguous Ties'][j] == 'None found':\n",
    "                            new_sources['Ambiguous Ties'][j] = unique_sources['SourceID'][i]\n",
    "                        elif unique_sources['SourceID'][i] not in new_sources['Ambiguous Ties'][j]:\n",
    "                            new_sources['Ambiguous Ties'][j] += '_{}'.format(unique_sources['SourceID'][i])\n",
    "                    if new_sources['Ambiguous Ties'][i] == 'Unknown':\n",
    "                        new_sources['Ambiguous Ties'][i] = 'None found'\n",
    "                    if new_sources['Ambiguous Ties'][j] == 'Unknown':\n",
    "                        new_sources['Ambiguous Ties'][j] = 'None found'\n",
    "    to_skip.sort(reverse=True)\n",
    "    for k in to_skip:\n",
    "        del new_sources['SourceID'][k]\n",
    "        del new_sources['RA'][k]\n",
    "        del new_sources['Dec'][k]\n",
    "        del new_sources['FWHM (arcsec)'][k]\n",
    "        del new_sources['Ambiguous Ties'][k]\n",
    "\n",
    "    #get averages for sources only matched with coarse matching\n",
    "    for i in range(len(new_sources['SourceID'])):\n",
    "        if new_sources['SourceID'][i] not in refined:\n",
    "            temp_df = low_df[(low_df['SourceID']) == new_sources['SourceID'][i]]\n",
    "            ra_list = [Angle(ra, u.deg) for ra in temp_df['RA']]\n",
    "            dec_list = [Angle(dec, u.deg) for dec in temp_df['Dec']]\n",
    "            fwhm_list = [Angle(fwhm, u.arcsec) for fwhm in temp_df['BeamMajAxis_arcsec']]\n",
    "            num_pts = len(ra_list)\n",
    "            avg_ra = sum(ra_list) / num_pts\n",
    "            hms_ra = avg_ra.hms\n",
    "            str_ra = f'{hms_ra.h}h{hms_ra.m}m{round(hms_ra.s, 2)}s'\n",
    "            avg_dec = sum(dec_list) / num_pts\n",
    "            geo_avg_fwhm = math.prod(fwhm_list) ** (1/num_pts)\n",
    "            new_sources['RA'][i] = str_ra\n",
    "            new_sources['Dec'][i] = avg_dec\n",
    "            new_sources['FWHM (arcsec)'][i] = round(geo_avg_fwhm.value, 3)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(new_sources)\n",
    "\n",
    "    # write into low and high level tables\n",
    "    con2_established = False\n",
    "    con2_closed = False\n",
    "    try:\n",
    "        con2 = sqlite3.connect(db_path)\n",
    "        con2_established = True\n",
    "        df.to_sql(\"high_level\", con=con2, if_exists='append')\n",
    "        low_df.to_sql(\"low_level\", con=con2, if_exists='append')\n",
    "        con2.close()\n",
    "        con2_closed = True\n",
    "    except Exception as e:\n",
    "        if con2_established and not con2_closed:\n",
    "            con2.close()\n",
    "        print(f'Error adding to table(s) at {db_path}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def light_curve(db_path: str = '../sources.db', unique_ids: list = None,\\\n",
    "                plot: bool = True, table: bool = True, save_path: str = ''):\n",
    "\n",
    "    if os.path.exists(db_path):\n",
    "        # get all rows from low level and high level tables, if they exist\n",
    "        con1_established = False\n",
    "        con1_closed = False\n",
    "        try:\n",
    "            con1 = sqlite3.connect(db_path)\n",
    "            con1_established = True\n",
    "            low_df = pd.read_sql_query(\"SELECT * FROM low_level;\", con1)\n",
    "            if low_df.empty:\n",
    "                raise ValueError('Table \"low_level\" is empty')\n",
    "            high_df = pd.read_sql_query(\"SELECT * FROM high_level;\", con1)\n",
    "            if high_df.empty:\n",
    "                raise ValueError('Table \"high_level\" is empty')\n",
    "            con1.close()\n",
    "            con1_closed = True\n",
    "        except Exception as e:\n",
    "            if con1_established and not con1_closed:\n",
    "                con1.close()\n",
    "            print(f'Error reading from database at {db_path}: {e}')\n",
    "    else:\n",
    "        raise OSError(f'Path {db_path} not found')\n",
    "\n",
    "    if unique_ids == None:\n",
    "        unique_ids = high_df['SourceID'].tolist()\n",
    "\n",
    "    if plot:\n",
    "        for source in unique_ids:\n",
    "            plt.subplots()\n",
    "            source_df = low_df[low_df['SourceID'] == source]\n",
    "            fluxes = source_df['Flux_mJy'].to_list()\n",
    "            flux_errs = source_df['FluxUncert_mJy'].to_list()\n",
    "            flux_unit = 'mJy'\n",
    "            if max(fluxes) > 1000:\n",
    "                flux_unit = 'Jy'\n",
    "                for i in range(len(fluxes)):\n",
    "                    fluxes[i] /= 1000\n",
    "                    flux_errs[i] /= 1000\n",
    "            date_times = source_df['ObsDateTime'].tolist()\n",
    "            fmt_str = '%m-%d-%y %H:%M:%S'\n",
    "            date_times = [Time(datetime.strptime(dt, fmt_str), format='datetime', scale='utc').mjd for dt in date_times]\n",
    "\n",
    "            freqs = source_df['Freq_GHz'].tolist()\n",
    "            other = []\n",
    "            small_milli = [] # 1.1-1.2mm\n",
    "            large_milli = [] # 1.3-1.4mm\n",
    "            micro = [] # 870µm\n",
    "            for i in range(len(freqs)):\n",
    "                if freqs[i] == 'Not found':\n",
    "                    other.append(i)\n",
    "                    pass\n",
    "                else:\n",
    "                    try:\n",
    "                        float_freq = float(freqs[i])\n",
    "                        if float_freq > 241.77 and float_freq < 282.82: # 1.24-1.06mm\n",
    "                            small_milli.append(i)\n",
    "                        elif float_freq > 237.93 and float_freq < 208.19: # 1.44-1.26mm\n",
    "                            large_milli.append(i)\n",
    "                        elif float_freq > 333.10 and float_freq < 356.90: # 900-840µm\n",
    "                            micro.append(i)\n",
    "                        else:\n",
    "                            other.append(i)\n",
    "                    except Exception as e:\n",
    "                        print(f'Error while getting the frequencies for source {source}: {e}')\n",
    "            other_dt = [date_times[a] for a in other]\n",
    "            other_flx = [fluxes[a] for a in other]\n",
    "            other_flx_err = [flux_errs[a] for a in other]\n",
    "            sm_milli_dt = [date_times[b] for b in small_milli]\n",
    "            sm_milli_flx = [fluxes[b] for b in small_milli]\n",
    "            sm_milli_flx_err = [flux_errs[b] for b in small_milli]\n",
    "            lg_milli_dt = [date_times[c] for c in large_milli]\n",
    "            lg_milli_flx = [fluxes[c] for c in large_milli]\n",
    "            lg_milli_flx_err = [flux_errs[c] for c in large_milli]\n",
    "            micro_dt = [date_times[d] for d in micro]\n",
    "            micro_flx = [fluxes[d] for d in micro]\n",
    "            micro_flx_err = [flux_errs[d] for d in micro]\n",
    "\n",
    "            plt.errorbar(sm_milli_dt, sm_milli_flx, yerr=sm_milli_flx_err, color='g', fmt='x', capsize=3, markersize=2,\\\n",
    "                        capthick=0.5, elinewidth=0.5, label='~1.1-1.2mm')\n",
    "            plt.errorbar(lg_milli_dt, lg_milli_flx, yerr=lg_milli_flx_err, color='r', fmt='x', capsize=3, markersize=2,\\\n",
    "                        capthick=0.5, elinewidth=0.5, label='~1.3-1.4mm')\n",
    "            plt.errorbar(micro_dt, micro_flx, yerr=micro_flx_err, color='b', fmt='x', capsize=3, markersize=2,\\\n",
    "                        capthick=0.5, elinewidth=0.5, label='~870µm')\n",
    "            plt.errorbar(other_dt, other_flx, yerr=other_flx_err, color='k', fmt='x', capsize=3, markersize=2,\\\n",
    "                        capthick=0.5, elinewidth=0.5, label='Other/not found')\n",
    "\n",
    "            plt.title(f'Source {source[2:]}')\n",
    "            plt.xlabel('Modified Julian Date')\n",
    "            plt.ylabel(f'Flux [{flux_unit}]')\n",
    "            plt.legend()\n",
    "            plt.ylim(bottom=0)\n",
    "\n",
    "            if save_path != '':\n",
    "                try:\n",
    "                    if save_path[-1] != '/':\n",
    "                        save_path = save_path + '/'\n",
    "                    plt.savefig(f'{save_path}{source}.jpg')\n",
    "                except:\n",
    "                    print('Error saving figure. Double check path entered.')\n",
    "\n",
    "    if table:\n",
    "        for j in range(len(unique_ids)):\n",
    "            source = unique_ids[j]\n",
    "            source_df = low_df[low_df['SourceID'] == source]\n",
    "            dat_name = f'./{source}_flux_history.dat'\n",
    "            with open(dat_name, 'w') as new_file:\n",
    "                new_file.write('#{}, RA: {}, Dec:{}\\n'.format(source, high_df.loc[j, 'RA'], low_df.loc[j, 'Dec']))\n",
    "            cal_df = source_df.copy()\n",
    "            for col in cal_df.columns:\n",
    "                if col not in ['ObsDateTime', 'ObsID', 'Flux_mJy', 'FluxUncert_mJy', 'Freq_GHz']:\n",
    "                    cal_df.drop(columns=col, inplace=True)\n",
    "            snr_list = [round(float(cal_df['Flux_mJy'].to_list()[i] / cal_df['FluxUncert_mJy'].to_list()[i]), 2) for i in range(len(cal_df))]\n",
    "            cal_df['SNR'] = snr_list\n",
    "            fmt_str = '%m-%d-%y %H:%M:%S'\n",
    "            mjd_list = [float(Time(datetime.strptime(dt, fmt_str), format='datetime', scale='utc').mjd) for dt in cal_df['ObsDateTime']]\n",
    "            cal_df['MJD'] = mjd_list\n",
    "            cal_df.to_csv(dat_name, sep='\\t', index=False, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/COMPASS9/sma/quality/13162\n",
      "/mnt/COMPASS9/sma/quality/00000\n",
      "/mnt/COMPASS9/sma/quality/13363\n",
      "/mnt/COMPASS9/sma/quality/12296\n",
      "/mnt/COMPASS9/sma/quality/13402\n",
      "/mnt/COMPASS9/sma/quality/11316\n",
      "/mnt/COMPASS9/sma/quality/10696\n",
      "/mnt/COMPASS9/sma/quality/10695\n",
      "/mnt/COMPASS9/sma/quality/11621\n",
      "/mnt/COMPASS9/sma/quality/13430\n",
      "/mnt/COMPASS9/sma/quality/12514\n",
      "/mnt/COMPASS9/sma/quality/12198\n",
      "/mnt/COMPASS9/sma/quality/11476\n",
      "/mnt/COMPASS9/sma/quality/09233\n",
      "/mnt/COMPASS9/sma/quality/09232\n",
      "/mnt/COMPASS9/sma/quality/09758\n",
      "/mnt/COMPASS9/sma/quality/09186\n",
      "/mnt/COMPASS9/sma/quality/09185\n",
      "/mnt/COMPASS9/sma/quality/10019\n",
      "/mnt/COMPASS9/sma/quality/07685\n",
      "/mnt/COMPASS9/sma/quality/07687\n",
      "/mnt/COMPASS9/sma/quality/07730\n",
      "/mnt/COMPASS9/sma/quality/07712\n",
      "/mnt/COMPASS9/sma/quality/07715\n",
      "/mnt/COMPASS9/sma/quality/07736\n",
      "/mnt/COMPASS9/sma/quality/09406\n",
      "/mnt/COMPASS9/sma/quality/09722\n",
      "/mnt/COMPASS9/sma/quality/09725\n",
      "/mnt/COMPASS9/sma/quality/09733\n",
      "/mnt/COMPASS9/sma/quality/11012\n",
      "/mnt/COMPASS9/sma/quality/12468\n",
      "/mnt/COMPASS9/sma/quality/12961\n",
      "/mnt/COMPASS9/sma/quality/12280\n",
      "/mnt/COMPASS9/sma/quality/12299\n",
      "/mnt/COMPASS9/sma/quality/13436\n",
      "/mnt/COMPASS9/sma/quality/12444\n",
      "/mnt/COMPASS9/sma/quality/11159\n",
      "/mnt/COMPASS9/sma/quality/11178\n",
      "/mnt/COMPASS9/sma/quality/10871\n",
      "/mnt/COMPASS9/sma/quality/10910\n",
      "/mnt/COMPASS9/sma/quality/10950\n",
      "/mnt/COMPASS9/sma/quality/11002\n",
      "/mnt/COMPASS9/sma/quality/10962\n",
      "/mnt/COMPASS9/sma/quality/11007\n",
      "/mnt/COMPASS9/sma/quality/11006\n",
      "/mnt/COMPASS9/sma/quality/11019\n",
      "/mnt/COMPASS9/sma/quality/12447\n",
      "/mnt/COMPASS9/sma/quality/12989\n",
      "/mnt/COMPASS9/sma/quality/12240\n",
      "/mnt/COMPASS9/sma/quality/12308\n",
      "/mnt/COMPASS9/sma/quality/12742\n",
      "/mnt/COMPASS9/sma/quality/11020\n",
      "/mnt/COMPASS9/sma/quality/11273\n",
      "/mnt/COMPASS9/sma/quality/10870\n",
      "/mnt/COMPASS9/sma/quality/12312\n",
      "/mnt/COMPASS9/sma/quality/13481\n",
      "/mnt/COMPASS9/sma/quality/10909\n",
      "/mnt/COMPASS9/sma/quality/10738\n",
      "/mnt/COMPASS9/sma/quality/10942\n",
      "/mnt/COMPASS9/sma/quality/13404\n",
      "/mnt/COMPASS9/sma/quality/10949\n",
      "/mnt/COMPASS9/sma/quality/12460\n",
      "/mnt/COMPASS9/sma/quality/12314\n",
      "/mnt/COMPASS9/sma/quality/13409\n",
      "/mnt/COMPASS9/sma/quality/10999\n",
      "/mnt/COMPASS9/sma/quality/12320\n",
      "/mnt/COMPASS9/sma/quality/13480\n",
      "/mnt/COMPASS9/sma/quality/12315\n",
      "/mnt/COMPASS9/sma/quality/13168\n",
      "/mnt/COMPASS9/sma/quality/12327\n",
      "/mnt/COMPASS9/sma/quality/13504\n",
      "/mnt/COMPASS9/sma/quality/11009\n",
      "/mnt/COMPASS9/sma/quality/11270\n",
      "/mnt/COMPASS9/sma/quality/11011\n",
      "/mnt/COMPASS9/sma/quality/13445\n",
      "/mnt/COMPASS9/sma/quality/11018\n",
      "/mnt/COMPASS9/sma/quality/12332\n",
      "/mnt/COMPASS9/sma/quality/12876\n",
      "/mnt/COMPASS9/sma/quality/12502\n",
      "/mnt/COMPASS9/sma/quality/12494\n",
      "/mnt/COMPASS9/sma/quality/10005\n",
      "/mnt/COMPASS9/sma/quality/10529\n",
      "/mnt/COMPASS9/sma/quality/10575\n",
      "/mnt/COMPASS9/sma/quality/10577\n",
      "/mnt/COMPASS9/sma/quality/10579\n",
      "/mnt/COMPASS9/sma/quality/10581\n",
      "/mnt/COMPASS9/sma/quality/11957\n",
      "/mnt/COMPASS9/sma/quality/11993\n",
      "/mnt/COMPASS9/sma/quality/11996\n",
      "/mnt/COMPASS9/sma/quality/12014\n",
      "/mnt/COMPASS9/sma/quality/12590\n",
      "/mnt/COMPASS9/sma/quality/12591\n",
      "/mnt/COMPASS9/sma/quality/12592\n",
      "/mnt/COMPASS9/sma/quality/12594\n",
      "/mnt/COMPASS9/sma/quality/12622\n",
      "/mnt/COMPASS9/sma/quality/12623\n",
      "/mnt/COMPASS9/sma/quality/12651\n",
      "/mnt/COMPASS9/sma/quality/12660\n",
      "/mnt/COMPASS9/sma/quality/12784\n",
      "/mnt/COMPASS9/sma/quality/12785\n",
      "/mnt/COMPASS9/sma/quality/12849\n",
      "/mnt/COMPASS9/sma/quality/99999\n",
      "/mnt/COMPASS9/sma/quality/12569\n",
      "/mnt/COMPASS9/sma/quality/12335\n",
      "/mnt/COMPASS9/sma/quality/11290\n",
      "/mnt/COMPASS9/sma/quality/12449\n",
      "/mnt/COMPASS9/sma/quality/09506\n",
      "/mnt/COMPASS9/sma/quality/09514\n",
      "/mnt/COMPASS9/sma/quality/09526\n",
      "/mnt/COMPASS9/sma/quality/09010\n",
      "/mnt/COMPASS9/sma/quality/09014\n",
      "/mnt/COMPASS9/sma/quality/12480\n",
      "/mnt/COMPASS9/sma/quality/13197\n",
      "/mnt/COMPASS9/sma/quality/12337\n",
      "/mnt/COMPASS9/sma/quality/13353\n",
      "/mnt/COMPASS9/sma/quality/13057\n",
      "/mnt/COMPASS9/sma/quality/13349\n",
      "/mnt/COMPASS9/sma/quality/12865\n",
      "/mnt/COMPASS9/sma/quality/13029\n",
      "/mnt/COMPASS9/sma/quality/12340\n",
      "/mnt/COMPASS9/sma/quality/13345\n",
      "/mnt/COMPASS9/sma/quality/12736\n",
      "/mnt/COMPASS9/sma/quality/12744\n",
      "/mnt/COMPASS9/sma/quality/13094\n",
      "/mnt/COMPASS9/sma/quality/13093\n",
      "/mnt/COMPASS9/sma/quality/12342\n",
      "/mnt/COMPASS9/sma/quality/13342\n",
      "/mnt/COMPASS9/sma/quality/12538\n",
      "/mnt/COMPASS9/sma/quality/12556\n",
      "/mnt/COMPASS9/sma/quality/12527\n",
      "/mnt/COMPASS9/sma/quality/13340\n",
      "/mnt/COMPASS9/sma/quality/11138\n",
      "/mnt/COMPASS9/sma/quality/12477\n",
      "/mnt/COMPASS9/sma/quality/12509\n",
      "/mnt/COMPASS9/sma/quality/12517\n",
      "/mnt/COMPASS9/sma/quality/12463\n",
      "/mnt/COMPASS9/sma/quality/13149\n",
      "/mnt/COMPASS9/sma/quality/10961\n",
      "/mnt/COMPASS9/sma/quality/12334\n",
      "/mnt/COMPASS9/sma/quality/13420\n",
      "/mnt/COMPASS9/sma/quality/11513\n",
      "/mnt/COMPASS9/sma/quality/12547\n",
      "/mnt/COMPASS9/sma/quality/12503\n",
      "/mnt/COMPASS9/sma/quality/11497\n",
      "/mnt/COMPASS9/sma/quality/13154\n",
      "/mnt/COMPASS9/sma/quality/13155\n",
      "/mnt/COMPASS9/sma/quality/13156\n",
      "/mnt/COMPASS9/sma/quality/13157\n",
      "/mnt/COMPASS9/sma/quality/12505\n",
      "/mnt/COMPASS9/sma/quality/13355\n",
      "/mnt/COMPASS9/sma/quality/11525\n",
      "/mnt/COMPASS9/sma/quality/13152\n",
      "/mnt/COMPASS9/sma/quality/13059\n",
      "/mnt/COMPASS9/sma/quality/13358\n",
      "/mnt/COMPASS9/sma/quality/13164\n",
      "/mnt/COMPASS9/sma/quality/12513\n",
      "/mnt/COMPASS9/sma/quality/13047\n",
      "/mnt/COMPASS9/sma/quality/13058\n",
      "/mnt/COMPASS9/sma/quality/13166\n",
      "/mnt/COMPASS9/sma/quality/13080\n",
      "/mnt/COMPASS9/sma/quality/13045\n",
      "/mnt/COMPASS9/sma/quality/13055\n",
      "/mnt/COMPASS9/sma/quality/12567\n",
      "/mnt/COMPASS9/sma/quality/13160\n",
      "/mnt/COMPASS9/sma/quality/13376\n",
      "/mnt/COMPASS9/sma/quality/13175\n",
      "/mnt/COMPASS9/sma/quality/13169\n",
      "/mnt/COMPASS9/sma/quality/13185\n",
      "/mnt/COMPASS9/sma/quality/12316\n",
      "/mnt/COMPASS9/sma/quality/13081\n",
      "/mnt/COMPASS9/sma/quality/13378\n",
      "/mnt/COMPASS9/sma/quality/13220\n",
      "/mnt/COMPASS9/sma/quality/13208\n",
      "/mnt/COMPASS9/sma/quality/13209\n",
      "/mnt/COMPASS9/sma/quality/12079\n",
      "/mnt/COMPASS9/sma/quality/13082\n",
      "/mnt/COMPASS9/sma/quality/12499\n",
      "/mnt/COMPASS9/sma/quality/13464\n",
      "/mnt/COMPASS9/sma/quality/13153\n",
      "/mnt/COMPASS9/sma/quality/13325\n",
      "/mnt/COMPASS9/sma/quality/11291\n",
      "/mnt/COMPASS9/sma/quality/12545\n",
      "/mnt/COMPASS9/sma/quality/11296\n",
      "/mnt/COMPASS9/sma/quality/12878\n",
      "/mnt/COMPASS9/sma/quality/13083\n",
      "/mnt/COMPASS9/sma/quality/13327\n",
      "/mnt/COMPASS9/sma/quality/11297\n",
      "/mnt/COMPASS9/sma/quality/13224\n",
      "/mnt/COMPASS9/sma/quality/13219\n",
      "/mnt/COMPASS9/sma/quality/13245\n",
      "/mnt/COMPASS9/sma/quality/13228\n",
      "/mnt/COMPASS9/sma/quality/13227\n",
      "/mnt/COMPASS9/sma/quality/11306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/py3opt/anaconda3/lib/python3.11/site-packages/astropy/units/quantity.py:1350: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return float(self.to_value(dimensionless_unscaled))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for /mnt/COMPASS9/sma/quality/11306/1743-038_img_RX3.fits: float() argument must be a string or a real number, not 'complex'\n",
      "/mnt/COMPASS9/sma/quality/13231\n",
      "/mnt/COMPASS9/sma/quality/13243\n",
      "/mnt/COMPASS9/sma/quality/11935\n",
      "/mnt/COMPASS9/sma/quality/09929\n",
      "/mnt/COMPASS9/sma/quality/09969\n",
      "/mnt/COMPASS9/sma/quality/12930\n",
      "/mnt/COMPASS9/sma/quality/12908\n",
      "/mnt/COMPASS9/sma/quality/12501\n",
      "/mnt/COMPASS9/sma/quality/13247\n",
      "/mnt/COMPASS9/sma/quality/13259\n",
      "/mnt/COMPASS9/sma/quality/13111\n",
      "/mnt/COMPASS9/sma/quality/13387\n",
      "/mnt/COMPASS9/sma/quality/11307\n",
      "/mnt/COMPASS9/sma/quality/12081\n",
      "/mnt/COMPASS9/sma/quality/12082\n",
      "/mnt/COMPASS9/sma/quality/12385\n",
      "/mnt/COMPASS9/sma/quality/11312\n",
      "/mnt/COMPASS9/sma/quality/13226\n",
      "/mnt/COMPASS9/sma/quality/12571\n",
      "/mnt/COMPASS9/sma/quality/11266\n",
      "/mnt/COMPASS9/sma/quality/12511\n",
      "/mnt/COMPASS9/sma/quality/12543\n",
      "/mnt/COMPASS9/sma/quality/12912\n",
      "/mnt/COMPASS9/sma/quality/12372\n",
      "/mnt/COMPASS9/sma/quality/12386\n",
      "/mnt/COMPASS9/sma/quality/12390\n",
      "/mnt/COMPASS9/sma/quality/10661\n",
      "/mnt/COMPASS9/sma/quality/10704\n",
      "/mnt/COMPASS9/sma/quality/10765\n",
      "/mnt/COMPASS9/sma/quality/10664\n",
      "/mnt/COMPASS9/sma/quality/10707\n",
      "/mnt/COMPASS9/sma/quality/10721\n",
      "/mnt/COMPASS9/sma/quality/11267\n",
      "/mnt/COMPASS9/sma/quality/12738\n",
      "/mnt/COMPASS9/sma/quality/11025\n",
      "/mnt/COMPASS9/sma/quality/11028\n",
      "/mnt/COMPASS9/sma/quality/11045\n",
      "/mnt/COMPASS9/sma/quality/11051\n",
      "/mnt/COMPASS9/sma/quality/11076\n",
      "/mnt/COMPASS9/sma/quality/11078\n",
      "/mnt/COMPASS9/sma/quality/11142\n",
      "/mnt/COMPASS9/sma/quality/11135\n",
      "/mnt/COMPASS9/sma/quality/11137\n",
      "/mnt/COMPASS9/sma/quality/11105\n",
      "/mnt/COMPASS9/sma/quality/11128\n",
      "/mnt/COMPASS9/sma/quality/11145\n",
      "/mnt/COMPASS9/sma/quality/11151\n",
      "/mnt/COMPASS9/sma/quality/11021\n",
      "/mnt/COMPASS9/sma/quality/10872\n",
      "/mnt/COMPASS9/sma/quality/10943\n",
      "/mnt/COMPASS9/sma/quality/10991\n",
      "/mnt/COMPASS9/sma/quality/11000\n",
      "/mnt/COMPASS9/sma/quality/11003\n",
      "/mnt/COMPASS9/sma/quality/11010\n",
      "/mnt/COMPASS9/sma/quality/12469\n",
      "/mnt/COMPASS9/sma/quality/12957\n",
      "/mnt/COMPASS9/sma/quality/12559\n",
      "/mnt/COMPASS9/sma/quality/13239\n",
      "/mnt/COMPASS9/sma/quality/13334\n",
      "/mnt/COMPASS9/sma/quality/11269\n",
      "/mnt/COMPASS9/sma/quality/11452\n",
      "/mnt/COMPASS9/sma/quality/11358\n",
      "/mnt/COMPASS9/sma/quality/11461\n",
      "/mnt/COMPASS9/sma/quality/11492\n",
      "/mnt/COMPASS9/sma/quality/11494\n",
      "/mnt/COMPASS9/sma/quality/8384\n",
      "/mnt/COMPASS9/sma/quality/8420\n",
      "/mnt/COMPASS9/sma/quality/12405\n",
      "/mnt/COMPASS9/sma/quality/12424\n",
      "/mnt/COMPASS9/sma/quality/12416\n",
      "/mnt/COMPASS9/sma/quality/12414\n",
      "/mnt/COMPASS9/sma/quality/12428\n",
      "/mnt/COMPASS9/sma/quality/12604\n",
      "/mnt/COMPASS9/sma/quality/12729\n",
      "/mnt/COMPASS9/sma/quality/12602\n",
      "/mnt/COMPASS9/sma/quality/13419\n",
      "/mnt/COMPASS9/sma/quality/13428\n",
      "/mnt/COMPASS9/sma/quality/13425\n",
      "/mnt/COMPASS9/sma/quality/13434\n",
      "/mnt/COMPASS9/sma/quality/13264\n",
      "/mnt/COMPASS9/sma/quality/11109\n",
      "/mnt/COMPASS9/sma/quality/11171\n",
      "/mnt/COMPASS9/sma/quality/11271\n",
      "/mnt/COMPASS9/sma/quality/11340\n",
      "/mnt/COMPASS9/sma/quality/13580\n",
      "/mnt/COMPASS9/sma/quality/12735\n",
      "/mnt/COMPASS9/sma/quality/11438\n",
      "/mnt/COMPASS9/sma/quality/11437\n",
      "/mnt/COMPASS9/sma/quality/11753\n",
      "/mnt/COMPASS9/sma/quality/11134\n",
      "/mnt/COMPASS9/sma/quality/13225\n",
      "/mnt/COMPASS9/sma/quality/13601\n",
      "/mnt/COMPASS9/sma/quality/13603\n",
      "/mnt/COMPASS9/sma/quality/13607\n",
      "/mnt/COMPASS9/sma/quality/13608\n",
      "/mnt/COMPASS9/sma/quality/7685\n",
      "/mnt/COMPASS9/sma/quality/13605\n",
      "/mnt/COMPASS9/sma/quality/13615\n",
      "/mnt/COMPASS9/sma/quality/13609\n",
      "/mnt/COMPASS9/sma/quality/13253\n",
      "/mnt/COMPASS9/sma/quality/13252\n",
      "/mnt/COMPASS9/sma/quality/10479\n",
      "/mnt/COMPASS9/sma/quality/13491\n",
      "/mnt/COMPASS9/sma/quality/13513\n",
      "Error for /mnt/COMPASS9/sma/quality/13513/1658+347_img_RX0.fits: index 0 is out of bounds for axis 0 with size 0\n",
      "/mnt/COMPASS9/sma/quality/13519\n",
      "/mnt/COMPASS9/sma/quality/13523\n",
      "/mnt/COMPASS9/sma/quality/13241\n",
      "/mnt/COMPASS9/sma/quality/13273\n",
      "/mnt/COMPASS9/sma/quality/13323\n",
      "/mnt/COMPASS9/sma/quality/12479\n",
      "/mnt/COMPASS9/sma/quality/12542\n",
      "/mnt/COMPASS9/sma/quality/13233\n",
      "/mnt/COMPASS9/sma/quality/13330\n",
      "/mnt/COMPASS9/sma/quality/13361\n",
      "/mnt/COMPASS9/sma/quality/13427\n",
      "/mnt/COMPASS9/sma/quality/10500\n",
      "/mnt/COMPASS9/sma/quality/10513\n",
      "/mnt/COMPASS9/sma/quality/10546\n",
      "/mnt/COMPASS9/sma/quality/13316\n",
      "Error for /mnt/COMPASS9/sma/quality/13316/1127-189_img_RX3.fits: index 0 is out of bounds for axis 0 with size 0\n",
      "Error for /mnt/COMPASS9/sma/quality/13316/1130-148_img_RX3.fits: index 0 is out of bounds for axis 0 with size 0\n",
      "Error for /mnt/COMPASS9/sma/quality/13316/3c279_img_RX3.fits: index 0 is out of bounds for axis 0 with size 0\n",
      "/mnt/COMPASS9/sma/quality/10562\n",
      "/mnt/COMPASS9/sma/quality/10599\n",
      "/mnt/COMPASS9/sma/quality/10618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/py3opt/anaconda3/lib/python3.11/site-packages/astropy/units/quantity.py:1350: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return float(self.to_value(dimensionless_unscaled))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for /mnt/COMPASS9/sma/quality/10618/1146+399_img_RX0.fits: float() argument must be a string or a real number, not 'complex'\n",
      "No values after mask applied. Check inclusion and exclusion radii.\n",
      "Error for /mnt/COMPASS9/sma/quality/10618/1146+399_img_RX3.fits: cannot access local variable 'peak' where it is not associated with a value\n",
      "Error for /mnt/COMPASS9/sma/quality/10618/1159+292_img_RX0.fits: float division by zero\n",
      "/mnt/COMPASS9/sma/quality/10638\n",
      "/mnt/COMPASS9/sma/quality/10675\n",
      "/mnt/COMPASS9/sma/quality/13459\n",
      "/mnt/COMPASS9/sma/quality/13483\n",
      "/mnt/COMPASS9/sma/quality/13628\n",
      "/mnt/COMPASS9/sma/quality/13621\n",
      "/mnt/COMPASS9/sma/quality/13622\n",
      "/mnt/COMPASS9/sma/quality/13624\n",
      "/mnt/COMPASS9/sma/quality/10700\n",
      "/mnt/COMPASS9/sma/quality/10715\n",
      "/mnt/COMPASS9/sma/quality/10750\n",
      "/mnt/COMPASS9/sma/quality/11085\n",
      "/mnt/COMPASS9/sma/quality/11100\n",
      "/mnt/COMPASS9/sma/quality/11111\n",
      "/mnt/COMPASS9/sma/quality/11124\n",
      "/mnt/COMPASS9/sma/quality/11144\n",
      "/mnt/COMPASS9/sma/quality/11161\n",
      "/mnt/COMPASS9/sma/quality/11170\n",
      "/mnt/COMPASS9/sma/quality/11195\n",
      "/mnt/COMPASS9/sma/quality/11286\n",
      "/mnt/COMPASS9/sma/quality/11310\n",
      "/mnt/COMPASS9/sma/quality/11339\n",
      "/mnt/COMPASS9/sma/quality/11172\n",
      "/mnt/COMPASS9/sma/quality/11356\n",
      "/mnt/COMPASS9/sma/quality/13654\n",
      "/mnt/COMPASS9/sma/quality/13657\n",
      "/mnt/COMPASS9/sma/quality/13625\n",
      "/mnt/COMPASS9/sma/quality/9506\n",
      "/mnt/COMPASS9/sma/quality/9514\n",
      "/mnt/COMPASS9/sma/quality/13369\n",
      "/mnt/COMPASS9/sma/quality/13371\n",
      "/mnt/COMPASS9/sma/quality/13384\n",
      "/mnt/COMPASS9/sma/quality/13386\n",
      "/mnt/COMPASS9/sma/quality/13663\n",
      "/mnt/COMPASS9/sma/quality/13665\n",
      "/mnt/COMPASS9/sma/quality/13647\n",
      "/mnt/COMPASS9/sma/quality/11107\n",
      "/mnt/COMPASS9/sma/quality/11357\n",
      "/mnt/COMPASS9/sma/quality/11201\n",
      "/mnt/COMPASS9/sma/quality/11451\n",
      "/mnt/COMPASS9/sma/quality/11173\n",
      "/mnt/COMPASS9/sma/quality/11460\n",
      "/mnt/COMPASS9/sma/quality/13668\n",
      "/mnt/COMPASS9/sma/quality/13671\n",
      "/mnt/COMPASS9/sma/quality/10526\n",
      "/mnt/COMPASS9/sma/quality/13613\n",
      "/mnt/COMPASS9/sma/quality/13699\n",
      "/mnt/COMPASS9/sma/quality/13692\n",
      "/mnt/COMPASS9/sma/quality/13700\n"
     ]
    }
   ],
   "source": [
    "for folder in glob.glob('/mnt/COMPASS9/sma/quality/*'):\n",
    "    print(folder)\n",
    "    low_level_table(folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
